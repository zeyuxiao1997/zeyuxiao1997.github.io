<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZeyuXiao @ USTC</title>
  
  <subtitle>Paper reading notes, code sharing platforms</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zeyuxiao1997.github.io/"/>
  <updated>2019-08-07T01:05:54.464Z</updated>
  <id>https://zeyuxiao1997.github.io/</id>
  
  <author>
    <name>Zeyuxiao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>用pytorch实现GAN</title>
    <link href="https://zeyuxiao1997.github.io/2019/08/06/pt-gan/"/>
    <id>https://zeyuxiao1997.github.io/2019/08/06/pt-gan/</id>
    <published>2019-08-06T12:49:21.000Z</published>
    <updated>2019-08-07T01:05:54.464Z</updated>
    
    <content type="html"><![CDATA[<p>用pytorch实现简单的GAN生成对抗网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@File    :   gan_comments.py</span></span><br><span class="line"><span class="string">@Time    :   2019/08/06 20:52:58</span></span><br><span class="line"><span class="string">@Author  :   ZeyuXiao </span></span><br><span class="line"><span class="string">@Version :   1.0</span></span><br><span class="line"><span class="string">@Contact :   zeyuxiao1997@gmail.com</span></span><br><span class="line"><span class="string">@License :   (C)Copyright 2018-2019</span></span><br><span class="line"><span class="string">@Desc    :   None</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># here put the import lib</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建文件夹</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'./img'</span>):</span><br><span class="line">    os.mkdir(<span class="string">'./img'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_img</span><span class="params">(x)</span>:</span></span><br><span class="line">    out = <span class="number">0.5</span> * (x + <span class="number">1</span>)</span><br><span class="line">    out=out.clamp(<span class="number">0</span>,<span class="number">1</span>)<span class="comment">#Clamp函数可以将随机变化的数值限制在一个给定的区间[min, max]内：</span></span><br><span class="line">    out=out.view(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)<span class="comment">#view()函数作用是将一个多行的Tensor,拼接成一行</span></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">num_epoch = <span class="number">100</span></span><br><span class="line">z_dimension = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">img_transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.5</span>],[<span class="number">0.5</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">mnist = datasets.MNIST(root=<span class="string">'../../data'</span>,</span><br><span class="line">                       train=<span class="literal">True</span>,</span><br><span class="line">                       transform=img_transform,</span><br><span class="line">                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">data_loader = torch.utils.data.DataLoader(dataset=mnist,</span><br><span class="line">                                     batch_size=batch_size,</span><br><span class="line">                                     shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">######       Discriminator      ######</span></span><br><span class="line"><span class="comment">######  使用多层网络来作为判别器  ######</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将图片28x28展开成784，然后通过多层感知器，中间经过斜率设置为0.2的LeakyReLU激活函数，</span></span><br><span class="line"><span class="comment"># 最后接sigmoid激活函数得到一个0到1之间的概率进行二分类。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        self.dis = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">784</span>, <span class="number">256</span>),    <span class="comment"># 输入特征为784，输出为256</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),  <span class="comment"># 非线性映射</span></span><br><span class="line">            nn.Linear(<span class="number">256</span>,<span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>,<span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()    <span class="comment"># 二分类中，sigmoid可以实数映射到[0,1](作为概率值)，</span></span><br><span class="line">                            <span class="comment"># 多分类用softmax函数</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.dis(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######         Generator        ######</span></span><br><span class="line"><span class="comment">######  使用多层网络来作为判别器  ######</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入一个100维的0～1之间的高斯分布，然后通过第一层线性变换将其映射到256维,</span></span><br><span class="line"><span class="comment"># 然后通过LeakyReLU激活函数，接着进行一个线性变换，再经过一个LeakyReLU激活函数，</span></span><br><span class="line"><span class="comment"># 然后经过线性变换将其变成784维，最后经过Tanh激活函数是希望生成的假的图片数据分布</span></span><br><span class="line"><span class="comment"># 能够在-1～1之间。</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.gen = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">100</span>, <span class="number">256</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>,<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">784</span>),</span><br><span class="line">            nn.Tanh()   <span class="comment"># Tanh激活使得生成数据分布在[-1,1]之间</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.gen(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建对象</span></span><br><span class="line">D = Discriminator()</span><br><span class="line">G = Generator()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    D = D.cuda()</span><br><span class="line">    G = G.cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#########       判别器训练      ##########</span></span><br><span class="line"><span class="comment"># 分为两部分：1、真的图像判别为真；2、假的图像判别为假</span></span><br><span class="line"><span class="comment"># 此过程中，生成器参数不断更新</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先需要定义loss的度量方式  （二分类的交叉熵）</span></span><br><span class="line"><span class="comment"># 其次定义 优化函数,优化函数的学习率为0.0003</span></span><br><span class="line">criterion = nn.BCELoss()    <span class="comment"># 单目标二分类交叉熵函数</span></span><br><span class="line">dis_optimizer = torch.optim.Adam(D.parameters(), lr=<span class="number">3e-4</span>)</span><br><span class="line">gen_optimizer = torch.optim.Adam(G.parameters(), lr=<span class="number">3e-4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##判别器的判断过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    <span class="keyword">for</span> i, (img, _) <span class="keyword">in</span> enumerate(data_loader):</span><br><span class="line">        num_img = img.size(<span class="number">0</span>)</span><br><span class="line">        img = img.view(num_img, <span class="number">-1</span>)  <span class="comment"># 将图片展开为28*28=784</span></span><br><span class="line">        real_img = Variable(img).cuda()  <span class="comment"># 将tensor变成Variable放入计算图中</span></span><br><span class="line"></span><br><span class="line">        real_label = Variable(torch.ones(num_img)).cuda()  <span class="comment"># 定义真实的图片label为1</span></span><br><span class="line">        fake_label = Variable(torch.zeros(num_img)).cuda()  <span class="comment"># 定义假的图片的label为0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算真实图片的损失</span></span><br><span class="line">        real_out = D(real_img)  <span class="comment"># 将真实图片放入判别器</span></span><br><span class="line">        d_loss_real = criterion(real_out, real_label)   <span class="comment"># 得到真实图片的label为1</span></span><br><span class="line">        real_scores = real_out  <span class="comment"># 得到真实图片的判别值，输出的值越接近1越好</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算假图片的损失</span></span><br><span class="line">        z = Variable(torch.randn(num_img, z_dimension)).cuda()  <span class="comment"># 随机生成噪声</span></span><br><span class="line">        fake_img = G(z)  <span class="comment"># 随机噪声放入生成网络中，生成一张假的图片</span></span><br><span class="line">        fake_out = D(fake_img)  <span class="comment"># 判别器判断假的图片</span></span><br><span class="line">        d_loss_fake = criterion(fake_out, fake_label)  <span class="comment"># 得到假的图片的loss</span></span><br><span class="line">        fake_scores = fake_out  <span class="comment"># 得到假图片的判别值，对于判别器来说，假图片的损失越接近0越好</span></span><br><span class="line"></span><br><span class="line">        d_loss = d_loss_real + d_loss_fake <span class="comment">#损失包括判真损失和判假损失</span></span><br><span class="line">        dis_optimizer.zero_grad()  <span class="comment"># 在反向传播之前，先将梯度归0</span></span><br><span class="line">        d_loss.backward()  <span class="comment"># 将误差反向传播</span></span><br><span class="line">        dis_optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ==================训练生成器============================</span></span><br><span class="line">        <span class="comment"># 原理：目的是希望生成的假的图片被判别器判断为真的图片，</span></span><br><span class="line">        <span class="comment"># 在此过程中，将判别器固定，将假的图片传入判别器的结果与真实的label对应，</span></span><br><span class="line">        <span class="comment"># 反向传播更新的参数是生成网络里面的参数，</span></span><br><span class="line">        <span class="comment"># 这样可以通过更新生成网络里面的参数，来训练网络，使得生成的图片让判别器以为是真的</span></span><br><span class="line">        <span class="comment"># 这样就达到了对抗的目的</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算假的图片的损失</span></span><br><span class="line">        z = Variable(torch.randn(num_img, z_dimension)).cuda()  <span class="comment"># 得到随机噪声</span></span><br><span class="line">        fake_img = G(z) <span class="comment">#随机噪声输入到生成器中，得到一副假的图片</span></span><br><span class="line">        output = D(fake_img)  <span class="comment"># 经过判别器得到的结果</span></span><br><span class="line">        g_loss = criterion(output, real_label)  <span class="comment"># 得到的假的图片与真实的图片的label的loss</span></span><br><span class="line"></span><br><span class="line">        gen_optimizer.zero_grad()</span><br><span class="line">        g_loss.backward()</span><br><span class="line">        gen_optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>)%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch[&#123;&#125;/&#123;&#125;], d_loss:&#123;:.6f&#125;, g_loss:&#123;:.6f&#125;, D real: &#123;:.6f&#125;, D fake: &#123;:.6f&#125;'</span></span><br><span class="line">                  .format(epoch, num_epoch, d_loss.item(), g_loss.item(),</span><br><span class="line">                          real_scores.mean().item(), fake_scores.mean().item()  <span class="comment">#打印的是真实图片的损失均值</span></span><br><span class="line">            ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch==<span class="number">0</span>:</span><br><span class="line">            real_images=to_img(real_img.cpu().data)</span><br><span class="line">            save_image(real_images, <span class="string">'./img/real_images.png'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        fake_images = to_img(real_img.cpu().data)</span><br><span class="line">        save_image(fake_images, <span class="string">'./img/fake_images-&#123;&#125;.png'</span>.format(epoch+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">torch.save(G.state_dict(),<span class="string">'./generator.pth'</span>)</span><br><span class="line">torch.save(D.state_dict(),<span class="string">'./discriminator.pth'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;用pytorch实现简单的GAN生成对抗网络。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="Technical guide" scheme="https://zeyuxiao1997.github.io/categories/Technical-guide/"/>
    
    
      <category term="GAN" scheme="https://zeyuxiao1997.github.io/tags/GAN/"/>
    
      <category term="pytorch" scheme="https://zeyuxiao1997.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅对抗生成网络笔记(2018)</title>
    <link href="https://zeyuxiao1997.github.io/2019/08/06/hongyili-gan/"/>
    <id>https://zeyuxiao1997.github.io/2019/08/06/hongyili-gan/</id>
    <published>2019-08-06T02:21:09.000Z</published>
    <updated>2019-08-06T04:42:05.031Z</updated>
    
    <content type="html"><![CDATA[<p>李宏毅对抗生成网络(GAN)国语教程(2018)，来源B站。<a href="https://www.bilibili.com/video/av24011528?from=search&amp;seid=8921944723648210067" target="_blank" rel="noopener">链接</a></p><h1 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h1><img src="/2019/08/06/hongyili-gan/1.png" width="1"><img src="/2019/08/06/hongyili-gan/2.png" width="2"><img src="/2019/08/06/hongyili-gan/3.png" width="3"><img src="/2019/08/06/hongyili-gan/4.png" width="4"><img src="/2019/08/06/hongyili-gan/5.png" width="5">]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;李宏毅对抗生成网络(GAN)国语教程(2018)，来源B站。&lt;a href=&quot;https://www.bilibili.com/video/av24011528?from=search&amp;amp;seid=8921944723648210067&quot; target=&quot;_blank
      
    
    </summary>
    
      <category term="course" scheme="https://zeyuxiao1997.github.io/categories/course/"/>
    
    
      <category term="GAN" scheme="https://zeyuxiao1997.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>paper-4D Light Field Segmentation with Spatial and Angular Consistencies</title>
    <link href="https://zeyuxiao1997.github.io/2019/08/04/paper-LFseg/"/>
    <id>https://zeyuxiao1997.github.io/2019/08/04/paper-LFseg/</id>
    <published>2019-08-04T13:43:01.000Z</published>
    <updated>2019-08-04T14:17:24.479Z</updated>
    
    <content type="html"><![CDATA[<p>ICCP2016的文章，应该是比较早做LF分割的，没有用到什么让人眼前一亮的方法，全部是基于机器学习。</p><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><ul><li><p>使用多视角光场表示法对光场建模(这个我也是第一次看到)。</p><img src="/2019/08/04/paper-LFseg/1.png" width="1"></li><li><p>使用SVM对亮度、深度进行特征向量分类，感觉还是挺6的。</p><img src="/2019/08/04/paper-LFseg/2.png" width="2"></li><li><p>考虑angular连续和spatial连续。</p><img src="/2019/08/04/paper-LFseg/3.png" width="3"></li></ul><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li><p>用一个end-to-end结构进行光场分割</p></li><li><p>光场编辑是不是可以先分割再编辑？比如文中提出的application</p><img src="/2019/08/04/paper-LFseg/4.png" width="4"></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ICCP2016的文章，应该是比较早做LF分割的，没有用到什么让人眼前一亮的方法，全部是基于机器学习。&lt;/p&gt;
&lt;h2 id=&quot;细节&quot;&gt;&lt;a href=&quot;#细节&quot; class=&quot;headerlink&quot; title=&quot;细节&quot;&gt;&lt;/a&gt;细节&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;使用
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="multi-view" scheme="https://zeyuxiao1997.github.io/tags/multi-view/"/>
    
      <category term="reconstruction" scheme="https://zeyuxiao1997.github.io/tags/reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>paper：DeepStereo：Learning to Predict New Views from the World’s Imagery</title>
    <link href="https://zeyuxiao1997.github.io/2019/08/04/paper-DRC/"/>
    <id>https://zeyuxiao1997.github.io/2019/08/04/paper-DRC/</id>
    <published>2019-08-04T01:59:29.000Z</published>
    <updated>2019-08-04T07:00:27.977Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2016的作品，源自google。文章提出一种新的使用CNN的直接从pixel生成new view的方法，端到端，其中的<strong>plane sweep volume</strong>方法对我很有启发。</p><h2 id="文章工作"><a href="#文章工作" class="headerlink" title="文章工作"></a>文章工作</h2><h3 id="plane-sweep-volumes"><a href="#plane-sweep-volumes" class="headerlink" title="plane-sweep volumes"></a>plane-sweep volumes</h3><p>文章中最精彩的部分莫过于此。将视图合成问题分解成不同深度上的pixel组合问题，使用PSV进行不同深度平面的分离与合成。<br><img src="/2019/08/04/paper-DRC/1.png" width="1"><br>细节这里不再赘述。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>分别提出两个子网络：selection tower(用于描述深度，具体说是输出图片每个像素所在的深度值)和color tower(用于描述颜色，具体说是输出图片像素的颜色，需要通过网络学习组合权重)<br><img src="/2019/08/04/paper-DRC/2.png" width="2"></p><h3 id="multi-resolution-patches"><a href="#multi-resolution-patches" class="headerlink" title="multi-resolution patches"></a>multi-resolution patches</h3><p>使用多scale的网络结构学习上下文匹配信息。【为了更好的在不同尺度上提取上下文信息，在立体匹配和重建任务中是常用的操作，PSMNet网络结构中的金字塔式的尺度学习是一个很好的结构】<br><img src="/2019/08/04/paper-DRC/3.png" width="3"></p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li><p>siggraph2018年的multi-plane image工作与本文类似，更确切的说的使用了文中的PSV结构，多平面的思想是不是可以用在inpainting、SR中呢？？</p></li><li><p>怎么样优化深度平面？如果平面层数多了，disparity resolution变大，准确性一定会变高，但是怎么样平衡参数量和计算量？文中在展望部分提出使用RNN进行深度估计操作，应该怎么样去做？</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;CVPR2016的作品，源自google。文章提出一种新的使用CNN的直接从pixel生成new view的方法，端到端，其中的&lt;strong&gt;plane sweep volume&lt;/strong&gt;方法对我很有启发。&lt;/p&gt;
&lt;h2 id=&quot;文章工作&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="multi-view" scheme="https://zeyuxiao1997.github.io/tags/multi-view/"/>
    
      <category term="reconstruction" scheme="https://zeyuxiao1997.github.io/tags/reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>图像修复(inpainting)调研</title>
    <link href="https://zeyuxiao1997.github.io/2019/08/03/survey-inpainting/"/>
    <id>https://zeyuxiao1997.github.io/2019/08/03/survey-inpainting/</id>
    <published>2019-08-03T06:29:07.000Z</published>
    <updated>2019-08-05T02:26:48.475Z</updated>
    
    <content type="html"><![CDATA[<p>inpainting方向的调研。先mark一下，后面慢慢填坑。。。</p><p>在知乎上面找了这么一个链接，非常不错<a href="https://www.zhihu.com/question/56801298" target="_blank" rel="noopener">https://www.zhihu.com/question/56801298</a>和<a href="https://www.zhihu.com/question/265840229/answer/302510840" target="_blank" rel="noopener">https://www.zhihu.com/question/265840229/answer/302510840</a>，先mark一下，后面慢慢看。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;inpainting方向的调研。先mark一下，后面慢慢填坑。。。&lt;/p&gt;
&lt;p&gt;在知乎上面找了这么一个链接，非常不错&lt;a href=&quot;https://www.zhihu.com/question/56801298&quot; target=&quot;_blank&quot; rel=&quot;noopene
      
    
    </summary>
    
      <category term="survey" scheme="https://zeyuxiao1997.github.io/categories/survey/"/>
    
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="inpainting" scheme="https://zeyuxiao1997.github.io/tags/inpainting/"/>
    
  </entry>
  
  <entry>
    <title>paper：Breaking the Spatio-Angular Trade-off for Light Field Super-Resolution via LSTM Modelling on Epipolar Plane Images</title>
    <link href="https://zeyuxiao1997.github.io/2019/08/03/paper-LSTMSR/"/>
    <id>https://zeyuxiao1997.github.io/2019/08/03/paper-LSTMSR/</id>
    <published>2019-08-03T04:04:07.000Z</published>
    <updated>2019-08-04T07:01:14.034Z</updated>
    
    <content type="html"><![CDATA[<p>一篇arxiv上面巨水无比的论文，唯一的亮点在于引入了LSTM进行光场超分辨率，并且是在angular和spatial上进行联合SR。</p><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><p>直接参考师兄的<a href="https://note.youdao.com/ynoteshare1/index.html?id=2e9ffad158a67e63bd1d77be85a954cb&amp;type=notebook#/WEBe0f648defdf017135c1132386877a954" target="_blank" rel="noopener">笔记</a>就好了。反正东西就是差不多的。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li><p>文中的LSTM到底是咋用的？到底他的序列是怎么用的？</p></li><li><p>文中启发了一个很好的点：在LF做数据扩充任务时，不能随意对EPI进行filp操作。</p></li><li><p>自己搞了一个数据集（生成了有遮挡、大baseline的数据进行操作），来源的链接为<a href="http://www.povray.org/" target="_blank" rel="noopener">http://www.povray.org/</a>和<a href="http://www.oyonale.com/" target="_blank" rel="noopener">http:<br>//www.oyonale.com/</a>，以后可以搞一下（if possible and necessary）</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一篇arxiv上面巨水无比的论文，唯一的亮点在于引入了LSTM进行光场超分辨率，并且是在angular和spatial上进行联合SR。&lt;/p&gt;
&lt;h2 id=&quot;细节&quot;&gt;&lt;a href=&quot;#细节&quot; class=&quot;headerlink&quot; title=&quot;细节&quot;&gt;&lt;/a&gt;细节&lt;/h
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="light field" scheme="https://zeyuxiao1997.github.io/tags/light-field/"/>
    
      <category term="SR" scheme="https://zeyuxiao1997.github.io/tags/SR/"/>
    
      <category term="LSTM" scheme="https://zeyuxiao1997.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>paper-Unsupervised Learning of Depth and Ego-Motion from Video</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/31/paper-SFMlearn/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/31/paper-SFMlearn/</id>
    <published>2019-07-31T11:30:19.000Z</published>
    <updated>2019-08-01T08:23:45.859Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2017年的工作。网上有几个很好的中文博客，梳理了这篇论文的工作。可以通过<a href="https://blog.csdn.net/kevin_cc98/article/details/78957218" target="_blank" rel="noopener">链接1</a>和<a href="https://www.jianshu.com/p/7283ed894c84" target="_blank" rel="noopener">链接二</a>访问。</p><p>文章是第一篇使用SFM(structure-from-motion)进行深度估计的，后来者在本文的工作上通过增加场景集合的约束、sourse数据的约束，进一步提高估计的准确性和鲁棒性。通俗来说，本文是<strong>从一段视频中恢复场景深度和相机pose的论文</strong>。</p><h2 id="细节描述"><a href="#细节描述" class="headerlink" title="细节描述"></a>细节描述</h2><h3 id="network-structure"><a href="#network-structure" class="headerlink" title="network structure"></a>network structure</h3><img src="/2019/07/31/paper-SFMlearn/1.png" width="1"><h3 id="supervision-pipeline"><a href="#supervision-pipeline" class="headerlink" title="supervision pipeline"></a>supervision pipeline</h3><img src="/2019/07/31/paper-SFMlearn/2.png" width="2"><p>用depth和pose网络分别输出的predicted D和T，弄出一个Loss</p><h3 id="双线性插值、warping"><a href="#双线性插值、warping" class="headerlink" title="双线性插值、warping"></a>双线性插值、warping</h3><img src="/2019/07/31/paper-SFMlearn/3.png" width="3"><p>对于target图中的某位置pt，用predicted D 和 T，算出该点对应到source图中是在ps这个位置（因为不一定是在整像素点，所以需要用双线性插值算出该ps点在source上的真实像素值）。对于所有target和source对，对于每个source中的每个像素点，计算像素值之差的和：</p><script type="math/tex; mode=display">\mathcal{L}_{v s}=\sum_{s} \sum_{p}\left|I_{t}(p)-\hat{I}_{s}(p)\right|</script><ul><li><p>考虑到环境不理想的情况（运动物体、相机运动模糊）,加一个置信参数：</p><script type="math/tex; mode=display">\mathcal{L}_{v s}=\sum_{<I_{1}, \ldots, I_{N}>\in \mathcal{S}} \sum_{p} \hat{E}_{s}(p)\left|I_{t}(p)-\hat{I}_{s}(p)\right|</script><p>但这样的话minimize loss就直接让E为0就可以了。为了避免这种现象，最终的loss要加一个关于E的项，to encourage nonzero predictions by 最小化每个像素点与1之间的交叉熵损失。</p></li><li><script type="math/tex; mode=display">\mathcal{L}_{f i n a l}=\sum_{l} \mathcal{L}_{v s}^{l}+\lambda_{s} \mathcal{L}_{s m o o t h}^{l}+\lambda_{e} \sum_{s} \mathcal{L}_{r e g}\left(\hat{E}_{s}^{l}\right)</script></li></ul><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><img src="/2019/07/31/paper-SFMlearn/4.png" width="4"><p>如图，可见该网络不如Godard的左右一致性检查一文的结果。作者认为原因可能是由于Godard所谓的左右一致性本质上是已知两帧之间的pose，而当前的深度预测是基于预测的不精确的pose。</p><p>不过要想到，本文的深度其实是一个中间值（intermediate value），它的主要公用是用于pose估计。此外，作者提到他们的解释网络没有很好地表现，可能是因为测试集中图像大多是静态的图，中没有太多遮挡的情况（遮挡的情况大多3帧内就会消失)，但发现他们的解释网络具有良好的预测图像动态部分能力。</p><p>作者最后认为当前还可以从以下几个方面进步：</p><ul><li><p>没有考虑实际环境中可能出现的物体移动和遮挡。当然作者提到可以借鉴 motion segmentation的工作成果。</p></li><li><p>目前还需要内参已知，如果利用往上随处可获得的video来估计（内参不知）？</p></li><li><p>用更加好的深度预测（我认为这也是pose估计一个很核心的任务。有了更好的三维点，就有更小的重投影误差），比如利用体素</p></li></ul><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li><p>一篇论文应该怎么思考？现在难道不是根据xxx场景想一堆idea，然后把idea变成loss或者是xx模块，然后测试不同网络、设计不同网络暴力让他收敛吗？？？？？</p></li><li><p>除了MPI和depth，还有什么好的描述3D场景的方式呢？体素会不会是一个好的出发点呢？</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;CVPR2017年的工作。网上有几个很好的中文博客，梳理了这篇论文的工作。可以通过&lt;a href=&quot;https://blog.csdn.net/kevin_cc98/article/details/78957218&quot; target=&quot;_blank&quot; rel=&quot;noopene
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="video" scheme="https://zeyuxiao1997.github.io/tags/video/"/>
    
      <category term="depth estimation" scheme="https://zeyuxiao1997.github.io/tags/depth-estimation/"/>
    
      <category term="SFM" scheme="https://zeyuxiao1997.github.io/tags/SFM/"/>
    
  </entry>
  
  <entry>
    <title>paper-Learning Depth from Monocular Videos using Direct Methods</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/31/paper-LKVOLearner/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/31/paper-LKVOLearner/</id>
    <published>2019-07-31T07:50:14.000Z</published>
    <updated>2019-08-01T08:23:49.342Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2018的作品，基于视频帧的深度估计；是unsupervised的，但是有预训练模型。</p><p>CSDN上有两个链接可以看到<a href="https://blog.csdn.net/xiao_lxl/article/details/90242005" target="_blank" rel="noopener">翻译</a>和<a href="https://blog.csdn.net/yueleileili/article/details/82946910" target="_blank" rel="noopener">讲解</a>，知乎上面也有对这篇论文的<a href="https://zhuanlan.zhihu.com/p/50351565" target="_blank" rel="noopener">讲解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;CVPR2018的作品，基于视频帧的深度估计；是unsupervised的，但是有预训练模型。&lt;/p&gt;
&lt;p&gt;CSDN上有两个链接可以看到&lt;a href=&quot;https://blog.csdn.net/xiao_lxl/article/details/90242005&quot; ta
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="video" scheme="https://zeyuxiao1997.github.io/tags/video/"/>
    
      <category term="depth estimation" scheme="https://zeyuxiao1997.github.io/tags/depth-estimation/"/>
    
      <category term="monocular" scheme="https://zeyuxiao1997.github.io/tags/monocular/"/>
    
  </entry>
  
  <entry>
    <title>paper-Learning to Synthesize a 4D RGBD Light Field from a Single Image</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/31/paper-RGBD2LF/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/31/paper-RGBD2LF/</id>
    <published>2019-07-31T01:24:09.000Z</published>
    <updated>2019-07-31T03:38:51.159Z</updated>
    
    <content type="html"><![CDATA[<p>ICCV2017的作品，oral视频在网上可以找到，(<a href="http://www.sohu.com/a/258551424_715754)[http://www.sohu.com/a/258551424_715754]，这篇文章提出一种机器学习算法，将2D" target="_blank" rel="noopener">http://www.sohu.com/a/258551424_715754)[http://www.sohu.com/a/258551424_715754]，这篇文章提出一种机器学习算法，将2D</a> RGB图像作为输入合成4D RGBD光场（每个光线方向上包含场景的颜色和深度）。</p><h2 id="细节描述"><a href="#细节描述" class="headerlink" title="细节描述"></a>细节描述</h2><p>文章的核心思想是通过一个RGB图片，提取图片中的深度信息，用这些深度信息合成光场。由于存在遮挡和非朗伯平面的问题，作者使用two-step的方法合成光场，这个two-step可以看做是先使用深度信息合成一个粗光场，再通过残差的方式refine之前的粗光场，用公式表示是这样的：</p><script type="math/tex; mode=display">\begin{aligned} D(\mathbf{x}, \mathbf{u}) &=d(L(\mathbf{x}, \mathbf{0})) \\ L_{r}(\mathbf{x}, \mathbf{u}) &=r(L(\mathbf{x}, \mathbf{0}), D(\mathbf{x}, \mathbf{u})) \\ \hat{L}(\mathbf{x}, \mathbf{u}) &=o\left(L_{r}(\mathbf{x}, \mathbf{u}), D(\mathbf{x}, \mathbf{u})\right) \end{aligned}</script><p>论文的pipeline如下图所示。<br><img src="/2019/07/31/paper-RGBD2LF/1.png" width="1"></p><h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><p>自己采集的花花草草的光场数据集，然后用central view作为input，我觉得意义并不大，应该是使用external信息进行暴力学习，使网络过拟合，从而提高泛化性。这一点文中提到了一句：网络只能用于相机baseline的光场合成。</p><h3 id="depth-estimation"><a href="#depth-estimation" class="headerlink" title="depth estimation"></a>depth estimation</h3><p>深度估计是这个文章的重点。文中将深度信息的表示转换成<strong>光线的深度</strong>，并使用“深度一致性”的概念进行loss的设计，还是很有新意的；包括里面用到了TV-loss，都是让人眼前一亮的。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li><p>关于refine的过程是不是可以不用resblock进行操作，而改用类似于DRCN网络中的递归结构呢？</p></li><li><p>新view的合成有两个途径，一个是copy from existed pixels，另一个是imagine/generate new pixels，既然我是有监督的LF，能不能使用GAN，设计一个生成模型去产生光场呢？</p></li><li><p>如果没有ground truth时，有什么办法去生成一个光场；或者说是怎么样从单帧生成多帧；或者是怎样从single view to multi view呢？</p></li><li><p>本文使用深度一致性进行scene representation，除了之前看的MPI之外，还有什么场景表征的方法呢？</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ICCV2017的作品，oral视频在网上可以找到，(&lt;a href=&quot;http://www.sohu.com/a/258551424_715754)[http://www.sohu.com/a/258551424_715754]，这篇文章提出一种机器学习算法，将2D&quot; t
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="light field" scheme="https://zeyuxiao1997.github.io/tags/light-field/"/>
    
      <category term="synthesis" scheme="https://zeyuxiao1997.github.io/tags/synthesis/"/>
    
      <category term="RGBD" scheme="https://zeyuxiao1997.github.io/tags/RGBD/"/>
    
  </entry>
  
  <entry>
    <title>paper-Stereo Magnification：Learning view synthesis using multiplane images</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/30/paper-MPI/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/30/paper-MPI/</id>
    <published>2019-07-30T06:00:08.000Z</published>
    <updated>2019-07-30T13:56:55.853Z</updated>
    
    <content type="html"><![CDATA[<p>SIGGRAPH2018的作品。可以将文章的task理解为，双目立体放大，即通过已知相机位置和其他参数的两幅stereo图片外插生成更大baseline的视图。使用深度神经网络学习并构建双目立体相机拍摄的两幅图像的多平面图像表示（multiplane images），然后基于此合成立体感更明显的立体图像。</p><p>立体放大技术即将基线距较近的双目相机拍摄的两幅图像，转换成看来起是基线距较远的双目相机拍摄的图像，也就是如下图所示的效果。<br><img src="/2019/07/30/paper-MPI/1.png" width="1"></p><h2 id="细节描述"><a href="#细节描述" class="headerlink" title="细节描述"></a>细节描述</h2><p>我觉得文章有三个大的创新点：</p><ul><li><p>提出MPI（multiplane images），通过MPI将图片展示的场景拆分成可以统一建模的以多个深度和透明度为代表的plane。</p></li><li><p>使用VGG loss(perceptual loss)进行loss function的学习。</p></li><li><p>将YouTube上面的视频通过SLAM的方法标记成适合本文所用的datasets。</p></li></ul><p>关于MPI，理解可以见下图：<br><img src="/2019/07/30/paper-MPI/2.png" width="2"></p><p>pipeline不是很难理解，放一张图片就好了。<br><img src="/2019/07/30/paper-MPI/3.png" width="3"></p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li><p>文章在最后提出两点他认为可以改进的地方：第一是有强透明和强反光的场景难以进行合成；第二是当边界像素变化特别多时，会产生stacks。见下图。</p><img src="/2019/07/30/paper-MPI/4.png" width="4"></li><li><p>是否还有其他进行scene representation的方法？比如文中一笔带过的layered depth images (LDIs)，是不是可以用在光场外插或者是Video2LF呢？</p></li><li><p>MPI是否可以代替disparity(或者是隐式或者是xx方式)进行几何结构的建模？正好cvpr2019有一篇也是MPI进行view插值，看完之后来回答这个问题</p></li></ul><h2 id="2019-07-30-21-52-08更新"><a href="#2019-07-30-21-52-08更新" class="headerlink" title="2019-07-30 21:52:08更新"></a>2019-07-30 21:52:08更新</h2><p>CVPR2019的论文《Pushing the Boundaries of View Extrapolation with Multiplane Images》是同一个团队里面的人写的，分析采样的思路和SIGGRAPH的一模一样。<br>文中的主要工作是对MPI文章的优化，有下面的三个创新点比较好：</p><ul><li><p>思路清奇，将生成MPI和view合成问题看成inpainting模型【是不是可以往inpainting上靠一靠？？】</p></li><li><p>优化MIP（使用了two-step的优化方法优化MPI）</p></li><li><p>新颖的metric（意义不大，task-oriented）</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;SIGGRAPH2018的作品。可以将文章的task理解为，双目立体放大，即通过已知相机位置和其他参数的两幅stereo图片外插生成更大baseline的视图。使用深度神经网络学习并构建双目立体相机拍摄的两幅图像的多平面图像表示（multiplane images），然后基
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="view synthesis" scheme="https://zeyuxiao1997.github.io/tags/view-synthesis/"/>
    
      <category term="stereo" scheme="https://zeyuxiao1997.github.io/tags/stereo/"/>
    
  </entry>
  
  <entry>
    <title>paper-Light Field Video Capture Using a Learning-Based Hybrid Imaging System</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/29/paper-sig2019/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/29/paper-sig2019/</id>
    <published>2019-07-29T12:19:59.000Z</published>
    <updated>2019-07-31T04:12:54.993Z</updated>
    
    <content type="html"><![CDATA[<p>SIGGRAPH2017的文章，作者是来自UCB和UCSD的大佬。文章讲了什么不重要，在组会上我已经汇报过了，文章重点和脉络可见<a href="/download/xzy_20190729.pdf">pdf</a>。</p><p>重点来了：刘老师从这篇工作中启发我去如何思考一个问题是不是solid、task是不是有意义、工作需不需要去follow</p><h2 id="该如何评估一个工作的价值和是不是值得被follow"><a href="#该如何评估一个工作的价值和是不是值得被follow" class="headerlink" title="该如何评估一个工作的价值和是不是值得被follow"></a>该如何评估一个工作的价值和是不是值得被follow</h2><p>首先能发表在SIGGRAPH上的文章一定是有一定贡献并且有一定创新点的好文章。SIGGRAPH上发表的文章（尤其是前几年，最近两年还好）大多数是做了一个复杂的系统，因此follow与否需要评估是不是值得被follow。本文由于缺乏数据集，因此自己搭建了一套hybrid的系统进行光场视频的采集，从搭建、配准、参数获取，再到网络设计、训练和得出结果，都耗费了大量的精力。并且这个工作从2017年至今都没有人follow，是因为这个课题没有意义还是因为follow成本太高了呢？可想而知。</p><p>换句话说，如果我follow了这篇工作后，通过研究可以发5篇SIGGRAPH，那就是有意义的、可以follow的工作；如果只能发一篇SIGGRAPH，那就不值得follow。</p><h2 id="怎么样去思考一个task是不是好？"><a href="#怎么样去思考一个task是不是好？" class="headerlink" title="怎么样去思考一个task是不是好？"></a>怎么样去思考一个task是不是好？</h2><ul><li><p><strong>首先，这个问题是不是真实存在的、并且解决真的很有意义的？</strong>光场相机因为硬件问题，确实拍摄不了高帧率的视频，并且光场视频在VR中有巨大应用，因此“光场视频生成”是一个很好的task。</p></li><li><p><strong>然后需要明确，follow可能性有多大，是否实用？</strong>见上文。</p></li><li><p><strong>然后看难度是不是大？</strong>光场视频最难的在于数据集，硬件的标定更是头疼，因此并没有必要非做光场视频不可。</p></li></ul><h2 id="怎么样去思考一个idea？"><a href="#怎么样去思考一个idea？" class="headerlink" title="怎么样去思考一个idea？"></a>怎么样去思考一个idea？</h2><p>以光场为例，讲述怎么样去思考idea？</p><p>首先一个好的idea一定是从很高的视角去思考一个领域的，这一点靠看论文是不可能获得的，因此需要多交流、多思考。让我们回顾3D的发展脉络，在3D发展的初期，分化出两个阵营，一个是light field，另一个是point cloud。light field在appearance上有得天独厚的优势，但是具有隐藏的几何信息；相反，点云具有直观的几何信息，但是appearance显示的很差【思考为什么需要两个阵营】。</p><p>在图形学中，render需要极致的appearance，所以我们需要光场去提供丰富的纹理、细节信息。</p><p>在思考一个问题的时候需要明确，还有什么问题没有被解决？然后去思考是否可以follow或者是否可以transfer。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;SIGGRAPH2017的文章，作者是来自UCB和UCSD的大佬。文章讲了什么不重要，在组会上我已经汇报过了，文章重点和脉络可见&lt;a href=&quot;/download/xzy_20190729.pdf&quot;&gt;pdf&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;重点来了：刘老师从这篇工作中启发我去如何
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="light field" scheme="https://zeyuxiao1997.github.io/tags/light-field/"/>
    
      <category term="hybrid system" scheme="https://zeyuxiao1997.github.io/tags/hybrid-system/"/>
    
  </entry>
  
  <entry>
    <title>paper——View Synthesis by Appearance Flow</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/28/paper-appearanceFlow/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/28/paper-appearanceFlow/</id>
    <published>2019-07-28T13:06:58.000Z</published>
    <updated>2019-07-28T13:35:31.803Z</updated>
    
    <content type="html"><![CDATA[<p>ECCV2016的作品，以一个独特的视角看待新视图(新场景)的合成问题—即，所有的新视图都可以在给定的老视图里面找到对应的原pixel。因此本文提出一个Appearance Flow的东西，用于新视图的合成，本质上是【2-D coordinate vectors specifying which pixels in the input view could be used to reconstruct the target view】。</p><h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>视图合成的效果是这样的。<br><img src="/2019/07/28/paper-appearanceFlow/1.png" width="1"></p><p>重要的表示如下：<br><img src="/2019/07/28/paper-appearanceFlow/2.png" width="2"></p><p>网络结构是两个encoder加一个decoder。<br><img src="/2019/07/28/paper-appearanceFlow/3.png" width="3"></p><p>文章的一个创新在于：可使用多视图输入进行合成，它本质上就是在上面的网络上加了confidence mask，并对其取平均数，得到新的视图。<br><img src="/2019/07/28/paper-appearanceFlow/4.png" width="4"></p><h2 id="灵感"><a href="#灵感" class="headerlink" title="灵感"></a>灵感</h2><ul><li><p>颜色只能是出现在图中的已有颜色，是否可以生成已知图片中没有的颜色，但是在真实图片中是实际存在的颜色呢？</p></li><li><p>数据集有限，并且知道了类别；是否可以在不知道类别的情况下进行合成。</p></li><li><p><strong>借用appearance flow是否能和现在光场视频中合成问题相结合？？？？？</strong>（需仔细思考一下。。。。。。）</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ECCV2016的作品，以一个独特的视角看待新视图(新场景)的合成问题—即，所有的新视图都可以在给定的老视图里面找到对应的原pixel。因此本文提出一个Appearance Flow的东西，用于新视图的合成，本质上是【2-D coordinate vectors speci
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="video" scheme="https://zeyuxiao1997.github.io/tags/video/"/>
    
      <category term="view synthesis" scheme="https://zeyuxiao1997.github.io/tags/view-synthesis/"/>
    
  </entry>
  
  <entry>
    <title>Important network structure——Dilated Convolutions(空洞卷积)</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/26/class-unet/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/26/class-unet/</id>
    <published>2019-07-26T05:53:04.000Z</published>
    <updated>2019-08-01T01:43:39.533Z</updated>
    
    <content type="html"><![CDATA[<p>在view synthesis中和light field重建过程中遇到很多次Dilated conv，但是对其原理不是很明白，不知道它为什么可以用于特征提取和表示。因此本文将记录我对空洞卷积的学习和理解。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在view synthesis中和light field重建过程中遇到很多次Dilated conv，但是对其原理不是很明白，不知道它为什么可以用于特征提取和表示。因此本文将记录我对空洞卷积的学习和理解。&lt;/p&gt;

      
    
    </summary>
    
      <category term="Important network structure" scheme="https://zeyuxiao1997.github.io/categories/Important-network-structure/"/>
    
    
      <category term="CNN" scheme="https://zeyuxiao1997.github.io/tags/CNN/"/>
    
      <category term="Dilated Convolutions" scheme="https://zeyuxiao1997.github.io/tags/Dilated-Convolutions/"/>
    
  </entry>
  
  <entry>
    <title>paper——Learning-Based View Synthesis for Light Field Cameras</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/24/paper-VSLF/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/24/paper-VSLF/</id>
    <published>2019-07-24T02:11:38.000Z</published>
    <updated>2019-07-28T06:33:34.590Z</updated>
    
    <content type="html"><![CDATA[<p>TOG2016的工作。近两年大量的文献均指出这一篇文章的弊端（包括但不限于深度估计模块和颜色估计模块并不能很好的挖掘光场的结构信息），故读之。<br><a id="more"></a></p><p>这篇文章主要讲述利用机器学习来合成光场中的虚拟视点。</p><p>输入为光场相机的四个角落的视点，合成新的视点。方法在细节方面保持不错。</p><p>主要步骤如下：<br><img src="/2019/07/24/paper-VSLF/1.png" width="1"><br>先提取图像特征，利用这些特征来计算DISPARITY, 将其disparity和原始image 一起输入新的网络，来合成新的视点。</p><img src="/2019/07/24/paper-VSLF/2.png" width="2"><p>网络结构相对而言比较简单。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TOG2016的工作。近两年大量的文献均指出这一篇文章的弊端（包括但不限于深度估计模块和颜色估计模块并不能很好的挖掘光场的结构信息），故读之。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="light field" scheme="https://zeyuxiao1997.github.io/tags/light-field/"/>
    
      <category term="Angular SR" scheme="https://zeyuxiao1997.github.io/tags/Angular-SR/"/>
    
  </entry>
  
  <entry>
    <title>paper——Light Field Reconstruction Using Deep Convolutional Network on EPI</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/24/paper-cnnEPI/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/24/paper-cnnEPI/</id>
    <published>2019-07-24T01:41:57.000Z</published>
    <updated>2019-07-25T05:53:22.598Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2017的工作。近两年大量的文献均指出这一篇文章的弊端（包括但不限于blur-restoration-deblur反复使用增加计算成本、2D的EPI减少了光场的信息维度等），故读之。<br><a id="more"></a><br>文章的重点在于重建以EPI为代表的角分辨率，因此所有的重建是针对EPI的(虽然是2D slices，但是在两年前已经是很好的结果了)。角分辨率和空间分辨率的不一致性导致直接进行angular SR会有很大的伪影，视觉效果不连续【angular resolution不行，但是spatial resolution保持正常】；为了平衡这种angular和spatial的不一致性，需要降低spatial的分辨率。</p><h2 id="文章工作"><a href="#文章工作" class="headerlink" title="文章工作"></a>文章工作</h2><p>文章发现直接对EPI进行upsampling会出现不好的情况，需要使用blur去除高频率信息，然后使用angular SR方法然后再deblur。方法见图。<br><img src="/2019/07/24/paper-cnnEPI/1.png" title="network"><br>使用bicubic插值将已经模糊的EPI插值成角空间中理想的resolution，然后使用残差卷积进行细致特征的提取，然后再使用deblur的方法去模糊。</p><p>想法不难，但是我需要更进一步了解，EPI和angular的关系。</p><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><ul><li><p>文中提出的blur-deblur的原因是什么？（需要再深入阅读）blur-deblur模式是不是只适用于EPI的2D slices，能否进行维度扩展，方向扩展等，比如结合voxel进行操作？</p></li><li><p>文中提出的高低频部分不是很明白，光场的高低频信息是否可以进一步引导angular SR或spatial SR甚至是其他的low-level的问题？</p></li><li><p>只用亮度信息真的不会损失任何有效信息吗，其他通道该怎么样加以利用？</p></li><li><p>换用更新的denoise方法会不会单纯在本文的结果上增加结果？</p></li><li><p>大家似乎都是用的内插，外插行吗？</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CVPR2017的工作。近两年大量的文献均指出这一篇文章的弊端（包括但不限于blur-restoration-deblur反复使用增加计算成本、2D的EPI减少了光场的信息维度等），故读之。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="light field" scheme="https://zeyuxiao1997.github.io/tags/light-field/"/>
    
      <category term="Angular SR" scheme="https://zeyuxiao1997.github.io/tags/Angular-SR/"/>
    
  </entry>
  
  <entry>
    <title>研学过程中的零碎知识点</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/18/knowledge/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/18/knowledge/</id>
    <published>2019-07-18T00:43:58.000Z</published>
    <updated>2019-08-06T12:48:54.806Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是图像的高频信息和低频信息？"><a href="#什么是图像的高频信息和低频信息？" class="headerlink" title="什么是图像的高频信息和低频信息？"></a>什么是图像的高频信息和低频信息？</h2><p>低频信号和高频信号也叫低频分量和高频分量。图像中的高频分量，指的是图像强度（亮度/灰度）变化剧烈的地方，也就是我们常说的边缘（轮廓）；图像中的低频分量，指的是图像强度（亮度/灰度）变换平缓的地方，也就是大片色块的地方。人眼对图像中的高频信号更为敏感，举个例子，在一张白纸上有一行字，那么我们肯定直接聚焦在文字上，而不会太在意白纸本身，这里文字就是高频信号，而白纸就是低频信号。</p><p>图像的高低频是对图像各个位置之间强度变化的一种度量方法。低频分量：主要对整副图像的强度的综合度量。高频分量：主要是对图像边缘和轮廓的度量。如果一副图像的各个位置的强度大小相等，则图像只存在低频分量,从图像的频谱图上看,只有一个主峰，且位于频率为零的位置。 如果一副图像的各个位置的强度变化剧烈，则图像不仅存在低频分量，同时也存在多种高频分量，从图像的频谱上看，不仅有一个主峰，同时也存在多个旁峰。</p><p>图像边缘的灰度值变化快，就对应着频率高，即高频显示图像边缘。图像的细节处也就是属于灰度值急剧变化的区域，正是因为灰度值的急剧变化，才会出现细节。另外<strong>噪声（即噪点）</strong>也是这样，在一个像素所在的位置，之所以是噪点，是因为它与正常的点颜色不一样了，也就是说该像素点灰度值明显不一样，当然也就是灰度有快速地变化了，所以是高频部分，因此有噪声在高频这一说。</p><p><strong>其实归根结底，是因为我们人眼识别物体就是这样的，假如你穿一个红衣服在红色背景布前拍照，你能很好地识别么？</strong>不能，因为衣服与背景融为一体，没有变化，所以看不出，除非有灯光从某角度照在人物身上，这样边缘处会出现高亮和阴影，这样我们就能看到一些轮廓线，这些就是颜色（即灰度）很不一样的地方。</p><h2 id="depth-of-field景深"><a href="#depth-of-field景深" class="headerlink" title="depth-of-field景深"></a>depth-of-field景深</h2><p>指相机对焦点前后相对清晰的成像范围。<br><a href="http://www3.xitek.com/xuetang/optics/depthoffield.htm" target="_blank" rel="noopener">参考链接</a></p><h2 id="LSTM的原理及其应用"><a href="#LSTM的原理及其应用" class="headerlink" title="LSTM的原理及其应用"></a>LSTM的原理及其应用</h2><h2 id="TV-loss及其实现"><a href="#TV-loss及其实现" class="headerlink" title="TV loss及其实现"></a>TV loss及其实现</h2><h2 id="什么是superpixel-超像素"><a href="#什么是superpixel-超像素" class="headerlink" title="什么是superpixel(超像素)"></a>什么是superpixel(超像素)</h2><p>知乎<a href="https://www.zhihu.com/question/27623988" target="_blank" rel="noopener">链接</a></p><h3 id="超像素"><a href="#超像素" class="headerlink" title="超像素"></a>超像素</h3><p>首先一张图片由一个个像素组成（可以看成网格），每个像素可以有一个灰度值（标量）或RGB值（三维向量）。首先一张图片由一个个像素组成（可以看成网格），每个像素可以有一个灰度值（标量）或RGB值（三维向量）。现今一张图片动辄1000<em>1000=100w像素，因此对于图像处理来说，是非常大的维度。超像素最大的功能之一，便是作为图像处理其他算法的预处理，在不牺牲太大精确度的情况下<em>*降维</em></em>！<br><img src="/2019/07/18/knowledge/1.png" width="1"></p><h3 id="超像素算法"><a href="#超像素算法" class="headerlink" title="超像素算法"></a>超像素算法</h3><p>超像素最直观的解释，便是把一些具有相似特性的像素“聚合”起来，形成一个更具有代表性的大“元素”。而这个新的元素，将作为其他图像处理算法的基本单位。一来大大降低了维度；二来可以剔除一些异常像素点。至于根据什么特性把一个个像素点聚集起来，可以是颜色、纹理、类别等。<br><img src="/2019/07/18/knowledge/2.png" width="2"></p><p>理论上，任何图像分割算法的过度分割(over-segmentation)，即可生成超像素。下面是一个图像分割算法的例子（举此例还因为这里分割标准是依据纹理）。<br><img src="/2019/07/18/knowledge/3.png" width="3"></p><h3 id="超像素算法判别条件"><a href="#超像素算法判别条件" class="headerlink" title="超像素算法判别条件"></a>超像素算法判别条件</h3><ul><li>Undersegmentation Error</li></ul><p>下图，白色是原图的一个物体，红线是一个个超像素的轮廓，而粉红色的区域就是undersegmentation的区域。显然，这部分区域越大就越不好。<br><img src="/2019/07/18/knowledge/4.png" width="4"></p><ul><li><p>Boundary Recall<br>下图，黑色虚线以及实现是原图物体的轮廓，红线是超像素的边界。一个好的超像素算法，应该覆盖原图物体的轮廓。在给予一定缓冲（粉红色区域）的情况下，超像素的边缘可以覆盖原图物体边缘的越多（黑色实线），该算法就越好。</p><img src="/2019/07/18/knowledge/5.png" width="5"></li><li><p>Compactness score<br>这个指标衡量了一个超像素是否“紧实”。</p><img src="/2019/07/18/knowledge/6.png" width="6"></li></ul><h2 id="极线约束-epipolar-constraint"><a href="#极线约束-epipolar-constraint" class="headerlink" title="极线约束(epipolar constraint)"></a>极线约束(epipolar constraint)</h2><p>三维空间中一点p，投影到两个不同的平面I1、I2，投影点分别为p1，p2。p、p1、p2在三维空间内构成一个平面S。S与面I1的交线L1过p1点，称之为对应于p2的极线。同理S与I2交线称之为对应于p1的极线（对应于左边图像点的极线在右边图像上，右边与之相同）。如图：<br><img src="/2019/07/18/knowledge/7.png" width="7"></p><p>所谓极线约束就是说同一个点在两幅图像上的映射，已知左图映射点p1，那么右图映射点p2一定在相对于p1的极线上，这样可以减少待匹配的点数量。</p><!-- ## 结构张量### 什么是结构张量结构张量技术可以很好地将结构信息突出的部分和结构信息微弱的部分进行区分， 例如区分图像中边缘轮廓等细节与平坦光滑的部分，这样可以大大地方便类似医学图像等对边缘保留要求比较高的图像去噪 --><h2 id="隐变量"><a href="#隐变量" class="headerlink" title="隐变量"></a>隐变量</h2><p>加入我们有X，Y两个随机变量，他们的概率分布如下。要直接用一个函数还表示这个分布是比较困难的。<br><img src="/2019/07/18/knowledge/8.png" width="8"><br>但我们发现这个分布可以分成三个聚类。如果我们给每个聚类编号为$z \in\{1,2,3\}$，那么$p(x ; z)$就是简单的高斯函数了。这里z就是隐变量。加入latent variable的意义在于，能够把复杂的问题变成多个简单的问题的和。</p><p>另外一种理解是，x是我们观察的值。这些观测值只是表面现象，但真正影响这些现象的是背后的一些latent variable。如果知道这些latent的取值，分布就会简单很多。这也是为什么概率inference里面也要研究latent variable。latent variable的问题，本身是个聚类的问题。也是找到系统的结构的问题。比如在求$p(x | \theta)$的分布时，如果引入一个latent variable，就可以用EM进行迭代求解。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是图像的高频信息和低频信息？&quot;&gt;&lt;a href=&quot;#什么是图像的高频信息和低频信息？&quot; class=&quot;headerlink&quot; title=&quot;什么是图像的高频信息和低频信息？&quot;&gt;&lt;/a&gt;什么是图像的高频信息和低频信息？&lt;/h2&gt;&lt;p&gt;低频信号和高频信号也叫低频分
      
    
    </summary>
    
      <category term="Knowledge Points" scheme="https://zeyuxiao1997.github.io/categories/Knowledge-Points/"/>
    
    
  </entry>
  
  <entry>
    <title>光场入门</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/16/light-field/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/16/light-field/</id>
    <published>2019-07-16T00:39:11.000Z</published>
    <updated>2019-07-17T13:53:37.896Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Principles-of-Light-Field-Imaging"><a href="#Principles-of-Light-Field-Imaging" class="headerlink" title="Principles of Light Field Imaging"></a><a href="/download/light_field_25_years.pdf">Principles of Light Field Imaging</a></h2><h3 id="光场成像综述"><a href="#光场成像综述" class="headerlink" title="光场成像综述"></a>光场成像综述</h3><ul><li>光场相机设计者需要优化镜头系统用于收集从物平面产生的光并尽可能将其汇聚在像平面上，当可以准确收敛的光束越大，捕获过程变得越有效，可实现的光学分辨率越高</li><li>相比于传统的光学相机的采集方式，光场相机并不需要将焦点引入像平面（导致焦平面的像清晰），光场相机捕捉全3D的场景信息</li><li>光场数据结构特殊，信息丰富（我的理解是全光函数中提出的不同slice samples中信息是不同的）</li><li>Depth-extraction和super-resolution技术增强了数据并减轻了通过采样两个额外维度引入的分辨率trade-off（不仅采集光的位置，还采集光的方向）</li><li>文章的行文思路是： acquisition and processing chain from optical acquisition principles to the final rendered output image</li></ul><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><h4 id="全光函数"><a href="#全光函数" class="headerlink" title="全光函数"></a>全光函数</h4><ul><li>全光函数使用的是非物理、非模型的一个框架，采用了更高维的一个描述方法【最重要的一点是引入了光的空间位置和入射方向】</li></ul><h4 id="光场"><a href="#光场" class="headerlink" title="光场"></a>光场</h4><ul><li><p>光场来源于全光函数及其限制：</p><blockquote><p>全光函数的时间维度通过积分消失<br>全光函数是单色的</p></blockquote></li><li><p>光场是一个4D函数</p></li></ul><h4 id="Calibration-and-preprocessing"><a href="#Calibration-and-preprocessing" class="headerlink" title="Calibration and preprocessing"></a>Calibration and preprocessing</h4><h2 id="光场成像原理"><a href="#光场成像原理" class="headerlink" title="光场成像原理"></a>光场成像原理</h2><h3 id="传统成像和光场成像"><a href="#传统成像和光场成像" class="headerlink" title="传统成像和光场成像"></a>传统成像和光场成像</h3><img src="/2019/07/16/light-field/1.png" width="1"><p>物点A向四面八方发出无数条光线，为了简洁，这里只取其中的三条来分析，发出的三条不同方向的光线在经过主透镜折射后在A’点相遇（费马原理）。这三束光线携带了A点不同方向的信息，但由于记录介质CCD只能记录三束光线干涉后叠加的振幅信息，所以会丢了A点的方向信息。简言之，传统成像方式只能记录光线所经过的位置信息，而丢失了与场景深度、目标几何形态、场景遮挡关系等光线角度信息，即三维信息。</p><img src="/2019/07/16/light-field/2.png" width="2"><p>为了记录A点的方向信息，即从根本上解决光线角度信息的采集问题，在传统成像图中CCD所在平面处加一种特殊的光学元件——微透镜阵列，通过记录光辐射在传播过程中的位置和方向的信息，这样就构成了光场采样方式，如上图所示：物点A的三束光线折射后在A’点相遇，但由于没有接收像的介质所以没有发生干涉叠加，他们仨相见后匆匆离开，彼此不干涉彼此，向着自己原有的方向继续前进，最终到达A1,A2,A3的位置，并在此记录下了A点的这三个方向的信息。</p><h3 id="四维光场函数模型与光场成像"><a href="#四维光场函数模型与光场成像" class="headerlink" title="四维光场函数模型与光场成像"></a>四维光场函数模型与光场成像</h3><img src="/2019/07/16/light-field/3.png" width="3"><p>我们常在很多论文中看到用全光函数模型分析光场，但鲜有将其与光场成像的关系讲清楚的，从而给我们造成误解。首先了解一下七维全光函数,如图3所示：我们用三维坐标(x, y, z)、任意传输方向(θ，φ)，以及光的波长(λ)和时间(t)描述一条光线。由于我们只关注光线的位置和传播方向，所以将其简化成四维光场函数（x, y, θ, φ），如图4所示：其中xy平面代表着光线的位置信息，θφ平面代表光线的方向信息，这样我们很容易将其与光场成像时的相关光学元件对应，即xy平面与微透镜阵所在列平面相对应，θφ平面与CCD所在平面相对应。这里一定要注意：初学者很容易将xy面和θφ平面与主透镜平面和微透镜阵列平面相对应</p><h3 id="位置和角度分辨率"><a href="#位置和角度分辨率" class="headerlink" title="位置和角度分辨率"></a>位置和角度分辨率</h3><img src="/2019/07/16/light-field/4.png" width="4"><p>有一个严重的问题：受CCD感光元件分辨率的限制，所采集光场的角度分辨率和位置分辨率相互制约，满足：位置分辨率×角度分辨率=CCD像素个数。借助图5（竖直方向）更好理解这种制约关系：图中若CCD上的可接受的像素个数为12，位置分辨率由微透镜的个数确定，为4，那么角度分辨率则不能超过3。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Principles-of-Light-Field-Imaging&quot;&gt;&lt;a href=&quot;#Principles-of-Light-Field-Imaging&quot; class=&quot;headerlink&quot; title=&quot;Principles of Light Field 
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
      <category term="review" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/review/"/>
    
    
      <category term="light field" scheme="https://zeyuxiao1997.github.io/tags/light-field/"/>
    
  </entry>
  
  <entry>
    <title>&lt;计算机视觉-算法与应用&gt;学习笔记</title>
    <link href="https://zeyuxiao1997.github.io/2019/07/07/cvaa/"/>
    <id>https://zeyuxiao1997.github.io/2019/07/07/cvaa/</id>
    <published>2019-07-07T01:54:37.000Z</published>
    <updated>2019-07-17T13:36:07.584Z</updated>
    
    <content type="html"><![CDATA[<p>计划在研究生入学前的暑假将这本书中与我相关的部分【前五章、第八章和第十章】阅读完，在入学前构建一套与我有关的知识体系。</p><p>【flag在2019年7月7日立下，长期有效】。阅读的主力是英文，在PDF中做的笔记同样是英文，部分有困惑的地方查询中文版的实体书。为了锻炼英语写话水平，本文的笔记部分将全部采用英文进行记录。在记笔记的时候如果遇到零碎的点，我将挑出重要的进行记录；如果是一个成熟的算法，我将在自己理解的基础上进行摘录。<br><a id="more"></a></p><h2 id="chapter1—Book-Review"><a href="#chapter1—Book-Review" class="headerlink" title="chapter1—Book Review"></a>chapter1—Book Review</h2><ul><li><p><strong>Computer vision involves going from images to a structural description of the scene (and computer graphics the converse)</strong>：reveal the difference between computer vision and computer graphics. Seen literally, CV is image—&gt;scene description while CG is scene description—&gt;image. My own understanding is THE DIFFERENCE between <strong>understanding and creation</strong>, CV means “I give you an image, and you tell me what is included in it” and CG means “I tell you my needs and things conclude in the image, you draw it”.</p></li><li><p>Computer vision tasks can be divided into 3 categories, namely Images, Geometry and Photometry.</p></li><li><p>Here I excerpt a significant point of view — <strong>The exercises can be used to build up your own personal library of selftested and validated vision algorithms, which is more worthwhile in the long term (assuming you have the time) than simply pulling algorithms out of a library whose performance you do not really understand</strong>, which emphasize the necessity of implementing algorithm.</p></li></ul><h2 id="chapter2—Image-Formation"><a href="#chapter2—Image-Formation" class="headerlink" title="chapter2—Image Formation"></a>chapter2—Image Formation</h2><p>Image formation process should be given following prerequisites—&gt;lighting conditions, scene geometry, surface properties, and camera optics.</p><h3 id="Geometric-primitives-and-transformations"><a href="#Geometric-primitives-and-transformations" class="headerlink" title="Geometric primitives and transformations"></a>Geometric primitives and transformations</h3><h4 id="Geometric-primitives"><a href="#Geometric-primitives" class="headerlink" title="Geometric primitives"></a>Geometric primitives</h4><ul><li><p>points, lines, planes</p></li><li><p>2D points:</p><blockquote><p>normal form: $\boldsymbol{x}=(x, y) \in \mathcal{R}^{2}$ or $\boldsymbol{x}=\left[\begin{array}{l}{x} \ {y}\end{array}\right]$<br>homogeneous coordinates: $\tilde{\boldsymbol{x}}=(\tilde{x}, \tilde{y}, \tilde{w}) \in \mathcal{P}^{2}$<br>2D projective space: $P^{2}=\mathcal{R}^{3}-(0,0,0)$<br>$\tilde{\boldsymbol{x}}=(\tilde{x}, \tilde{y}, \tilde{w})=\tilde{w}(x, y, 1)=\tilde{w} \overline{\boldsymbol{x}}$</p></blockquote></li><li><p>2D lines:</p><blockquote><p>homogeneous coordinates(使用齐次坐标表示): $\tilde{\boldsymbol{l}}=(a, b, c)$<br>corresponing line equation: $\overline{\boldsymbol{x}} \cdot \tilde{\boldsymbol{l}}=a x+b y+c=0$<br>normalize the line equation vector: $l=\left(\hat{n}_{x}, \hat{n}_{y}, d\right)=(\hat{\boldsymbol{n}}, d)$<br>calculate intersection of two lines: $\tilde{\boldsymbol{x}}=\tilde{\boldsymbol{l}}_{1} \times \tilde{\boldsymbol{l}}_{2}$<br>line joining two points: $\tilde{\boldsymbol{l}}=\tilde{\boldsymbol{x}}_{1} \times \tilde{\boldsymbol{x}}_{2}$</p></blockquote></li><li><p>2D conics(圆锥曲线)</p><blockquote><p>$\tilde{x}^{T} Q \tilde{x}=0$</p></blockquote></li><li><p>3D points</p><blockquote><p>inhomogeneous coordinates: $\boldsymbol{x}=(x, y, z) \in \mathcal{R}^{3}$<br>homogeneous coordinates: $\tilde{\boldsymbol{x}}=(\tilde{x}, \tilde{y}, \tilde{z}, \tilde{w}) \in \mathcal{P}^{3}$<br>augmented vector: $\tilde{\boldsymbol{x}}=\tilde{w} \overline{\boldsymbol{x}}$, where $\overline{\boldsymbol{x}}=(x, y, z, 1)$</p></blockquote></li><li><p>3D planes</p><blockquote><p>homogeneous coordinates: $\tilde{\boldsymbol{m}}=(a, b, c, d)$<br>$\overline{\boldsymbol{x}} \cdot \tilde{\boldsymbol{m}}=a x+b y+c z+d=0$<br>$\boldsymbol{m}=\left(\hat{n}_{x}, \hat{n}_{y}, \hat{n}_{z}, d\right)=(\hat{\boldsymbol{n}}, d)$, where $\hat{\boldsymbol{n}}=(\cos \theta \cos \phi, \sin \theta \cos \phi, \sin \phi)$ (<strong>spherical coordinates</strong>)</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;计划在研究生入学前的暑假将这本书中与我相关的部分【前五章、第八章和第十章】阅读完，在入学前构建一套与我有关的知识体系。&lt;/p&gt;
&lt;p&gt;【flag在2019年7月7日立下，长期有效】。阅读的主力是英文，在PDF中做的笔记同样是英文，部分有困惑的地方查询中文版的实体书。为了锻炼英语写话水平，本文的笔记部分将全部采用英文进行记录。在记笔记的时候如果遇到零碎的点，我将挑出重要的进行记录；如果是一个成熟的算法，我将在自己理解的基础上进行摘录。&lt;br&gt;
    
    </summary>
    
      <category term="book reading" scheme="https://zeyuxiao1997.github.io/categories/book-reading/"/>
    
    
      <category term="computer vision" scheme="https://zeyuxiao1997.github.io/tags/computer-vision/"/>
    
      <category term="review" scheme="https://zeyuxiao1997.github.io/tags/review/"/>
    
  </entry>
  
  <entry>
    <title>paper-derain_CVPR2017</title>
    <link href="https://zeyuxiao1997.github.io/2019/05/05/paper-derain-CVPR2017/"/>
    <id>https://zeyuxiao1997.github.io/2019/05/05/paper-derain-CVPR2017/</id>
    <published>2019-05-05T07:17:12.000Z</published>
    <updated>2019-05-05T08:46:51.060Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2017的作品。我阅读的第一篇关于图像去雨的论文，希望对raw climate data 中的噪声进行一个良好的建模和去除。<br><a id="more"></a><br>论文可<a href="/download/Yang_Deep_Joint_Rain_CVPR_2017_paper.pdf">点击下载</a>,代码可<a href="http://www.icst.pku.edu.cn/struct/Projects/joint_rain_removal.html" target="_blank" rel="noopener">点击获取</a>.</p><h2 id="一、introduction"><a href="#一、introduction" class="headerlink" title="一、introduction"></a>一、introduction</h2><h3 id="1-关于图像去雨的background"><a href="#1-关于图像去雨的background" class="headerlink" title="1.关于图像去雨的background"></a>1.关于图像去雨的background</h3><p>【第一次接触low-level中的derain，势必需要好好理解一下derain是个啥】</p><h3 id="2-现有方法的弊端"><a href="#2-现有方法的弊端" class="headerlink" title="2.现有方法的弊端"></a>2.现有方法的弊端</h3><ul><li><p>Due to the intrinsic overlapping between rain streaks and background texture patterns, most methods tend to remove texture details in non-rain regions, leading to over-smoothing the regions.</p></li><li><p>The degradation of rain is complex, and the existing rain model widely used in previous methods is insufficient to over some important factors in real rain images, such as the atmospheric veils due to rain streak accumulation, and different shapes or directions of streaks</p></li><li><p>The basic operation of many existing algorithms is on a local image patch or a limited receptive field (a limited spatial range). Thus, spatial contextual information in larger regions, which has been proven to be useful for rain removal, is rarely used.</p></li></ul><h3 id="3-claimed-contributions"><a href="#3-claimed-contributions" class="headerlink" title="3.claimed contributions"></a>3.claimed contributions</h3><ul><li><p>The first method to model the rain-streak binary mask, and also to model the atmospheric veils due to rain streak accumulation as well as various shapes and directions of overlapping rain streaks. This enables us to synthesize more similar data to real rain images for the network training.</p></li><li><p>The first method to jointly detect and remove rains from single images. With the additional information of detected rain regions, our rain removal achieves better performance.</p></li><li><p>The first rain removal method that uses a contextualized dilated network to obtain more context while preserving rich local details.</p></li><li><p>The first method that addresses heavy rain by introducing a recurrent rain detection and removal network, where it removes rain progressively, enabling us to obtain good results even in significantly complex cases.</p></li></ul><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p>[1]<a href="https://blog.csdn.net/zhangjunhit/article/details/64442994" target="_blank" rel="noopener">https://blog.csdn.net/zhangjunhit/article/details/64442994</a><br>[2]<a href="https://www.leiphone.com/news/201707/ZXZ450ilP3PnyUUx.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201707/ZXZ450ilP3PnyUUx.html</a><br>[3]<a href="https://www.jianshu.com/p/15ca85da6ae2" target="_blank" rel="noopener">https://www.jianshu.com/p/15ca85da6ae2</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CVPR2017的作品。我阅读的第一篇关于图像去雨的论文，希望对raw climate data 中的噪声进行一个良好的建模和去除。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="derain" scheme="https://zeyuxiao1997.github.io/tags/derain/"/>
    
      <category term="low-level vision" scheme="https://zeyuxiao1997.github.io/tags/low-level-vision/"/>
    
  </entry>
  
  <entry>
    <title>为什么深度学习去噪都采用高斯白噪声？</title>
    <link href="https://zeyuxiao1997.github.io/2019/05/05/why-gaussian-noise/"/>
    <id>https://zeyuxiao1997.github.io/2019/05/05/why-gaussian-noise/</id>
    <published>2019-05-05T02:05:24.000Z</published>
    <updated>2019-05-05T02:29:36.053Z</updated>
    
    <content type="html"><![CDATA[<p>来源知乎：<a href="https://www.zhihu.com/question/67938028" target="_blank" rel="noopener">https://www.zhihu.com/question/67938028</a><br><a id="more"></a></p><p>需要通过以下三个子问题来说明：</p><ul><li><p>为什么要做仿真噪音synthetic noise的实验？</p></li><li><p>在所有的synthetic noise里，为什么大家都用高斯白噪声，而不太常用其他distribution的噪声？</p></li><li><p>基于synthetic noise，比如高斯噪音的算法，可以适用于真实噪音吗？ </p></li></ul><p>以下回答，我尽量不引用论文，不安利自己的工作，先说结论，后说论据，以方便阅读。</p><h2 id="问题一：Why-synthetic-noise？"><a href="#问题一：Why-synthetic-noise？" class="headerlink" title="问题一：Why synthetic noise？"></a>问题一：Why synthetic noise？</h2><p>先说结论：相对于real noise，用synthetic noise的好处是便于分析问题/设计算法，便于量化和评价算法效果。</p><p>便于对降噪问题的分析/算法的设计：降噪的本质是对数据本身的重建，以起到排除污染（corruption）的作用。这里面涉及到需要对(1)数据，(2)污染（噪音）的模型和分析。数据的模型就是我们一般常用的那些，比如稀疏表达(sparse coding），统计（probabilistic），低秩（low-rankness），collaborative filtering之类的。这些都是基于一定的数学假设。说穿了，事实上没不存在对数据100%精确的model，或者所谓的true model。再来说噪音模型，我们一般把noise这种污染定义为一个additive或者multiplicative的随机变量。那么这个随机变量的随机分布是什么？如果知道了这个，我们就可以设计出对应的合理的算法。</p><p>那么如果是real noise，他是什么分布呢？没有人知道，因为real就意味着未知。噪音可以是unstructured的，也可以是structured的。real的数据里面的噪音，可以是consistent的，也可以是变动的。甚至一幅图，一个视频里的real noise在不同位置都是不一样的。那这种情况下的问题分析就是极难的，或者说这个问题本身就是untrackable的，not well defined的。</p><p>所以科研或者工程设计里面，都会对这类问题做出合理的假设，比如这里的：噪音是高斯白噪声。基于这个假设再来分析问题。</p><p>便于量化和评价算法效果：评价一个降噪算法的效果，需要采用一定的评价标准（metric）。我们一般把评价标准分为客观（objective）和主观（subjective）的：客观标准很好理解：给我一个数学计算方式，算出这个降噪过后的数据，到底有多好。这样做清晰明了，一般没有什么好争议的。常见的这样的metric有Peak Signal-to-Noise Ratio （PSNR），Mean Square Error（MSE），Structured Similarity（SSIM），等等。你经常可以在降噪论文里面看到这三个家伙的身影。他们这些metric的绝对数值的高低，直观地反应方法效果的好坏。</p><p>虽然我知道也有一些工作，试着propose一些不需要ground truth的objective quality metric，但最常用的这类经典metric无一例外地需要图片的无噪音真实值（ground truth）作为参考。如果你是使用仿真噪音，你自然是有ground truth的。但如果是真实噪音，你确一般不知道ground truth是什么。</p><p>所以一般对于真实噪音的降噪实验，我们都只好算法一些subjective的metric：让人眼来辨认降噪出来的图效果是否好。这不同的人，可能对图的喜好也会不一样，这样就经常会产生评价的个体差异，产生争议。就算想要组织一大批人来做测试，成本会很高，不利于科研的高效性。</p><h2 id="问题二：Why-Gaussian-noise？"><a href="#问题二：Why-Gaussian-noise？" class="headerlink" title="问题二：Why Gaussian noise？"></a>问题二：Why Gaussian noise？</h2><p>先说结论：相比于其他的synthetic noise distribution，高斯噪音确实有他的合理性。在真实噪音的噪音源特别复杂的时候，高斯噪音可能算是最好的对真实噪音的模拟。</p><p>其实不光是深度学习的降噪算法，传统方法（好吧，自从有了深度学习以后，什么sparse coding，GMM，low-rank，collaborative filtering都变成传统方法了…）也大多喜欢用高斯白噪声来做仿真实验。那么大家不约而同地都玩儿高斯噪音可能有背后的原因。我觉得这个可能才是题主最关心的问题。</p><p>那这里的答案就是，采用高斯噪音，是为了更好地模拟未知的真实噪音：在真实环境中，噪音往往不是由单一源头造成的，而是很多不同来源的噪音复合体。假设，我们把真实噪音看成非常多不同概率分布的随机变量的加合，并且每一个随机变量都是独立的，那么根据Central Limit Theorem，他们的normalized sum就随着噪音源数量的上升，趋近于一个高斯分布。基于这种假设来看，采用合成的高斯噪音，是在处理这种复杂，且不知道噪音分布为何的情况下，一个既简单又不差的近似仿真。</p><h2 id="问题三：Can-it-work-for-real-noise？"><a href="#问题三：Can-it-work-for-real-noise？" class="headerlink" title="问题三：Can it work for real noise？"></a>问题三：Can it work for real noise？</h2><p>先说结论：在高斯噪音试验下效果的算法，不一定在真实噪音下效果也同样地好。这个要看真实噪音具体长啥样，还要看算法本身的设计是否对噪音分布有一定的鲁棒性。</p><p>在搞清楚了问题一和二之后，相信问题三应该就很好理解了：因为Gaussian noise只是对real noise的一个近似和仿真，没有任何的保证说，设计的算法在处理real noise的时候就一定要表现得同样得好。但由于问题二我们讲了，Gaussian noise test有一定的合理性，所以这类算法在real noise的情况下都会有一定的降噪功用。</p><p>最近有一些新的数据库，包括了真实噪音图片以及他们捕捉到的ground truth。我认为这类数据库将会带来一波专注于真实噪音除去的工作。</p><h2 id="deep-learning"><a href="#deep-learning" class="headerlink" title="deep learning"></a>deep learning</h2><p>最后再来说说深度学习，在降噪问题上的特殊性：深度学习之类算法，模型本身是高度data-driven，而不是rule-based的。换句话说，深度学习算法的设计，或者说网络结构的设计，并不强烈依赖于噪音的概率分布。这对于降噪算法的generalization是很好的。</p><p>然而这并不是说，深度学习的降噪算法，是对所有噪音类型通吃的。深度学习算法一般需要supervised training。这样在训练数据上的选择，确实往往依赖于噪音的概率分布：如果我们要做Gaussian noise removal，那训练数据就应该是添加了Gaussian noise的结果。那么如果我们要做真实噪音的denoising，要怎么准备训练数据？你的训练数据的噪音分布，和你的测试数据是一样的吗？这些都没有保证，或者说不一定说是consistent的。</p><p>但是我个人看法是，可能相对于传统方法而言，深度学习算法在从一种特定噪音的处理，generalize到未知噪音，鲁棒性应该会更高。虽然没有理论上的证明（深度学习上搞这种证明，臣妾确实办不到…），我们近期的工作也证实了这一点。这一段都是私货，如果有其他大神有对这个更好的看法，欢迎讨论。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;来源知乎：&lt;a href=&quot;https://www.zhihu.com/question/67938028&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.zhihu.com/question/67938028&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Technical guide" scheme="https://zeyuxiao1997.github.io/categories/Technical-guide/"/>
    
    
      <category term="denoise" scheme="https://zeyuxiao1997.github.io/tags/denoise/"/>
    
  </entry>
  
  <entry>
    <title>paper-NLRN</title>
    <link href="https://zeyuxiao1997.github.io/2019/05/05/paper-NLRN/"/>
    <id>https://zeyuxiao1997.github.io/2019/05/05/paper-NLRN/</id>
    <published>2019-05-05T02:00:48.000Z</published>
    <updated>2019-05-05T02:00:49.136Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>paper——Video Frame Synthesis using Deep Voxel Flow</title>
    <link href="https://zeyuxiao1997.github.io/2019/05/04/paper-dvf/"/>
    <id>https://zeyuxiao1997.github.io/2019/05/04/paper-dvf/</id>
    <published>2019-05-04T08:33:05.000Z</published>
    <updated>2019-08-05T07:48:25.402Z</updated>
    
    <content type="html"><![CDATA[<p>ICCV2017的作品。解决了模拟新的视频帧的问题，要么是现有视频帧之间的插值，要么是紧跟着他们的探索。这个问题是非常具有挑战性的，因为，视频的外观和运动是非常复杂的。传统 optical-flow-based solutions 当 flow estimation 失败的时候，就变得非常困难；而最新的基于神经网络的方法直接预测像素值，经常产生模糊的结果。于是，在此motivation的基础上，作者提出了结合这两种方法的思路，通过训练一个神经网络，来学习去合成视频帧，通过 flowing pixel values from existing ones, 我们称之为：deep voxel flow. 所提出的方法不需要人类监督，任何video都可以用于训练，通过丢掉，并且预测现有的frames。<br><a id="more"></a><br>论文可<a href="/download/Liu_Video_Frame_Synthesis_ICCV_2017_paper.pdf">点击下载</a>,代码可<a href="https://github.com/liuziwei7/voxel-flow" target="_blank" rel="noopener">点击获取</a>.</p><h2 id="一、Introduction"><a href="#一、Introduction" class="headerlink" title="一、Introduction"></a>一、Introduction</h2><h3 id="1-motivation"><a href="#1-motivation" class="headerlink" title="1. motivation"></a>1. motivation</h3><p>传统的方法解决视频插帧（内插或外插），就是依赖于帧与帧之间的optical flow，然后进行 optical flow vectors 之间的【插值】或者【预测】。这种方法称为：“optical-flow-complete”；当光流准确的时候，这种方法是非常有效的，但是当不准确的时候，就会产生额外的错误信息。一种基于产生式CNN的方法，直接产生RGB像素值。但是这种方法经常会产生模糊的情况，并非像光流一样有效。</p><h3 id="2-现有方法的弊端"><a href="#2-现有方法的弊端" class="headerlink" title="2.现有方法的弊端"></a>2.现有方法的弊端</h3><p>【略】</p><h2 id="二、method"><a href="#二、method" class="headerlink" title="二、method"></a>二、method</h2><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p>[1] <a href="https://yq.aliyun.com/articles/310845" target="_blank" rel="noopener">https://yq.aliyun.com/articles/310845</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICCV2017的作品。解决了模拟新的视频帧的问题，要么是现有视频帧之间的插值，要么是紧跟着他们的探索。这个问题是非常具有挑战性的，因为，视频的外观和运动是非常复杂的。传统 optical-flow-based solutions 当 flow estimation 失败的时候，就变得非常困难；而最新的基于神经网络的方法直接预测像素值，经常产生模糊的结果。于是，在此motivation的基础上，作者提出了结合这两种方法的思路，通过训练一个神经网络，来学习去合成视频帧，通过 flowing pixel values from existing ones, 我们称之为：deep voxel flow. 所提出的方法不需要人类监督，任何video都可以用于训练，通过丢掉，并且预测现有的frames。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="Video Frame Interpolation" scheme="https://zeyuxiao1997.github.io/tags/Video-Frame-Interpolation/"/>
    
  </entry>
  
  <entry>
    <title>paper——Deep Video Frame Interpolation using Cyclic Frame Generation</title>
    <link href="https://zeyuxiao1997.github.io/2019/05/04/paper-VFICFG/"/>
    <id>https://zeyuxiao1997.github.io/2019/05/04/paper-VFICFG/</id>
    <published>2019-05-04T01:48:49.000Z</published>
    <updated>2019-05-05T02:29:33.433Z</updated>
    
    <content type="html"><![CDATA[<p>AAAI2019的作品。基于DVF，加入3个extensions，使表现进一步提高。<br><a id="more"></a><br>论文可<a href="/download/liu.pdf">点击下载</a>,代码可<a href="https://github.com/alex04072000/CyclicGen" target="_blank" rel="noopener">点击获取</a>.</p><h2 id="一、introduction"><a href="#一、introduction" class="headerlink" title="一、introduction"></a>一、introduction</h2><h3 id="1-motivation"><a href="#1-motivation" class="headerlink" title="1.motivation"></a>1.motivation</h3><p>高帧率视频获取成本高，因此使用视频插帧的方法生成连续的中间帧是很实用的。但是现有方法常常使得插值的帧过于平滑或者过于模糊，这些情况在运动很大或者纹理丰富的局部区域尤其明显。因此寻找一种好的限制方式很重要，用于捕捉帧之间的变化和纹理细节。</p><p>作者提出的CyclicGen使用循环一致性损失，并增加了两个extension（motion linearity loss和edge guided training），使得表现进一步提升。</p><h3 id="2-现有方法的弊端"><a href="#2-现有方法的弊端" class="headerlink" title="2.现有方法的弊端"></a>2.现有方法的弊端</h3><h4 id="视频插帧"><a href="#视频插帧" class="headerlink" title="视频插帧"></a>视频插帧</h4><p>现有的视频插帧方法计算复杂、为了改善结果一味增加网络深度用于提取特征。</p><p>【这篇文章是我看的第一篇关于视频插帧的论文，里面提到的论文我不是很了解，后续会把相关工作补上】</p><h4 id="cycle-constrant"><a href="#cycle-constrant" class="headerlink" title="cycle constrant"></a>cycle constrant</h4><p>这个思想是第一次用于视频插帧。之前常用于语言翻译、运动追踪、深度估计、3D视觉匹配等方向。</p><h2 id="二、method"><a href="#二、method" class="headerlink" title="二、method"></a>二、method</h2><p>假设输入为：$S=\left\{I_{0}, I_{1}, I_{2}, \dots, I_{N}\right\}$，2倍的插值为$S^{\prime}=\left\{I_{0.5}, I_{1.5}, I_{2.5}, \dots, I_{N-0.5}\right\}$，当对$S \cup S^{\prime}$进行插值时，$S^{\prime \prime}=\left\{I_{0.25}, I_{0.75}, \dots, I_{N-0.25}\right\}$是4倍的插值结果。</p><p>网络是两步的训练结果。有3个重要的部件：</p><ul><li><p>cycle consistency loss：boosts the model to produce plausible intermediate frames so that these frames can be used to reversely reconstruct the given frames. </p></li><li><p>motion linearity loss：regularize the estimation of motions in training. </p></li><li><p>Edge guided training：help preserve the edge structure</p></li></ul><p>网络结构如下图：<br><img src="/2019/05/04/paper-VFICFG/1.png" width="1"></p><h3 id="Cycle-Consistency-Loss-mathcal-L-c"><a href="#Cycle-Consistency-Loss-mathcal-L-c" class="headerlink" title="Cycle Consistency Loss $\mathcal{L}_{c}$"></a>Cycle Consistency Loss $\mathcal{L}_{c}$</h3><script type="math/tex; mode=display">\mathcal{L}_{r}=\sum_{n=1}^{N}\left\|f\left(I_{n, 0}, I_{n, 2}\right)-I_{n, 1}\right\|_{1}=\sum_{n=1}^{N}\left\|I_{n, 1}^{\prime}-I_{n, 1}\right\|_{1}</script><p><script type="math/tex">\mathcal{L}=\mathcal{L}_{r}+\mathcal{L}_{c}=\sum_{n=1}^{N}\left\|I_{n, 1}^{\prime}-I_{n, 1}\right\|_{1}+\left\|I_{n, 1}^{\prime \prime}-I_{n, 1}\right\|_{1}</script>，where $I_{n, 1}^{\prime \prime}=f\left(I_{n, 0.5}^{\prime}, I_{n, 1.5}^{\prime}\right), I_{n, 0.5}^{\prime}=f\left(I_{n, 0}, I_{n, 1}\right)$ $I_{n, 1.5}^{\prime}=f\left(I_{n, 1}, I_{n, 2}\right)$</p><h3 id="Motion-Linearity-Loss-mathcal-L-m"><a href="#Motion-Linearity-Loss-mathcal-L-m" class="headerlink" title="Motion Linearity Loss $\mathcal{L}_{m}$"></a>Motion Linearity Loss $\mathcal{L}_{m}$</h3><p>运动很大的区域会有巨大的预测错误。因此假设两个连续帧之间的时间间隔足够短，使得两个帧之间的运动是线性的。 在大多数情况下，该假设有助于减少运动的不确定性并减轻近似误差</p><p><script type="math/tex">\mathcal{L}_{m}=\sum_{n=1}^{N}\left\|F_{n, 0 \rightarrow 2}-2 \cdot F_{n, 0.5 \rightarrow 1.5}\right\|_{2}^{2}</script>，where F表示flow map，其下标表示其数据索引和输入帧的时间间隔。</p><h3 id="Edge-guided-Training-mathcal-E"><a href="#Edge-guided-Training-mathcal-E" class="headerlink" title="Edge-guided Training $\mathcal{E}$"></a>Edge-guided Training $\mathcal{E}$</h3><p>使用holistically-nested edge detection</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>似乎结果都蛮不错的，后续跑完实验再来更。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;AAAI2019的作品。基于DVF，加入3个extensions，使表现进一步提高。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="Video Frame Interpolation" scheme="https://zeyuxiao1997.github.io/tags/Video-Frame-Interpolation/"/>
    
  </entry>
  
  <entry>
    <title>paper——Video Frame Interpolation via Adaptive Convolution</title>
    <link href="https://zeyuxiao1997.github.io/2019/05/04/paper-adaconv/"/>
    <id>https://zeyuxiao1997.github.io/2019/05/04/paper-adaconv/</id>
    <published>2019-05-04T01:03:41.000Z</published>
    <updated>2019-05-04T01:29:00.560Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2017的作品。为了避免遮挡、模糊、亮度突变造成伪像的问题，将传统的视频插帧的motion estimation+pixel synthesis合并为一步。将像素插值视为两个输入视频帧中相应图像块的卷积，并使用深度完全卷积神经网络估计空间自适应卷积核。<br><a id="more"></a><br>论文可<a href="/download/1703.07514.pdf">点击下载</a>,代码可<a href="http://web.cecs.pdx.edu/~fliu/project/adaconv/" target="_blank" rel="noopener">点击获取</a>.</p><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p>[1] <a href="https://blog.csdn.net/zhangjunhit/article/details/78181959" target="_blank" rel="noopener">https://blog.csdn.net/zhangjunhit/article/details/78181959</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CVPR2017的作品。为了避免遮挡、模糊、亮度突变造成伪像的问题，将传统的视频插帧的motion estimation+pixel synthesis合并为一步。将像素插值视为两个输入视频帧中相应图像块的卷积，并使用深度完全卷积神经网络估计空间自适应卷积核。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="Video Frame Interpolation" scheme="https://zeyuxiao1997.github.io/tags/Video-Frame-Interpolation/"/>
    
  </entry>
  
  <entry>
    <title>paper——Deep Image Prior</title>
    <link href="https://zeyuxiao1997.github.io/2019/05/03/paper-Deep-Image-Prior/"/>
    <id>https://zeyuxiao1997.github.io/2019/05/03/paper-Deep-Image-Prior/</id>
    <published>2019-05-03T13:39:28.000Z</published>
    <updated>2019-05-03T13:46:39.372Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2018的论文。不同的神经网络可以实现给图像去噪、去水印、消除马赛克等等功能，但我们能否让一个模型完成上述所有事？事实证明 AI 确实有这样的能力。来自 Skoltech、Yandex 和牛津大学的学者们提出了一种可以满足所有大胆想法的神经网络，这就是Deep Image Prior。<br><a id="more"></a><br><a href="/download/deep_image_prior_journal.pdf">会议论文</a>和<a href="/download/deep_image_prior.pdf">期刊论文</a>可分别点击下载；<a href="https://github.com/DmitryUlyanov/deep-image-prior" target="_blank" rel="noopener">代码链接</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CVPR2018的论文。不同的神经网络可以实现给图像去噪、去水印、消除马赛克等等功能，但我们能否让一个模型完成上述所有事？事实证明 AI 确实有这样的能力。来自 Skoltech、Yandex 和牛津大学的学者们提出了一种可以满足所有大胆想法的神经网络，这就是Deep Image Prior。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
  </entry>
  
  <entry>
    <title>BDGAN</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/30/BDGAN/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/30/BDGAN/</id>
    <published>2019-04-30T12:35:14.000Z</published>
    <updated>2019-04-30T12:35:14.721Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>MatconvNet安装与使用</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/30/matlab-conv/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/30/matlab-conv/</id>
    <published>2019-04-30T03:21:35.000Z</published>
    <updated>2019-04-30T12:34:32.515Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍matlab中MatconvNet的安装。matlab事实上也是一个很有用的深度学习库，并且现在看的论文有部分是基于matlab+MatconvNet实现的，因此安装支持GPU的matlab是必须的。<br><a id="more"></a></p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="查看matlab-cuda-cudnn的版本"><a href="#查看matlab-cuda-cudnn的版本" class="headerlink" title="查看matlab/cuda/cudnn的版本"></a>查看matlab/cuda/cudnn的版本</h3><p>输入如下指令，查看cuda版本<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">nvcc -V</span></span><br></pre></td></tr></table></figure></p><p>或者进入到C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0 目录，输入如下指令查看<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cat</span> <span class="keyword">version</span>.txt</span><br></pre></td></tr></table></figure></p><p>未完待续。。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍matlab中MatconvNet的安装。matlab事实上也是一个很有用的深度学习库，并且现在看的论文有部分是基于matlab+MatconvNet实现的，因此安装支持GPU的matlab是必须的。&lt;br&gt;
    
    </summary>
    
      <category term="tools" scheme="https://zeyuxiao1997.github.io/categories/tools/"/>
    
    
      <category term="matlab" scheme="https://zeyuxiao1997.github.io/tags/matlab/"/>
    
      <category term="MatconvNet" scheme="https://zeyuxiao1997.github.io/tags/MatconvNet/"/>
    
  </entry>
  
  <entry>
    <title>IPS流程（camera成像原理的介绍）</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/28/camera/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/28/camera/</id>
    <published>2019-04-28T01:37:26.000Z</published>
    <updated>2019-04-28T01:47:25.506Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>在看真实图像盲去噪的时候偶然看到了这个博客，很遗憾没有在网上找到原版ppt或者pdf，因此把链接放在这，具体的成像原理可以通过<a href="https://blog.csdn.net/gwplovekimi/article/details/84638925" target="_blank" rel="noopener">https://blog.csdn.net/gwplovekimi/article/details/84638925</a>进行查看。</p><p>感谢原链接作者。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;在看真实图像盲去噪的时候偶然看到了这个博客，很遗憾没有在网上找到原版ppt或者pdf，因此把链接放在这，具体的成像原理可以通过&lt;a href=&quot;https://blog.csdn.net/gwplovekimi/article/detai
      
    
    </summary>
    
      <category term="Theory and application" scheme="https://zeyuxiao1997.github.io/categories/Theory-and-application/"/>
    
    
      <category term="camera imaging" scheme="https://zeyuxiao1997.github.io/tags/camera-imaging/"/>
    
  </entry>
  
  <entry>
    <title>pytorch的一些学习记录/困惑/笔记等(持续更新)</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/25/pytorch-learn/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/25/pytorch-learn/</id>
    <published>2019-04-25T08:13:25.000Z</published>
    <updated>2019-08-06T08:25:59.390Z</updated>
    
    <content type="html"><![CDATA[<p>pytorch的相关学习记录、困惑、学习笔记等，持续更新。<br><a id="more"></a></p><h2 id="查看网络参数总量"><a href="#查看网络参数总量" class="headerlink" title="查看网络参数总量"></a>查看网络参数总量</h2><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">netG = Generator(UPSCALE_FACTOR)</span><br><span class="line"><span class="comment"># here is generator network, which generate a super-resolved image</span></span><br><span class="line">print(<span class="string">'# generator parameters:'</span>, <span class="built_in">sum</span>(<span class="built_in">param</span>.numel() <span class="keyword">for</span> <span class="built_in">param</span> <span class="keyword">in</span> netG.parameters()))</span><br><span class="line">netD = Discriminator()</span><br><span class="line"><span class="comment"># here is discriminator network, which decide whether the image is the real one or the super-resolved one</span></span><br><span class="line">print(<span class="string">'# discriminator parameters:'</span>, <span class="built_in">sum</span>(<span class="built_in">param</span>.numel() <span class="keyword">for</span> <span class="built_in">param</span> <span class="keyword">in</span> netD.parameters()))</span><br></pre></td></tr></table></figure><h2 id="关于pytorch的dataloader和dataset"><a href="#关于pytorch的dataloader和dataset" class="headerlink" title="关于pytorch的dataloader和dataset"></a>关于pytorch的dataloader和dataset</h2><p><a href="https://blog.csdn.net/qq_36556893/article/details/86505934" target="_blank" rel="noopener">参考网站</a></p><h2 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h2><p>只有浮点类型才可以对Tensor设置自动求导，如果是Int类型的话，会报如下错误：<img src="/2019/04/25/pytorch-learn/1.png" width="1">，所以一定要使用float进行自动求导。</p><h2 id="关于net-parameters"><a href="#关于net-parameters" class="headerlink" title="关于net.parameters()"></a>关于net.parameters()</h2><p>可以通过Module.parameters()获取网络的参数，也就是构建好神经网络后，网络的参数都保存在parameters()函数当中</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">print</span><span class="params">(net.parameters()</span></span>)</span><br><span class="line"></span><br><span class="line">para = list(net.parameters())</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(para)</span></span></span><br><span class="line">#len返回列表项个数</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(len(para)</span></span>)</span><br></pre></td></tr></table></figure><p>可以使用enumerate函数实现逐列表项输出列表元素和index，也就是：<br><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">num</span>,temp <span class="keyword">in</span> enumerate(para):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'number:'</span>,<span class="built_in">num</span>)</span><br><span class="line">    <span class="built_in">print</span>(temp)</span><br></pre></td></tr></table></figure></p><h2 id="optimizer-step-和scheduler-step-的区别"><a href="#optimizer-step-和scheduler-step-的区别" class="headerlink" title="optimizer.step()和scheduler.step()的区别"></a>optimizer.step()和scheduler.step()的区别</h2><p>optimizer.step()通常用在每个mini-batch之中，而scheduler.step()通常用在epoch里面,但是不绝对，可以根据具体的需求来做。只有用了optimizer.step()，模型才会更新，而scheduler.step()是对lr进行调整。通常我们有<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)</span><br><span class="line">scheduler = lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1)</span><br><span class="line">model = net.train(model, loss_function, optimizer, scheduler, num_epochs = 100)</span><br></pre></td></tr></table></figure></p><p>在scheduler的step_size表示scheduler.step()每调用step_size次，对应的学习率就会按照策略调整一次。所以如果scheduler.step()是放在mini-batch里面，那么step_size指的是经过这么多次迭代，学习率改变一次。</p><h2 id="dataloader使用"><a href="#dataloader使用" class="headerlink" title="dataloader使用"></a>dataloader使用</h2><p>来源<a href="https://www.jianshu.com/p/8ea7fba72673" target="_blank" rel="noopener">简书</a></p><h3 id="加载头文件"><a href="#加载头文件" class="headerlink" title="加载头文件"></a>加载头文件</h3><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.<span class="keyword">data</span> <span class="keyword">import</span> DataLoader, Sampler</span><br><span class="line">from torchvision <span class="keyword">import</span> datasets, transforms</span><br></pre></td></tr></table></figure><h3 id="transforms表示对图片的预处理方式"><a href="#transforms表示对图片的预处理方式" class="headerlink" title="transforms表示对图片的预处理方式"></a><strong>transforms</strong>表示对图片的预处理方式</h3><figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">data_transform=&#123;'<span class="attribute">train'</span>:transforms<span class="variable">.Compose</span>([</span><br><span class="line">                    # transforms<span class="variable">.RandomResizedCrop</span>(image_size),</span><br><span class="line">                    # transforms<span class="variable">.Resize</span>(224),</span><br><span class="line">                    transforms<span class="variable">.RandomResizedCrop</span>(int(image_size*1.2)),</span><br><span class="line">                    # transforms<span class="variable">.ToPILImage</span>(),</span><br><span class="line">                    transforms<span class="variable">.RandomAffine</span>(15),</span><br><span class="line">                    # transforms<span class="variable">.RandomHorizontalFlip</span>(),</span><br><span class="line">                    transforms<span class="variable">.RandomVerticalFlip</span>(),</span><br><span class="line">                    transforms<span class="variable">.RandomRotation</span>(10),</span><br><span class="line">                    transforms<span class="variable">.ColorJitter</span>(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),</span><br><span class="line">                    transforms<span class="variable">.RandomGrayscale</span>(),</span><br><span class="line">                    transforms<span class="variable">.TenCrop</span>(image_size),</span><br><span class="line">                    transforms<span class="variable">.Lambda</span>(lambda crops: torch<span class="variable">.stack</span>([transforms<span class="variable">.ToTensor</span>()(crop) for crop in crops])),</span><br><span class="line">                    transforms<span class="variable">.Lambda</span>(lambda crops: torch<span class="variable">.stack</span>([transforms<span class="variable">.Normalize</span>([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(crop) for crop in crops])),</span><br><span class="line">                    # transforms<span class="variable">.FiveCop</span>(image_size),</span><br><span class="line">                    # Lambda(lambda crops: torch<span class="variable">.stack</span>([transfoms<span class="variable">.ToTensor</span>()(crop) for crop in crops])),</span><br><span class="line">                    # transforms<span class="variable">.ToTensor</span>(),</span><br><span class="line">                    # transforms<span class="variable">.Normalize</span>([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</span><br><span class="line">                    ]),</span><br><span class="line">                "val":transforms<span class="variable">.Compose</span>([</span><br><span class="line">                    transforms<span class="variable">.Resize</span>(image_size),</span><br><span class="line">                    transforms<span class="variable">.CenterCrop</span>(image_size),</span><br><span class="line">                    transforms<span class="variable">.ToTensor</span>(),</span><br><span class="line">                    transforms<span class="variable">.Normalize</span>([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</span><br><span class="line">                ]),</span><br><span class="line">                "test":transforms<span class="variable">.Compose</span>([</span><br><span class="line">                    transforms<span class="variable">.Resize</span>(image_size),</span><br><span class="line">                    transforms<span class="variable">.CenterCrop</span>(image_size),</span><br><span class="line">                    transforms<span class="variable">.ToTensor</span>(),</span><br><span class="line">                    transforms<span class="variable">.Normalize</span>([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</span><br><span class="line">                ])&#125;</span><br></pre></td></tr></table></figure><h3 id="使用datasets-ImageFolder加载图片数据"><a href="#使用datasets-ImageFolder加载图片数据" class="headerlink" title="使用datasets.ImageFolder加载图片数据"></a>使用<strong>datasets.ImageFolder</strong>加载图片数据</h3><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image_datasets=&#123;<span class="built_in">name</span>:datasets.ImageFolder(os.path.<span class="built_in">join</span>(rootpath,<span class="built_in">name</span>),data_transform[<span class="built_in">name</span>]) <span class="keyword">for</span> <span class="built_in">name</span> <span class="built_in">in</span> [<span class="string">'train'</span>,<span class="string">'val'</span>,<span class="string">'test'</span>]&#125;</span><br></pre></td></tr></table></figure><h3 id="生成dataloader"><a href="#生成dataloader" class="headerlink" title="生成dataloader"></a>生成<strong>dataloader</strong></h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataloaders=&#123;name : torch<span class="selector-class">.utils</span><span class="selector-class">.data</span><span class="selector-class">.DataLoader</span>(image_datasets[name],batch_size=batch_size,shuffle=True) <span class="keyword">for</span> name <span class="keyword">in</span> [<span class="string">'train'</span>,<span class="string">'val'</span>]&#125;</span><br><span class="line">testDataloader=torch<span class="selector-class">.utils</span><span class="selector-class">.data</span><span class="selector-class">.DataLoader</span>(image_datasets[<span class="string">'test'</span>],batch_size=<span class="number">1</span>,shuffle=False)</span><br></pre></td></tr></table></figure><h3 id="每次读出一个batch-size的数据"><a href="#每次读出一个batch-size的数据" class="headerlink" title="每次读出一个batch_size的数据"></a>每次读出一个batch_size的数据</h3><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="keyword">index</span>,item <span class="keyword">in</span> enumerate(dataloaders[<span class="string">'train'</span>])</span><br></pre></td></tr></table></figure><h2 id="设置梯度"><a href="#设置梯度" class="headerlink" title="设置梯度"></a>设置梯度</h2><p>深度学习很重要的一点是反向传播，所以梯度的计算尤为重要，pytorch中对于tensor可以自动的计算梯度，对于其中已经包含的层，梯度的计算也是自动的。但是当你使用一个fine-tunning的操作，或者自己定义层时，如何定义处理梯度问题，或者设置梯度就尤为关键了。这里介绍pytorch中如何开启或关闭tensor的梯度计算和层的梯度计算。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.randn(5, 5)  # <span class="attribute">requires_grad</span>=<span class="literal">False</span> by default</span><br><span class="line">&gt;&gt;&gt; y = torch.randn(5, 5)  # <span class="attribute">requires_grad</span>=<span class="literal">False</span> by default</span><br><span class="line">&gt;&gt;&gt; z = torch.randn((5, 5), <span class="attribute">requires_grad</span>=<span class="literal">True</span>)</span><br><span class="line">&gt;&gt;&gt; a = x + y</span><br><span class="line">&gt;&gt;&gt; a.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br><span class="line">&gt;&gt;&gt; b = a + z</span><br><span class="line">&gt;&gt;&gt; b.requires_grad</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><p>pytorch的tensor生成函数randn中设置参数requires_grad能够开启或者关闭该tensor的梯度计算。从上面的结果可以看出梯度的计算是传递的，虽然a没有梯度，但是z有梯度，所以b也有了。</p><p>对于设置层的梯度计算，有下面的例子：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载预训练的模型，这里是加载resnet18</span></span><br><span class="line">model = torchvision.models.resnet18(<span class="attribute">pretrained</span>=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace the last fully-connected layer</span></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></span><br><span class="line">model.fc = nn.Linear(512, 100)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimize only the classifier</span></span><br><span class="line">optimizer = optim.SGD(model.fc.parameters(), <span class="attribute">lr</span>=1e-2, <span class="attribute">momentum</span>=0.9)</span><br></pre></td></tr></table></figure></p><p>这里主要是在<strong>fine-tunning</strong>的场合用的很多。首先使用torchvision加载一个预训练模型，resnet18，并且加载他的预训练参数。在fine-tunning的时候，我们不需要重新训练其上面的层的参数，因此我们设置其上的每一层的requires_grad为false。之后在其上增加一个fc层，会默认设置该层的requires_grad的为true。下面设置学习方法，SGD，并且只需要设置新增加的fc层的学习参数即可。</p><h2 id="保存和读取模型"><a href="#保存和读取模型" class="headerlink" title="保存和读取模型"></a>保存和读取模型</h2><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Save <span class="keyword">and</span> load the entire <span class="keyword">model</span>.</span><br><span class="line">torch.save(resnet, <span class="string">'model.ckpt'</span>)</span><br><span class="line"><span class="keyword">model</span> = torch.load(<span class="string">'model.ckpt'</span>)</span><br><span class="line"></span><br><span class="line"># Save <span class="keyword">and</span> load only the <span class="keyword">model</span> <span class="keyword">parameters</span> (recommended).</span><br><span class="line">torch.save(resnet.state_dict(), <span class="string">'params.ckpt'</span>)</span><br><span class="line">resnet.load_state_dict(torch.load(<span class="string">'params.ckpt'</span>))</span><br></pre></td></tr></table></figure><h2 id="状态字典-state-dict"><a href="#状态字典-state-dict" class="headerlink" title="状态字典 state_dict"></a>状态字典 state_dict</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>state_dict 是一个简单的python的字典对象,将每一层与它的对应参数建立映射关系.(如model的每一层的weights及偏置等等)【只有那些参数可以训练的layer才会被保存到模型的state_dict中,如卷积层,线性层等等】。</p><p>优化器对象Optimizer也有一个state_dict,它包含了优化器的状态以及被使用的超参数(如lr, momentum,weight_decay等)</p><ul><li><p>state_dict是在定义了model或optimizer之后pytorch自动生成的,可以直接调用.常用的保存state_dict的格式是”.pt”或’.pth’的文件,即下面命令的 PATH=”./<em>*</em>.pt”。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">torch</span><span class="selector-class">.save</span>(<span class="selector-tag">model</span><span class="selector-class">.state_dict</span>(), <span class="selector-tag">PATH</span>)</span><br></pre></td></tr></table></figure></li><li><p>load_state_dict也是model或optimizer之后pytorch自动具备的函数,可以直接调用</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">model</span> = TheModelClass(*args, **kwargs)</span><br><span class="line"><span class="keyword">model</span>.load_state_dict(torch.load(PATH))</span><br><span class="line"><span class="keyword">model</span>.eval()</span><br></pre></td></tr></table></figure></li></ul><p>注意:model.eval() 的重要性,在2)中最后用到了model.eval(),是因为,只有在执行该命令后,”dropout层”及”batch normalization层”才会进入 evalution 模态. 而在”训练(training)模态”与”评估(evalution)模态”下,这两层有不同的表现形式.</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ul><li><p>仅保存学习到的参数,用以下命令</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">torch</span><span class="selector-class">.save</span>(<span class="selector-tag">model</span><span class="selector-class">.state_dict</span>(), <span class="selector-tag">PATH</span>)</span><br></pre></td></tr></table></figure></li><li><p>加载model.state_dict,用以下命令</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">model</span> = TheModelClass(*args, **kwargs)</span><br><span class="line"><span class="keyword">model</span>.load_state_dict(torch.load(PATH))</span><br><span class="line"><span class="keyword">model</span>.eval()</span><br></pre></td></tr></table></figure></li></ul><p>【备注】:model.load_state_dict的操作对象是<strong>一个具体的对象,而不能是文件名</strong></p><ul><li><p>保存整个model的状态,用以下命令</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">torch</span><span class="selector-class">.save</span>(<span class="selector-tag">model</span>,<span class="selector-tag">PATH</span>)</span><br></pre></td></tr></table></figure></li><li><p>加载整个model的状态,用以下命令:</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">Model</span> class must be defined somewhere</span><br><span class="line"><span class="keyword">model</span> = torch.load(PATH)</span><br><span class="line"><span class="keyword">model</span>.eval()</span><br></pre></td></tr></table></figure></li></ul><p>state_dict 是一个python的字典格式,以字典的格式存储,然后以字典的格式被加载,而且<strong>只加载key匹配的项</strong></p><ul><li><p>仅加载某一层的训练的到的参数(某一层的state)</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">conv1_weight_state</span> = torch.load(<span class="string">'./model_state_dict.pt'</span>)[<span class="string">'conv1.weight'</span>]</span><br></pre></td></tr></table></figure></li><li><p>加载模型参数后,如何设置某层某参数的”是否需要训练”(param.requires_grad)</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> list(model<span class="selector-class">.pretrained</span><span class="selector-class">.parameters</span>()):</span><br><span class="line">    param<span class="selector-class">.requires_grad</span> = False</span><br></pre></td></tr></table></figure></li></ul><p>【注意】requires_grad的操作对象是tensor.</p><p>【疑问】能否直接对某个层直接之用requires_grad呢?例如:model.conv1.requires_grad=False</p><p>【回答】经测试,不可以.model.conv1 没有requires_grad属性</p><h2 id="model-train和model-eval"><a href="#model-train和model-eval" class="headerlink" title="model.train和model.eval"></a>model.train和model.eval</h2><p>上面在讲解读取model的时候提到需要说明model.eval()和model.train()，两条语句有固定的使用场景。</p><p>他俩的主要区别是：<br>model.train()启用BatchNormalization 和 Dropout；model.eval()不启用BatchNormalization 和 Dropout</p><h2 id="每一轮batch需要设置optimizer-zero-grad"><a href="#每一轮batch需要设置optimizer-zero-grad" class="headerlink" title="每一轮batch需要设置optimizer.zero_grad"></a>每一轮batch需要设置optimizer.zero_grad</h2><p>根据pytorch中的backward()函数的计算，当网络参量进行反馈时，梯度是被积累的而不是被替换掉；但是在每一个batch时毫无疑问并不需要将两个batch的梯度混合起来累积，因此这里就需要每个batch设置一遍zero_grad了</p><h2 id="detach"><a href="#detach" class="headerlink" title="detach"></a>detach</h2><p>来源于<a href="https://www.jianshu.com/p/f1bd4ff84926" target="_blank" rel="noopener">简书</a><br>detach所做的就是,重新声明一个变量,指向原变量的存放位置,但是requires_grad为false.更深入一点的理解是,计算图从detach过的变量这里就断了, 它变成了一个leaf_node.即使之后重新将它的requires_node置为true,它也不会具有梯度.</p><p>另一方面,在调用完backward函数之后,非leaf_node的梯度计算完会立刻被清空.这也是为什么在执行backward之前显存占用很大,执行完之后显存占用立刻下降很多的原因.当然,这其中也包含了一些中间结果被存在buffer中,调用结束后也会被释放.<br>至于另一个参数volatile,如果一个变量的volatile=true,它可以将所有依赖于它的节点全部设为volatile=true,优先级高于requires_grad=true.这样的节点不会进行求导,即使requires_grad为真,也无法进行反向传播.在inference中如果采用这种设置,可以实现一定程度的速度提升,并且节约大概一半显存.</p><h2 id="BatchNorm2d的参数解释"><a href="#BatchNorm2d的参数解释" class="headerlink" title="BatchNorm2d的参数解释"></a>BatchNorm2d的参数解释</h2><p>来源<a href="https://www.jianshu.com/p/a646cbc913b4" target="_blank" rel="noopener">简书</a>。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BatchNorm1d(num_features,</span><br><span class="line"><span class="attribute">eps</span>=1e-05,</span><br><span class="line"><span class="attribute">momentum</span>=0.1,</span><br><span class="line"><span class="attribute">affine</span>=<span class="literal">True</span>,</span><br><span class="line"><span class="attribute">track_running_stats</span>=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="resnet实现"><a href="#resnet实现" class="headerlink" title="resnet实现"></a>resnet实现</h2><h3 id="ResNet模块"><a href="#ResNet模块" class="headerlink" title="ResNet模块"></a>ResNet模块</h3><p>来源<a href="https://blog.csdn.net/Dr_destiny/article/details/85253158" target="_blank" rel="noopener">CSDN</a></p><p>如下的左图对应于resnet-18/34使用的基本块，右图是50/101/152所使用的，由于他们都比较深，所以右图相比于左图使用了1x1卷积来降维。<br><img src="/2019/04/25/pytorch-learn/2.png" width="2"></p><ul><li><p>conv3x3:将原有的pytorch函数固定卷积和尺寸为3重新封装了一次</p></li><li><p>BasicBlock:搭建上图左边的模块</p></li></ul><blockquote><p>每个卷积块后面连接BN层进行归一化；</p><p>残差连接前的3x3卷积之后只接入BN，不使用ReLU，避免加和之后的特征皆为正，保持特征的多样；</p><p>跳层连接：两种情况，当模块输入和残差支路（3x3-&gt;3x3）的通道数一致时，直接相加；当两者通道不一致时（一般发生在分辨率降低之后，同分辨率一般通道数一致），需要对模块输入特征使用1x1卷积进行升/降维（步长为2，上面说了分辨率会降低），之后同样接BN，不用ReLU。</p></blockquote><ul><li>Bottleneck:搭建上图右边的模块</li></ul><blockquote><p>使用1x1卷积先降维，再使用3x3卷积进行特征提取，最后再使用1x1卷积把维度升回去；</p><p>每个卷积块后面连接BN层进行归一化；</p><p>残差连接前的1x1卷积之后只接入BN，不使用ReLU，避免加和之后的特征皆为正，保持特征的多样性。</p><p>跳层连接：两种情况，当模块输入和残差支路（1x1-&gt;3x3-&gt;1x1）的通道数一致时，直接相加；当两者通道不一致时（一般发生在分辨率降低之后，同分辨率一般通道数一致），需要对模块输入特征使用1x1卷积进行升/降维（步长为2，上面说了分辨率会降低），之后同样接BN，不用ReLU。</p></blockquote><h2 id="设置学习率衰减"><a href="#设置学习率衰减" class="headerlink" title="设置学习率衰减"></a>设置学习率衰减</h2><h3 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h3><p>对学习率（learning rate）进行衰减，下面的代码示范了如何每30个epoch按10%的速率衰减：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate</span><span class="params">(optimizer, epoch)</span>:</span></span><br><span class="line">    <span class="string">"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""</span></span><br><span class="line">    lr = args.lr * (<span class="number">0.1</span> ** (epoch // <span class="number">30</span>))</span><br><span class="line">    <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">        param_group[<span class="string">'lr'</span>] = lr</span><br></pre></td></tr></table></figure></p><h3 id="什么是param-groups"><a href="#什么是param-groups" class="headerlink" title="什么是param_groups?"></a>什么是param_groups?</h3><p>optimizer通过param_group来管理参数组.param_group中保存了参数组及其对应的学习率,动量等等.所以我们可以通过更改param_group[‘lr’]的值来更改对应参数组的学习率。Pytorch可以使用多个参数组，从而使网络的不通部分拥有不同的优化方式。(Pytorch不同的层使用不同的学习率参考：pytorch在不同的层使用不同的学习率)[<a href="https://blog.csdn.net/liuweiyuxiang/article/details/84647659" target="_blank" rel="noopener">https://blog.csdn.net/liuweiyuxiang/article/details/84647659</a>]</p><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有两个`param_group`即,len(optim.param_groups)==2</span></span><br><span class="line"><span class="comment"># 没有指定学习率的group使用后后面lr参数指定的默认学习率。</span></span><br><span class="line">optim.SGD([</span><br><span class="line">                &#123;'params': <span class="title">model</span>.base.<span class="title">parameters</span>()&#125;,<span class="comment">#lr=lr=1e-2</span></span><br><span class="line">                &#123;'params': <span class="title">model</span>.classifier.<span class="title">parameters</span>(), 'lr': <span class="number">1e-3</span>&#125;</span><br><span class="line">            ], lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#一个参数组</span></span><br><span class="line">optim.SGD(<span class="title">model</span>.<span class="title">parameters</span>(), lr=<span class="number">1e-2</span>, momentum=<span class="number">.9</span>)</span><br></pre></td></tr></table></figure><h2 id="inplace-True字段"><a href="#inplace-True字段" class="headerlink" title="inplace=True字段"></a>inplace=True字段</h2><p>在例如nn.ReLU(inplace=True)中的inplace字段是什么意思呢？有什么用？</p><p><strong>inplace=True的意思是进行原地操作，例如x=x+5，对x就是一个原地操作，y=x+5,x=y，完成了与x=x+5同样的功能但是不是原地操作，上面LeakyReLU中的inplace=True的含义是一样的，是对于Conv2d这样的上层网络传递下来的tensor直接进行修改，好处就是可以节省运算内存，不用多储存变量y</strong></p><h2 id="多标签分类与BCEloss"><a href="#多标签分类与BCEloss" class="headerlink" title="多标签分类与BCEloss"></a>多标签分类与BCEloss</h2><p>参考<a href="https://www.jianshu.com/p/ac3bec3dde3e" target="_blank" rel="noopener">简书</a> 和 <a href="https://blog.csdn.net/u010995990/article/details/79739887" target="_blank" rel="noopener">CSDN</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;pytorch的相关学习记录、困惑、学习笔记等，持续更新。&lt;br&gt;
    
    </summary>
    
      <category term="Technical guide" scheme="https://zeyuxiao1997.github.io/categories/Technical-guide/"/>
    
    
      <category term="pytorch" scheme="https://zeyuxiao1997.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>paper——Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/22/paper-SRGAN/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/22/paper-SRGAN/</id>
    <published>2019-04-22T05:40:12.000Z</published>
    <updated>2019-04-23T14:56:26.690Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2017的论文，第一篇使用GAN做SR的。由于SRGAN年代较为久远、网络结构较为经典，网上有很多解读。我主要参考的链接将出现在reference部分。<br>事实上，文章的出发点基于：高的数值准确性并不能表征高的人体感知，也就是说，PSNR高的值在人眼中不一定视觉效果好。<br><a id="more"></a><br>论文可<a href="https://arxiv.org/pdf/1609.04802v1.pdf" target="_blank" rel="noopener">点击下载</a></p><h2 id="一、Introduction"><a href="#一、Introduction" class="headerlink" title="一、Introduction"></a>一、Introduction</h2><h3 id="3-文中claim的contributions"><a href="#3-文中claim的contributions" class="headerlink" title="3. 文中claim的contributions"></a>3. 文中claim的contributions</h3><ul><li><p>通过对16块深度ResNet (SRResNet)进行MSE优化，通过PSNR（峰值信噪比）和结构相似性(SSIM)测量了具有高上标度因子(4x)的图像SR，并为其设置了一种新的技术的最新水平。</p></li><li><p>我们提出了一种基于神经网络的感知损失优化算法。在这里，我们将基于mse的内容损失替换为基于VGG网络[49]特征图计算的损失，该特征图对像素空间[38]的变化更加不变。</p></li><li><p>我们通过对三个公共基准数据集的图像进行广泛的平均意见得分(MOS)测试，证实了SRGAN是一种新的技术状态，在很大程度上可以用于估计具有高放大因子的照片真实感SR图像(4x)。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CVPR2017的论文，第一篇使用GAN做SR的。由于SRGAN年代较为久远、网络结构较为经典，网上有很多解读。我主要参考的链接将出现在reference部分。&lt;br&gt;事实上，文章的出发点基于：高的数值准确性并不能表征高的人体感知，也就是说，PSNR高的值在人眼中不一定视觉效果好。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="GAN" scheme="https://zeyuxiao1997.github.io/tags/GAN/"/>
    
      <category term="SR" scheme="https://zeyuxiao1997.github.io/tags/SR/"/>
    
  </entry>
  
  <entry>
    <title>paper——Benchmarking Denoising Algorithms with Real Photographs</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/21/paper-denoise/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/21/paper-denoise/</id>
    <published>2019-04-21T15:22:23.000Z</published>
    <updated>2019-04-21T15:24:56.873Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="denoising" scheme="https://zeyuxiao1997.github.io/tags/denoising/"/>
    
  </entry>
  
  <entry>
    <title>Important network structure——C3D</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/21/class-C3D/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/21/class-C3D/</id>
    <published>2019-04-21T14:29:04.000Z</published>
    <updated>2019-04-22T07:27:32.854Z</updated>
    
    <content type="html"><![CDATA[<p>C3D，也就是3D卷积神经网络，首先被提出并应用于action recognition。论文距今已经有了一些年月，这个网络现在对于行为识别已经有点过时了，只是里面的3D卷积成为了经典，没有花里胡哨的连接，只有传统网络的一条路，卷积，池化，分类。<br><a id="more"></a><br>论文可<a href="http://vlg.cs.dartmouth.edu/c3d/c3d_video.pdf" target="_blank" rel="noopener">点击下载</a></p><p>论文之前在看过，这里就不再写笔记了，我直接写3D卷积部分。</p><h2 id="关于C3D"><a href="#关于C3D" class="headerlink" title="关于C3D"></a>关于C3D</h2><p>C3D网络设计主要是用来解决Action Recognition,之前有用2D-CNN网络来识别的，但是2D的不能很好的提取时间特性，所以效果也不是很好。</p><img src="/2019/04/21/class-C3D/1.png" width="1"><p>a)和b)分别为2D卷积用于单通道图像和多通道图像的情况（此处多通道图像可以指同一张图片的3个颜色通道，也指多张堆叠在一起的图片，即一小段视频），对于一个滤波器，输出为一张二维的特征图，多通道的信息被完全压缩了。而c)中的3D卷积的输出仍然为3D的特征图。</p><p>现在考虑一个视频段输入，其大小为 $c \times l \times h \times w$ ,其中c为图像通道(一般为3),l为视频序列的长度，h和w分别为视频的宽与高。进行一次kernel size为$3 \times 3$,stride为1,padding=True,滤波器个数为K的3D 卷积后，输出的大小为$K \times l \times h \times w$。池化同理。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>C3D使用3D CNN构造了一个效果不错的网络结构，对于基于视频的问题均可以用来提取特征。可以将其全连接层去掉，将前面的卷积层放入自己的模型中，就像使用预训练好的VGG模型一样。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;C3D，也就是3D卷积神经网络，首先被提出并应用于action recognition。论文距今已经有了一些年月，这个网络现在对于行为识别已经有点过时了，只是里面的3D卷积成为了经典，没有花里胡哨的连接，只有传统网络的一条路，卷积，池化，分类。&lt;br&gt;
    
    </summary>
    
      <category term="Important network structure" scheme="https://zeyuxiao1997.github.io/categories/Important-network-structure/"/>
    
    
      <category term="CNN" scheme="https://zeyuxiao1997.github.io/tags/CNN/"/>
    
      <category term="3D" scheme="https://zeyuxiao1997.github.io/tags/3D/"/>
    
  </entry>
  
  <entry>
    <title>paper——Fast Spatio-Temporal Residual Network for Video Super-Resolution(FSTRN)</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/21/paper-STRVSR/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/21/paper-STRVSR/</id>
    <published>2019-04-21T13:31:16.000Z</published>
    <updated>2019-04-21T13:42:28.884Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2019新作,阅读笔记敬请期待。<br><a id="more"></a><br>论文可<a href="/download/1904.02870.pdf">点击下载</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CVPR2019新作,阅读笔记敬请期待。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="VSR" scheme="https://zeyuxiao1997.github.io/tags/VSR/"/>
    
  </entry>
  
  <entry>
    <title>paper——Camera Lens Super-Resolution</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/21/paper-CameraSR/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/21/paper-CameraSR/</id>
    <published>2019-04-21T02:08:11.000Z</published>
    <updated>2019-04-21T11:14:18.182Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2019新作，一篇关于真实图像的超分辨率作品，还是具有开创意义的。和之前那篇《Bridging the Simulated-to-Real Gap:Benchmarking Super-Resolution on Real Data》一样，也是自己手工搭建了一套数据集。文章从R-V trade-off入手，也就是视野和分辨率之间的权衡，构建了一套（HR，LR）数据集并提出CameraSR方法，较好的提高真实图像场景下的分辨率。<br><a id="more"></a><br>论文可<a href="/download/cameraSR.pdf">点击下载</a>.</p><h2 id="一、Introduction"><a href="#一、Introduction" class="headerlink" title="一、Introduction"></a>一、Introduction</h2><h3 id="1-motivation"><a href="#1-motivation" class="headerlink" title="1. motivation"></a>1. motivation</h3><p>现有的SR方法都是基于bicubic downsampling和gaussian downsampling的degradation方法。但是真实的图像系统不能这么搞，因为存在其他更复杂的影响因素。因此作者探究了一个分辨率和视野的一个权衡，通过学习R-V degradation（resolution vs field-of-view）的这么一个过程提高了实际情况下的分辨率。</p><p>下图分别说明R-V degradation对分辨率的影响和重建之后的效果图<br><img src="/2019/04/21/paper-CameraSR/1.png" width="1"><br><img src="/2019/04/21/paper-CameraSR/2.png" width="2"><br>上图表明，R-V确实对分辨率有较大影响，必须考虑R-V才能对重建产生积极影响。</p><h3 id="2-文中claim的contributions"><a href="#2-文中claim的contributions" class="headerlink" title="2. 文中claim的contributions"></a>2. 文中claim的contributions</h3><ul><li>考虑真实世界图片系统的重建，本文的切入点是R-V degradation of camera lense</li><li>获取LR-HR pair的新方法</li><li>将常用的SR方法迁移到真实数据集上</li></ul><h3 id="3-几个有用的前人工作"><a href="#3-几个有用的前人工作" class="headerlink" title="3. 几个有用的前人工作"></a>3. 几个有用的前人工作</h3><ul><li><p>[1] introduced more degradation operators into the bicubicdownsampled LR images, including motion blur and Poisson noise</p></li><li><p>[4] defined the LR face images with the low-quality assumptions (e.g., noise, blur, and compression artifacts) and trained a GAN to learn the degradation process</p></li><li><p>[20] estimated the degradation model relying on the inherent recurrence of the input image</p></li><li><p>[23]  further optimized an imagespecific CNN with examples solely extracted from the input image </p></li></ul><p>和上面几个重要的前人工作相比，cameraSR使用真实图像采集系统进行操作，参考[21]，对degradation过程进行建模。高ISO值捕获的对象被定义为noise并且低ISO值捕获的相同对象被定义为clean。 将此定义扩展到SR场景，该场景解决了获得真实LR-HR图像对的关键挑战将此定义扩展到SR场景，该场景解决了获得真实LR-HR图像对的关键挑战</p><p>采用VDSR [13]和SRGAN [16]作为两个代表性实施例来证明CameraSR的有效性和普遍性，可以用任何基于CNN的方法代替。</p><h2 id="二、Method"><a href="#二、Method" class="headerlink" title="二、Method"></a>二、Method</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>一个既定的事实是：<font color="#FF0000">缩小镜头会以牺牲对象的分辨率损失为代价获得更大的FoV</font>。假设R-V退化过程是$D_{R V}(\cdot)$，我们的目标是获得$S(\cdot)$，方程可以写作：<script type="math/tex">\hat{X}=S\left(D_{R V}(X)\right)</script>，其中$\hat{X}$和$X$分别表示super-resolved和原图。</p><p>要注意的是，针对不同的退化方法，表达式可以进行变化。$\hat{X}=S\left(D_{B i c}(X)\right)$为bicubic downsample；更复杂一点可以表示为$\hat{X}=S\left(D_{B l u r}\left(D_{B i c}(X)\right)+v\right)$，where $D_{B l u r}(\cdot)$是blurring operator，$v$是某一个类型的噪声。</p><p>使用深度学习方法进行隐式计算$D_{R V}(\cdot)$，parametric SR function为$S_{\Theta}(\cdot)$，<script type="math/tex">\hat{X}=S_{\Theta}\left(\hat{D}_{R V}(X)\right)</script>，大数定理表明$\hat{D}_{R V}(\cdot) \rightarrow {D}_{R V}$，loss function是</p><script type="math/tex; mode=display">\min _{\Theta} \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}\left(X_{i}-S_{\Theta}\left(Y_{i}\right)\right)</script><h3 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h3><p>在采数据的时候出现了包括空间错位，强度变化和颜色不匹配的问题。 这可能是由于焦距的变化是一个无法理想控制的机械过程。 因此，它导致相机机身的轻微抖动以及曝光配置</p><p>然后使了一套方法进行校正标定</p><h2 id="三、结论和讨论"><a href="#三、结论和讨论" class="headerlink" title="三、结论和讨论"></a>三、结论和讨论</h2><ul><li>没有更多的在采集数据的时候考虑噪声</li><li>可以加入时间维</li><li>self-similarity based methods to utilize the inherent recurrence</li></ul><h2 id="四、启发"><a href="#四、启发" class="headerlink" title="四、启发"></a>四、启发</h2><ul><li>气候信息是有噪声的，我是不是可以用什么方法把噪声去了或者减轻噪声的影响？</li><li>有什么好的去噪方法可以尝试的呢？把噪声去了就好了</li><li>除了考虑时间，我是不是也可以加强先验呢？</li></ul><h2 id="references"><a href="#references" class="headerlink" title="references"></a>references</h2><p>[1] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In CVPR, 2018.<br>[4] Adrian Bulat, Jing Yang, and Georgios Tzimiropoulos. To learn image super-resolution, use a gan to learn how to do image degradation first. In ECCV, 2018.<br>[20]Tomer Michaeli and Michal Irani. Nonparametric blind super-resolution. In CVPR, 2013.<br>[23] Assaf Shocher, Nadav Cohen, and Michal Irani. zero-shot super-resolution using deep internal learning. In CVPR, 2018.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CVPR2019新作，一篇关于真实图像的超分辨率作品，还是具有开创意义的。和之前那篇《Bridging the Simulated-to-Real Gap:Benchmarking Super-Resolution on Real Data》一样，也是自己手工搭建了一套数据集。文章从R-V trade-off入手，也就是视野和分辨率之间的权衡，构建了一套（HR，LR）数据集并提出CameraSR方法，较好的提高真实图像场景下的分辨率。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="SR" scheme="https://zeyuxiao1997.github.io/tags/SR/"/>
    
  </entry>
  
  <entry>
    <title>paper——Deep High-Resolution Representation Learning for Human Pose Estimation(HRnet)</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/20/paper-HRnet/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/20/paper-HRnet/</id>
    <published>2019-04-20T01:51:49.000Z</published>
    <updated>2019-04-22T07:35:37.292Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2019新作，各大微信公众号已经吹爆了，我的个人解读即将出炉。。。。<br><a id="more"></a><br>论文可<a href="/download/1902.09212.pdf">点击下载</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CVPR2019新作，各大微信公众号已经吹爆了，我的个人解读即将出炉。。。。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="net structure" scheme="https://zeyuxiao1997.github.io/tags/net-structure/"/>
    
  </entry>
  
  <entry>
    <title>paper——Bridging the Simulated-to-Real Gap:Benchmarking Super-Resolution on Real Data</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/18/paper-1809-06420/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/18/paper-1809-06420/</id>
    <published>2019-04-18T12:50:17.000Z</published>
    <updated>2019-04-21T13:42:26.045Z</updated>
    
    <content type="html"><![CDATA[<p>本文是TPAMI的论文，构建了一个真实世界下的LR-HR数据集，综合考虑了CMOS sensor noise, real sampling at four resolution levels, nine scene motion types, two photometric conditions, and lossy video coding at five levels这几个不同的影响因素；可以说，这篇文章将SR的发展又向前推进了一大步。<font color="#FF0000">  尤其值得注意的一点是：在模拟数据上表现好的方法在真实数据集上表现的并不好</font>。<br><a id="more"></a><br>论文可<a href="/download/1809.06420.pdf">点击下载</a>.</p><h2 id="一、Introduction"><a href="#一、Introduction" class="headerlink" title="一、Introduction"></a>一、Introduction</h2><h3 id="1-motivation"><a href="#1-motivation" class="headerlink" title="1. motivation"></a>1. motivation</h3><p>事实上，现有的SISR、MISR、VSR方法都是基于模拟方法获取pair并在此基础上进一步实现相关算法，简单来说，bicubic interpolation就是一个获取模拟数据的直接方法。真实SR数据更容易在精细结构处受到噪声影响，下图就是一个真实的例子。<br><img src="/2019/04/18/paper-1809-06420/1.png" title="real-vs-simulation"></p><h3 id="2-contributions"><a href="#2-contributions" class="headerlink" title="2.contributions"></a>2.contributions</h3><ul><li>propose SupER database (<a href="https://superresolution.tf.fau.de/" target="_blank" rel="noopener">https://superresolution.tf.fau.de/</a>)</li><li>uncover mismatches between quantitative evaluations and human perception</li><li>etc</li></ul><h3 id="3-你通过读论文可以获得什么？"><a href="#3-你通过读论文可以获得什么？" class="headerlink" title="3.你通过读论文可以获得什么？"></a>3.你通过读论文可以获得什么？</h3><ul><li>existing SR datasets and evaluation strategies</li><li>introduce the proposed benchmark database</li><li>underlying evaluation protocol</li><li>evaluate current SR approaches quantitatively and in the observer study</li><li>draw conclusions for future algorithm developments</li></ul><h3 id="Benchmarking-on-Simulated-Data"><a href="#Benchmarking-on-Simulated-Data" class="headerlink" title="Benchmarking on Simulated Data"></a>Benchmarking on Simulated Data</h3><p>All of these benchmarks have in common that only simplistic sampling kernels that are known a priori (e.g. bicubic [37] or Gaussian kernels [24], [39]) are simulated but SR in case of more general kernels is unexplored</p><p>模拟数据的使用使得能够通过全参考质量评估方法来比较算法性能，但限制了在实际约束下的性能。 物理上有意义的采样内核、真实噪声模型或环境条件对SR的影响是巨大的。</p><p>简化的模拟方法对真实世界真实图像的性能差异没有被很好的评估。作者做了<font color="#FF0000"> address this question and show the overall weak correlations between benchmarks on simulated and real data termed simulated-to-real gap</font>。</p><h3 id="Benchmarking-on-Real-Data"><a href="#Benchmarking-on-Real-Data" class="headerlink" title="Benchmarking on Real Data"></a>Benchmarking on Real Data</h3><font color="#FF0000">Work aims at broadly benchmarking SR algorithms on real captured images</font>.Qu et.al.[48]在多摄像机设置中所需的LR/HR对齐可能会受到容易出错的校准和图像配准的影响。 这使得用于像素比较的全参考质量测量的使用不可靠。 此外，[48]的数据仅包括单个图像，不包括MFSR。 我们提出单相机设置，避免这些限制，并允许我们获得两个以上的分辨率级别。### 数据集的获取和细节原文是这样的：We collect sets of LR and HR images at multiple resolutions with a single camera by capturing stop-motion videos. At each time step of a stop-motion video, the underlying scene, environmental conditions, and the camera pose are kept static. For consecutive time steps, the scene undergoes changes related to camera and/or object movements and/or environmental variations.一个time step由一个n+1维元组构成，元组为：$\left(\boldsymbol{X}_{\mathrm{gt}}, \boldsymbol{Y}_{b_{1}}, \boldsymbol{Y}_{b_{2}} \ldots, \boldsymbol{Y}_{b_{n}}\right)$，其中$\boldsymbol{X}_{\mathrm{gt}}$是ground truth，size=$N_{u} \times N_{v}$，$\boldsymbol{Y}_{b_{i}}, i=1, \dots, n$是LR帧，size=$N_{u} / b_{i} \times N_{v} / b_{i}$,$b_{i}$是binning值。### Image Formation使用取平均降低sensor noise，得到最终的ground truth。$$\boldsymbol{X}_{\mathrm{gt}}=\frac{1}{L} \sum_{l=1}^{L} \boldsymbol{X}^{(l)}$$<font color="#00FFFF"> 然后讲的是LR图片的获取，这一块涉及到image acquisition相关的理论，等到暑假的时候再看看吧，现在看不懂。 </font><p>大多数SR算法处理灰度或单个亮度通道，而色度通道只是插值[13]，[25]，[37]，[49]。 因此，我们仅限于单色采集，以在硬件要求和实际适用性之间进行折衷。 为了研究全色SR，可以推广设置以提供多个通道，例如， G。 使用彩色滤光片或全RGB相机。</p><h3 id="Image-Postprocessing"><a href="#Image-Postprocessing" class="headerlink" title="Image Postprocessing"></a>Image Postprocessing</h3><p>考虑了视频压缩等因素</p><h3 id="运动和光照"><a href="#运动和光照" class="headerlink" title="运动和光照"></a>运动和光照</h3><img src="/2019/04/18/paper-1809-06420/3.png" title="facility"><p>运动由设备控制（上升 下降 旋转 综合运动等），环境光照用明暗光进行控制—为了模拟白天和夜晚【光度条件由人工照明控制，我们考虑明亮（日光）和弱光照明（夜光）。 结合场景中对象的移动，这形成了具有不同难度级别的五个数据集类别】。</p><img src="/2019/04/18/paper-1809-06420/2.png" title="table"><p>下面是5种数据集类型:</p><ul><li>global motion: 包含具有恒定日光条件的静态场景。 所有帧间运动是使用Tab中具有均匀分布的相机位置的轨迹的全局相机运动</li><li>local motion: 包括在日光条件下使用静态相机拍摄但移动物体的动态场景，所有帧间运动是平移和/或旋转物体运动</li><li>mixed motion: 结合了全局和局部运动。 为此，每个相机轨迹与平移和/或旋转物体运动相结合</li><li>phptpmetric variation: 包括K帧序列，其中第一个K-Knight帧取自全局，局部和混合运动数据，剩余的Knight异常值帧在夜间条件下获得</li><li>Video compression: 进一步将上述数据集扩展了五个H.265 / HEVC压缩级别，即。即 所有LR图像都以未压缩和压缩的形式提供</li></ul><h3 id="和现有数据集的比较"><a href="#和现有数据集的比较" class="headerlink" title="和现有数据集的比较"></a>和现有数据集的比较</h3><img src="/2019/04/18/paper-1809-06420/4.png" title="table"><h4 id="比较pinning"><a href="#比较pinning" class="headerlink" title="比较pinning"></a>比较pinning</h4><img src="/2019/04/18/paper-1809-06420/5.png" title="pinning_compare"><h4 id="单camera和多camera对比"><a href="#单camera和多camera对比" class="headerlink" title="单camera和多camera对比"></a>单camera和多camera对比</h4><ul><li>保证LR和ground truth图像的对齐</li><li>可使用full-reference质量评价指标进行pixel-wise比较</li><li>避免多camera设置出错的可能</li><li>存在多个分辨率等级，收集图像序列而不是单个图像；这使得数据可用于SISR和MFSR</li></ul><h2 id="二、比较和结论"><a href="#二、比较和结论" class="headerlink" title="二、比较和结论"></a>二、比较和结论</h2><h3 id="评价方法"><a href="#评价方法" class="headerlink" title="评价方法"></a>评价方法</h3><p>使用$K=2 M+1$个连续LR帧$\boldsymbol{Y}^{(-M)}, \ldots, \boldsymbol{Y}^{(0)}, \ldots, \boldsymbol{Y}^{(M)}$。$\boldsymbol{Y}^{(0)}$是参照帧。对于SISR，$\boldsymbol{Y}^{(0)}$对应于HR帧$X_{sr}$；在MFSR中$\boldsymbol{Y}^{(-M)}, \ldots, \boldsymbol{Y}^{(M)}$需要使用optical flow算法进行中间帧$\boldsymbol{Y}^{(0)}$的估计。</p><p>评价指标使用的是PSNR、SSIM、MS-SSIM、IFC、S3、BRISQUE、SSEQ、NIQE和SRM【Higher S3 and SRM measure express higher perceptual quality of the assessed SR image. For BRISQUE, SSEQ and NIQE, we used the negated scores such that higher measures express higher quality.】由于场景会影响这些值，使用标准化的质量度量：</p><p><script type="math/tex">\tilde{Q}\left(\boldsymbol{X}_{\mathrm{sr}}\right)=\left(Q\left(\boldsymbol{X}_{\mathrm{sr}}\right)-Q(\tilde{\boldsymbol{Y}})\right) / Q(\tilde{\boldsymbol{Y}})</script>，where $\tilde{Y}$ denotes the nearest-neighbor interpolation of the reference frame $\boldsymbol{Y}^{(0)}$ on the target HR grid.</p><h3 id="比较方法"><a href="#比较方法" class="headerlink" title="比较方法"></a>比较方法</h3><p>DL方法有三个，SRCNN、VDSR、和DRCN</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><img src="/2019/04/18/paper-1809-06420/6.png" title="result1"><img src="/2019/04/18/paper-1809-06420/7.png" title="result2"><img src="/2019/04/18/paper-1809-06420/8.png" title="result3"><img src="/2019/04/18/paper-1809-06420/9.png" title="result4"><h2 id="三、一点感想"><a href="#三、一点感想" class="headerlink" title="三、一点感想"></a>三、一点感想</h2><ul><li>本来还想看看这篇文章对VSR的分析，结果竟然没有！！！！未来也许可以使用deep learning方法分析一波VSR在真实数据集的表现；</li><li>关于肉眼视觉质量评价和数值质量评价，我将会在后面的阅读论文中着重学习，这应该是一个很好的点。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是TPAMI的论文，构建了一个真实世界下的LR-HR数据集，综合考虑了CMOS sensor noise, real sampling at four resolution levels, nine scene motion types, two photometric conditions, and lossy video coding at five levels这几个不同的影响因素；可以说，这篇文章将SR的发展又向前推进了一大步。&lt;font color=&quot;#FF0000&quot;&gt;  尤其值得注意的一点是：在模拟数据上表现好的方法在真实数据集上表现的并不好&lt;/font&gt;。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="SR" scheme="https://zeyuxiao1997.github.io/tags/SR/"/>
    
      <category term="real-data" scheme="https://zeyuxiao1997.github.io/tags/real-data/"/>
    
      <category term="datasets" scheme="https://zeyuxiao1997.github.io/tags/datasets/"/>
    
  </entry>
  
  <entry>
    <title>several useful tools--updating</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/18/useful-tool/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/18/useful-tool/</id>
    <published>2019-04-18T08:11:48.000Z</published>
    <updated>2019-05-05T06:50:00.313Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章用于记录我在工作和学习中发现的好工具、软件等。文章将持续更新。<br><a id="more"></a></p><h2 id="github大文件的下载"><a href="#github大文件的下载" class="headerlink" title="github大文件的下载"></a>github大文件的下载</h2><p>github仓库在好大的时候基本上下载不了。找了几个解决这个问题的方法。</p><h3 id="使用插件进行下载"><a href="#使用插件进行下载" class="headerlink" title="使用插件进行下载"></a>使用插件进行下载</h3><p>还算是可以用，但是但是，下载总会出现一些东西下载不了，emmm这就比较郁闷了，不过该下的基本上都会下下来。<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">npm install git-down-repo -g <span class="comment">// 安装全局</span></span><br><span class="line"><span class="comment">// test </span></span><br><span class="line">gitdown <span class="string">https:</span><span class="comment">//github.com/hua1995116/webchat  // 下载整个仓库（默认master）</span></span><br><span class="line">gitdown <span class="string">https:</span><span class="comment">//github.com/hua1995116/webchat dev // 下载某个仓库的dev分支</span></span><br><span class="line">gitdown <span class="string">https:</span><span class="comment">//github.com/hua1995116/webchat/tree/master/config // 下载仓库某个文件夹</span></span><br><span class="line">gitdown <span class="string">https:</span><span class="comment">//github.com/hua1995116/webchat/blob/master/config/dev.env.js // 下载某个文件</span></span><br></pre></td></tr></table></figure></p><h2 id="不用梯子看油管"><a href="#不用梯子看油管" class="headerlink" title="不用梯子看油管"></a>不用梯子看油管</h2><p><a href="https://www.clipconverter.cc/" target="_blank" rel="noopener">https://www.clipconverter.cc/</a>用这个网站就好了。</p><p>但是！！！！需要视频的链接，emmm这是个大问题，这样看的话只能在有链接的情况下看了。<br><img src="/2019/04/18/useful-tool/1.png" title="compare"></p><h2 id="使用wget进行下载"><a href="#使用wget进行下载" class="headerlink" title="使用wget进行下载"></a>使用wget进行下载</h2><p>Wget主要用于下载文件，在安装软件时会经常用到，以下对wget做简单说明。</p><ul><li><p>下载单个文件：<figure class="highlight plain"><figcaption><span>http://www.baidu.com```。命令会直接在当前目录下载一个index.html的文件</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 将下载的文件存放到指定的文件夹下，同时重命名下载的文件，利用```-O：wget -O /home/index http://www.baidu.com</span><br></pre></td></tr></table></figure></p></li><li><p>下载多个文件：首先，创建一个file.txt文件，写入两个url（换行），如<a href="http://www.baidu.com;然后，wget" target="_blank" rel="noopener">http://www.baidu.com;然后，wget</a> -i file.txt;命令执行后会下载两个两个文件。</p></li><li><p>下载时，不显示详细信息，即在后台下载：wget -b <a href="http://www.baidu.com。命令执行后会，下载的详细信息不会显示在终端，会在当前目录下生成一个web-log记录下载的详细信息。" target="_blank" rel="noopener">http://www.baidu.com。命令执行后会，下载的详细信息不会显示在终端，会在当前目录下生成一个web-log记录下载的详细信息。</a></p></li><li><p>下载时，不显示详细信息，同时将下载信息保存到执行的文件中（同4）：<figure class="highlight plain"><figcaption><span>-o dw.txt</span><a href="http://www.baidu.com```" target="_blank" rel="noopener">link</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 断点续传：```wget -c http://www.baidu.com</span><br></pre></td></tr></table></figure></p></li><li><p>限制下载的的速度：<figure class="highlight plain"><figcaption><span>--limit-rate</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 测试是否能正常访问：```wget --spider http://www.baidu.com</span><br></pre></td></tr></table></figure></p></li><li><p>设置下载重试的次数：<figure class="highlight plain"><figcaption><span>--tries</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 下载的过程中拒绝下载指定类型的文件:```wget --reject=png --mirror -p --convert-links -P./test http://localhost</span><br></pre></td></tr></table></figure></p></li><li><p>多文件下载中拒绝下载超过设置大小的文件：<figure class="highlight plain"><figcaption><span>-Q5m -i file.txt```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">　　注意：此选项只能在下载多个文件时有用，当你下载一个文件时没用。</span><br><span class="line"></span><br><span class="line">- 从指定网站中下载所有指定类型的文件：```wget -r -A .png http://www.baidu.com</span><br></pre></td></tr></table></figure></p></li><li><p>wget下载时，某些资源必须使用<figure class="highlight plain"><figcaption><span>http://www.baidu.com```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 使用wget实现FTP下载：```wget --file-user=USERNAME --file-password=PASSWORD url</span><br></pre></td></tr></table></figure></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章用于记录我在工作和学习中发现的好工具、软件等。文章将持续更新。&lt;br&gt;
    
    </summary>
    
      <category term="tools" scheme="https://zeyuxiao1997.github.io/categories/tools/"/>
    
    
      <category term="github" scheme="https://zeyuxiao1997.github.io/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>SURVEY——A Deep Journey into Super-resolution-A Survey</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/18/review-SISR-2papers/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/18/review-SISR-2papers/</id>
    <published>2019-04-18T04:44:37.000Z</published>
    <updated>2019-04-20T05:38:56.235Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是我在arxiv上偶然看见的，发布于2019年4月中旬，可以和华南理工的综述论文结合起来看。《A Deep Journey into Super-resolution: A Survey》一文是一篇很好的综述，总结了30+种基于深度学习的超分辨率网络，并将之分类为<font color="#FF0000"> linear, residual, multi-branch, recursive, progressive, attention-based and adversarial等九类</font>，并详细比较了<font color="#FF0000">网络复杂性，内存占用，模型输入和输出，学习细节，网络损耗类型和重要架构差异（例如，深度，跳过连接，过滤器）</font>。<br><a id="more"></a><br>两篇文章的链接分别为：(<a href="https://arxiv.org/abs/1904.07523)和(https://arxiv.org/abs/1902.06068" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07523)和(https://arxiv.org/abs/1902.06068</a>)</p><h1 id="总体介绍"><a href="#总体介绍" class="headerlink" title="总体介绍"></a>总体介绍</h1><h2 id="SISR目前面临的challenge"><a href="#SISR目前面临的challenge" class="headerlink" title="SISR目前面临的challenge"></a>SISR目前面临的challenge</h2><ul><li>ill-posed inverse problem</li><li>只能SR小的scale factor</li><li>评价指标和人类感知差异巨大</li></ul><h2 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h2><ul><li>回顾SISR方法</li><li>提出基于结构差异的SR分类法</li><li>分析参数、算法设计、训练细节、结构创新对性能的提升</li><li>系统给出SISR在不同数据集的表现</li><li>分析挑战、展望未来</li></ul><h2 id="SR表示"><a href="#SR表示" class="headerlink" title="SR表示"></a>SR表示</h2><p>LR为$y$，HR为$x$，降质过程为：</p><script type="math/tex; mode=display">\mathbf{y}=\mathbf{\Phi}\left(\mathbf{x} ; \theta_{\eta}\right)</script><p>where $\Phi$是降质函数，$\theta_{\eta}$是降质参数。但是实际情况下，降质函数不可知、降质参数也不可知。SR的工作是从ground truth中恢复$\hat{\mathbf{X}}$，使用公式为：<script type="math/tex">\hat{\mathbf{x}}=\Phi^{-1}\left(\mathbf{y}, \theta_{\zeta}\right)</script><br>where $\theta_{\zeta}$是$\Phi^{-1}$的参数。</p><p>大多数的文献都采用下式的方法。<script type="math/tex">\mathbf{y}=(\mathbf{x} \otimes \mathbf{k}) \downarrow_{s}+\mathbf{n}</script></p><h1 id="网络结构及其思想"><a href="#网络结构及其思想" class="headerlink" title="网络结构及其思想"></a>网络结构及其思想</h1><h2 id="linear-model"><a href="#linear-model" class="headerlink" title="linear model"></a>linear model</h2><p>作者定义线性模型的概念是：<font color="#FF0000">single path for signal flow without skip connections or multiple-branches</font>。不同的线性模型区别主要在于upsampling的时期不同。</p><h3 id="SRCNN"><a href="#SRCNN" class="headerlink" title="SRCNN"></a>SRCNN</h3><ul><li>董超何凯明的开山之作，汤晓鸥的灌水乐园</li><li>三层conv，根据功能不同起了三个不同的名字</li><li>没什么好说的，开山之作，一定要看的</li></ul><h3 id="VDSR"><a href="#VDSR" class="headerlink" title="VDSR"></a>VDSR</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是我在arxiv上偶然看见的，发布于2019年4月中旬，可以和华南理工的综述论文结合起来看。《A Deep Journey into Super-resolution: A Survey》一文是一篇很好的综述，总结了30+种基于深度学习的超分辨率网络，并将之分类为&lt;font color=&quot;#FF0000&quot;&gt; linear, residual, multi-branch, recursive, progressive, attention-based and adversarial等九类&lt;/font&gt;，并详细比较了&lt;font color=&quot;#FF0000&quot;&gt;网络复杂性，内存占用，模型输入和输出，学习细节，网络损耗类型和重要架构差异（例如，深度，跳过连接，过滤器）&lt;/font&gt;。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="SR" scheme="https://zeyuxiao1997.github.io/tags/SR/"/>
    
      <category term="survey" scheme="https://zeyuxiao1997.github.io/tags/survey/"/>
    
  </entry>
  
  <entry>
    <title>learn-docker</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/18/learn-docker/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/18/learn-docker/</id>
    <published>2019-04-18T00:46:15.000Z</published>
    <updated>2019-04-22T07:35:23.733Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Technical guide" scheme="https://zeyuxiao1997.github.io/categories/Technical-guide/"/>
    
    
      <category term="docker" scheme="https://zeyuxiao1997.github.io/tags/docker/"/>
    
      <category term="dockerfile" scheme="https://zeyuxiao1997.github.io/tags/dockerfile/"/>
    
  </entry>
  
  <entry>
    <title>paper——Real-Time Non-Rigid Multi-Frame Depth Video Super-Resolution(NGVSR)</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/17/paper-NGVSR/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/17/paper-NGVSR/</id>
    <published>2019-04-17T14:11:17.000Z</published>
    <updated>2019-04-21T13:42:55.458Z</updated>
    
    <content type="html"><![CDATA[<p>《paper-Real-Time Non-Rigid Multi-Frame Depth Video Super-Resolution》是CVPRW2015的作品。从作品的时间可以看出，很遗憾，论文是基于数学方法，也就是传统方法的。因此这样一篇论文应该将侧重点放在思路理解和思维迁移上，看看Kalman filter能不能迁移到end-to-end上，3D non-rigid能不能迁移到一般图像、视频，甚至是气候变换图像等。<br><a id="more"></a></p><p>论文可<a href="/download/Ismaeil_Real-Time_Non-Rigid_Multi-Frame_2015_CVPR_paper.pdf">点击下载</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《paper-Real-Time Non-Rigid Multi-Frame Depth Video Super-Resolution》是CVPRW2015的作品。从作品的时间可以看出，很遗憾，论文是基于数学方法，也就是传统方法的。因此这样一篇论文应该将侧重点放在思路理解和思维迁移上，看看Kalman filter能不能迁移到end-to-end上，3D non-rigid能不能迁移到一般图像、视频，甚至是气候变换图像等。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="VSR" scheme="https://zeyuxiao1997.github.io/tags/VSR/"/>
    
      <category term="non-rigid" scheme="https://zeyuxiao1997.github.io/tags/non-rigid/"/>
    
  </entry>
  
  <entry>
    <title>paper——Deep Back-Projection Networks For Super-Resolution(DBPN)</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/17/paper-DBPN/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/17/paper-DBPN/</id>
    <published>2019-04-17T07:13:52.000Z</published>
    <updated>2019-04-21T13:42:44.045Z</updated>
    
    <content type="html"><![CDATA[<p>《Deep Back-Projection Networks For Super-Resolution》是CVPR2018的作品。</p><a id="more"></a><p>论文可<a href="/download/1803.02735.pdf">点击下载</a>,代码可<a href="https://github.com/alterzero/DBPN-Pytorch" target="_blank" rel="noopener">点击获取</a>.</p><p>事实上，网上已经有两篇中文论文解析了，我在写paper reading notes的时候参考了它们。</p><p>(<a href="https://blog.csdn.net/shwan_ma/article/details/79611869" target="_blank" rel="noopener">https://blog.csdn.net/shwan_ma/article/details/79611869</a>)<br>(<a href="https://www.cnblogs.com/king-lps/p/9128072.html" target="_blank" rel="noopener">https://www.cnblogs.com/king-lps/p/9128072.html</a>)</p><h2 id="一、Introduction"><a href="#一、Introduction" class="headerlink" title="一、Introduction"></a>一、Introduction</h2><h3 id="1-motivation"><a href="#1-motivation" class="headerlink" title="1.motivation"></a>1.motivation</h3><p>deep super resolution networks通常是先学习一个特征，在通过network强大的non-linear mapping 将LR space 映射到 HR space。而这种操作可能对LR和HR图像之间的mutual dependencies挖掘的不是那么的有效。构建了一个不断相互上采样及下采样阶段的网络进行误差反馈，有种类似于”迭代反投影”算法的味道，从而将效果达到了state-of-the-art.</p><h3 id="2-现有的几个方法"><a href="#2-现有的几个方法" class="headerlink" title="2. 现有的几个方法"></a>2. 现有的几个方法</h3><p>直观的对比图如下。<br><img src="/2019/04/17/paper-DBPN/compare.png" title="compare"></p><h4 id="Predefined-upsampling"><a href="#Predefined-upsampling" class="headerlink" title="Predefined upsampling"></a>Predefined upsampling</h4><ul><li>这种算法需要进行预插值来使得输入及输出的size统一</li><li>Dong认为将输入输出的feature maps的size设置成一致，将有利于进行非线性映射。否则stride需要设置为分数，这将为带来很多不便之处</li><li>计算量的增大</li></ul><h4 id="Single-upsampling"><a href="#Single-upsampling" class="headerlink" title="Single upsampling"></a>Single upsampling</h4><ul><li>直接对LR Image进行处理，减少计算量</li><li>deconvlution layer： 代表FSRCNN, Dong et,al </li><li>sub-pixel convolution layer: 代表：ESPCN， twitter</li></ul><h4 id="Progressive-upsampling"><a href="#Progressive-upsampling" class="headerlink" title="Progressive upsampling"></a>Progressive upsampling</h4><ul><li>在每个阶段progress进行上采样，这种网络结构应对与upscaling factor为x4，x8时效果好</li></ul><h3 id="3-文中claim的contributions"><a href="#3-文中claim的contributions" class="headerlink" title="3.文中claim的contributions"></a>3.文中claim的contributions</h3><ul><li>Error feedback：分别计算up- and down-projection的误差，以获得更好的结果</li><li>Mutually connected up- and down-sampling stages：不断的上采样和下采样使得特征获取的更好</li><li>Deep concatenation：使用DesNet进行改进，提出的D-DBPN更好的融合特征进行重建</li></ul><h2 id="二、Method"><a href="#二、Method" class="headerlink" title="二、Method"></a>二、Method</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《Deep Back-Projection Networks For Super-Resolution》是CVPR2018的作品。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="SR" scheme="https://zeyuxiao1997.github.io/tags/SR/"/>
    
      <category term="RNN" scheme="https://zeyuxiao1997.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>paper——Recurrent Back-Projection Network for Video Super-Resolution(RBPN)</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/17/paper-RBPN/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/17/paper-RBPN/</id>
    <published>2019-04-17T06:03:07.000Z</published>
    <updated>2019-07-30T10:07:27.757Z</updated>
    
    <content type="html"><![CDATA[<p>《Recurrent Back-Projection Network for Video Super-Resolution》是CVPR2019的作品，主要思想是：结合RNN和back-projection和encoder/decoder，对视频的每一帧都进行融合重建（事实上每一帧都进行了一个motion compensation，每一帧对最后需要重建的目标帧都有一定的贡献）。通过上图的网络结构，较好的捕捉帧间subtle motion和significant motion，不光适用于传统的VSR重建，更可以对较大运动、较多运动的数据集进行重建。<br><a id="more"></a><br>论文可<a href="/download/1903.10128v1.pdf">点击下载</a>,代码可<a href="https://github.com/alterzero/RBPN-PyTorch" target="_blank" rel="noopener">点击获取</a>.</p><h2 id="一、Introduction"><a href="#一、Introduction" class="headerlink" title="一、Introduction"></a>一、Introduction</h2><h3 id="1-motivation"><a href="#1-motivation" class="headerlink" title="1. motivation"></a>1. motivation</h3><p>其他用于SR的方法，包括SISR、MISR、VSR，都不同程度的存在问题，总结下来就是：</p><blockquote><p>stacking or wraping frames and pooling together导致很多细节不能精确重建<br>alignment的方法不能很好的捕捉inter-frame motion<br>为了更好的重建、通过估计帧间运动而不是对齐帧，迭代地对image 进行super resolve.</p></blockquote><h3 id="2-现有的几个方法"><a href="#2-现有的几个方法" class="headerlink" title="2. 现有的几个方法"></a>2. 现有的几个方法</h3><p>方法可以归结到三类，SISR、MISR、VSR。</p><h4 id="SISR"><a href="#SISR" class="headerlink" title="SISR"></a>SISR</h4><ul><li>只对某一帧进行重建，比如SRCNN，这样做浪费了其他帧可以提供的互补信息；<h4 id="MISR"><a href="#MISR" class="headerlink" title="MISR"></a>MISR</h4></li><li>结合其他帧的互补信息；</li><li>为了互补的提取丢失信息，帧之间需要隐式或显式的帧对齐，但是很难考虑到时间平滑（temporal smoothness）；<h4 id="VSR"><a href="#VSR" class="headerlink" title="VSR"></a>VSR</h4></li><li>考虑到了temporal smoothness；</li><li>有两种方法：frame concatenation和使用RNN。前者同时处理多帧造成训练困难；后者很难对subtle or significant changes进行描述（即使使用LSTM）</li></ul><h3 id="3-文中claim的contributions"><a href="#3-文中claim的contributions" class="headerlink" title="3. 文中claim的contributions"></a>3. 文中claim的contributions</h3><ul><li>Integrating SISR and MISR in a unified VSR framework：论文中详细解释了RBPN融合SISR和MISR，并融合encoder、decoder、back-projection等部件</li><li>Back-projection modules：帧之间的巨大时间差异用back-projection模块进行连接</li><li>Extended evaluation protocol：没有使用主流testing set</li></ul><h3 id="4-现有方法的弊端"><a href="#4-现有方法的弊端" class="headerlink" title="4.现有方法的弊端"></a>4.现有方法的弊端</h3><!-- ![compare](paper-RBPN/compare.png) --><img src="/2019/04/17/paper-RBPN/compare.png" title="compare"><p>上图是几种方法的示意图。<br>这篇论文讲了不少VSR的方法并且很详细，对他们的优势和劣势分析的非常到位，后面会好好研读并且做好笔记。</p><h2 id="二、Method"><a href="#二、Method" class="headerlink" title="二、Method"></a>二、Method</h2><h3 id="1-总体架构"><a href="#1-总体架构" class="headerlink" title="1.总体架构"></a>1.总体架构</h3><p>RBPN被分成三部分：initial feature extraction, multiple projections and reconstruction. 网络端到端。</p><h4 id="initial-feature-extraction"><a href="#initial-feature-extraction" class="headerlink" title="initial feature extraction"></a>initial feature extraction</h4><p>这个模块是特征表示模块。总的来说，输入由三部分组成，目标帧、目标帧之前的某一帧和上述两帧之间的motion flow tensor组成。正是由于加入motion flow tensor，使得每一帧都与目标帧发生关联，使其或多或少都对目标帧重建做出贡献.</p><h4 id="multiple-projections"><a href="#multiple-projections" class="headerlink" title="multiple projections"></a>multiple projections</h4><p>这个模块是核心，结合SISR和MISR，输入$L_{t-k-1}$和$M_{t-k}$，输出$H_{t-k}$，下图就是总的IPO。<br><img src="/2019/04/17/paper-RBPN/ED1.png" title="The proposed projection module"></p><h4 id="reconstrction"><a href="#reconstrction" class="headerlink" title="reconstrction"></a>reconstrction</h4><p>重建的所有帧是：所有在之前输出的HR帧，最后一起进行下式的操作：$\mathrm{SR}_{t}=f_{r e c}\left(\left[H_{t-1}, H_{t-2}, \dots, H_{t-n}\right]\right)$，在论文中$f_{r e c}$是单卷积层。</p><h3 id="核心模块解析"><a href="#核心模块解析" class="headerlink" title="核心模块解析"></a>核心模块解析</h3><h4 id="Multiple-Projection"><a href="#Multiple-Projection" class="headerlink" title="Multiple Projection"></a>Multiple Projection</h4><p>multiple projection stage of RBPN uses a recurrent chain of encoder-decoder modules。总的IPO可以见上图。输入由两个，输出有两个。</p><p>将大图拆分成小图后的视觉效果如下图所示<br><img src="/2019/04/17/paper-RBPN/1.png" width="1"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《Recurrent Back-Projection Network for Video Super-Resolution》是CVPR2019的作品，主要思想是：结合RNN和back-projection和encoder/decoder，对视频的每一帧都进行融合重建（事实上每一帧都进行了一个motion compensation，每一帧对最后需要重建的目标帧都有一定的贡献）。通过上图的网络结构，较好的捕捉帧间subtle motion和significant motion，不光适用于传统的VSR重建，更可以对较大运动、较多运动的数据集进行重建。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="RNN" scheme="https://zeyuxiao1997.github.io/tags/RNN/"/>
    
      <category term="VSR" scheme="https://zeyuxiao1997.github.io/tags/VSR/"/>
    
  </entry>
  
  <entry>
    <title>How to Write an Essay</title>
    <link href="https://zeyuxiao1997.github.io/2019/04/17/how_to_write_an_essay/"/>
    <id>https://zeyuxiao1997.github.io/2019/04/17/how_to_write_an_essay/</id>
    <published>2019-04-17T04:34:10.000Z</published>
    <updated>2019-04-17T05:26:32.869Z</updated>
    
    <content type="html"><![CDATA[<p>摘自清华大学刘洋在第十届全国机器翻译研讨会上的报告：《机器翻译学术论文写作方法和技巧》，非常受用！<br><a id="more"></a><br>很多内容我已经删减了，留下的部分是最值得学习的部分，pdf版本可以从下面的链接获取并下载。<br><a href="/download/how_to_write_an_essay.pdf">点击下载</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘自清华大学刘洋在第十届全国机器翻译研讨会上的报告：《机器翻译学术论文写作方法和技巧》，非常受用！&lt;br&gt;
    
    </summary>
    
      <category term="learning notes" scheme="https://zeyuxiao1997.github.io/categories/learning-notes/"/>
    
    
      <category term="write essay" scheme="https://zeyuxiao1997.github.io/tags/write-essay/"/>
    
  </entry>
  
</feed>
