<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZeyuXiao @ USTC</title>
  
  <subtitle>Paper reading notes, code sharing platforms</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zeyuxiao1997.github.io/"/>
  <updated>2020-01-06T04:12:23.419Z</updated>
  <id>https://zeyuxiao1997.github.io/</id>
  
  <author>
    <name>Zeyuxiao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【论文阅读】Manipulating Attributes of Natural Scenes via Hallucination</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/06/attribute-hallucination/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/06/attribute-hallucination/</id>
    <published>2020-01-06T04:11:28.000Z</published>
    <updated>2020-01-06T04:12:23.419Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="GAN" scheme="https://zeyuxiao1997.github.io/tags/GAN/"/>
    
      <category term="manipulation" scheme="https://zeyuxiao1997.github.io/tags/manipulation/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】EnlightenGAN：Deep Light Enhancement without Paired Supervision(Arxiv201906)</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/06/VDMV/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/06/VDMV/</id>
    <published>2020-01-06T03:02:24.000Z</published>
    <updated>2020-01-06T03:53:06.671Z</updated>
    
    <content type="html"><![CDATA[<p>arxiv上一篇关于unpair数据低光照增强的论文，看格式应该是投稿CVPR。</p><p>文章使用非成对图像做无监督的图像增强，实现低照域图像转换，不仅解决成对数据收集难的问题，还实现了不同场景低光照图像对应参考图像亮度不一致的问题。结构图如下所示:<br><img src="/2020/01/06/VDMV/1.png" width="1"><br>如上所示，网络整体是一个UNet的结构。对于一张图，一般希望增强其亮度暗的区域，所以这里将输入图提取亮度分量I，然后1-I并将其resize到对应的feature大小，和UNet转换中各部分的feature相乘，如上图所示。上采样使用的是resize加卷积的方式去除棋盘效应。Loss部分，包括，一个全局的GAN loss，判别器输入是生成图和目标域的整张图像，一个局部的GAN loss，输出是五个上述两者的patch，两个部分分别做全局和局部的约束，这里的GAN使用了LSGAN。此外，论文还使用了特征保留loss，即生成图和输入图内容要有一致性，这里使用的是VGG特征，同样也分为全局和局部两个部分。</p><p>增强常使用attention map、UNet等方法。这个方法是否可以用于video enhancement呢？</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;arxiv上一篇关于unpair数据低光照增强的论文，看格式应该是投稿CVPR。&lt;/p&gt;
&lt;p&gt;文章使用非成对图像做无监督的图像增强，实现低照域图像转换，不仅解决成对数据收集难的问题，还实现了不同场景低光照图像对应参考图像亮度不一致的问题。结构图如下所示:&lt;br&gt;&lt;img 
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="enhancement" scheme="https://zeyuxiao1997.github.io/tags/enhancement/"/>
    
      <category term="GAN" scheme="https://zeyuxiao1997.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Face Video Deblurring using 3D Facial Priors(ICCV2019)</title>
    <link href="https://zeyuxiao1997.github.io/2019/11/16/FVD3DFP/"/>
    <id>https://zeyuxiao1997.github.io/2019/11/16/FVD3DFP/</id>
    <published>2019-11-16T06:15:23.000Z</published>
    <updated>2019-11-20T02:46:49.330Z</updated>
    
    <content type="html"><![CDATA[<p>ICCV2019的作品。作者将人脸的3D信息加入到人脸deblur的网络中，相当于多给了一个表征层面的监督，获得了不错的效果。</p><p>文章中直接用了xin tong的人脸重建算法，把其中的identity信息揉到UNet中。网络结构如下：<br><img src="/2019/11/16/FVD3DFP/1.png" width="1"></p><p>具体没有什么好多说的，总之能揉的信息往里面揉就是了。个人感觉文章有些水吧。</p><p>【思考】</p><ul><li><p>人脸3D层面的identity information是否可以加到其他low-level的重建网络中？</p></li><li><p>更广意义上的3D信息（body gesture等）是否也可以加入到SR、deblur中呢？</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ICCV2019的作品。作者将人脸的3D信息加入到人脸deblur的网络中，相当于多给了一个表征层面的监督，获得了不错的效果。&lt;/p&gt;
&lt;p&gt;文章中直接用了xin tong的人脸重建算法，把其中的identity信息揉到UNet中。网络结构如下：&lt;br&gt;&lt;img src=&quot;
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="video" scheme="https://zeyuxiao1997.github.io/tags/video/"/>
    
      <category term="deblurring" scheme="https://zeyuxiao1997.github.io/tags/deblurring/"/>
    
      <category term="face" scheme="https://zeyuxiao1997.github.io/tags/face/"/>
    
  </entry>
  
  <entry>
    <title>【课程笔记】GPU编程(cuda)</title>
    <link href="https://zeyuxiao1997.github.io/2019/11/16/course-cuda/"/>
    <id>https://zeyuxiao1997.github.io/2019/11/16/course-cuda/</id>
    <published>2019-11-16T06:15:23.000Z</published>
    <updated>2019-11-26T02:43:31.381Z</updated>
    
    <content type="html"><![CDATA[<p>中科大《GPU并行计算和CUDA程序开发及优化》课程笔记，授课教师：谭立湘。<br><a id="more"></a></p><h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><p>GPU计算强控制弱，更多的资源用于数据计算；CPU强控制弱计算，更多的资源用于缓存。在使用GPU计算前，CPU必须先将数据传到GPU显存中；在GPU计算完成后，GPU再将结果数据返回给主机内存。CPU和GPU的通信尤其重要！！</p><h1 id="并行计算基础"><a href="#并行计算基础" class="headerlink" title="并行计算基础"></a>并行计算基础</h1><h2 id="并行计算"><a href="#并行计算" class="headerlink" title="并行计算"></a>并行计算</h2><ul><li><p>时间上并行（流水线）+空间上并行（矩阵分块） </p></li><li><p>并行计算的三个基本条件</p><ul><li>并行机（包含多个处理器核心+通过特定硬件相互连接、相互通信）</li><li>问题具有并行度，可以分解为多个可以并行执行的子任务</li><li>并行编程</li></ul></li></ul><h2 id="PCAM设计方法"><a href="#PCAM设计方法" class="headerlink" title="PCAM设计方法"></a>PCAM设计方法</h2><ul><li><p>设计并行算法的四个阶段：</p><ul><li>划分(Partitioning)；分解成小的任务，开拓并发性<ul><li>又分为域分解和功能分解</li><li>域分解的对象是数据，如果一个任务需要别的任务中的数据， 则会产生任务间的通讯</li><li>功能分解划分的对象是计算，将计算划分为不同的任务；划分后研究不同任务所需的数据，数据应当不相交</li></ul></li><li>通讯(Communication)：确定诸任务间的数据交换，检测划分的合理性<ul><li>划分产生的多个任务不能完全独立执行，需要在任务间进行数据交流； 功能分解确定了任务之间的数据流</li><li>分为局部/全局通讯、结构化/非结构化通讯、静态/动态通讯、同步/异步通讯</li></ul></li><li>组合(Agglomeration)：依据任务的局部性，组合成更大的任务<ul><li>组合是由抽象到具体的过程，是将组合的任务能在一类并行机上有效的执行</li><li>合并小尺寸任务，减少任务数。如果任务数恰好等于处理器数，则也完成了映射过程</li><li>通过增加任务的粒度和重复计算，可以减少通讯成本</li></ul></li><li>映射(Mapping)：将每个任务分配到处理器上，提高算法的性能<ul><li>每个任务要映射到具体的处理器，定位到运行机器上</li><li>任务数大于处理器数时，存在负载平衡和任务调度问题</li><li>映射的目标：减少算法的执行时间<img src="/2019/11/16/course-cuda/1.png" width="1"></li></ul></li></ul></li><li><p>原则<br>先尽量开发算法的并发性和扩展性，其次考虑通信成本和局部性，再次利用局部性相互组合减少通信成本，最后将组合后的任务分配到各个处理器</p></li><li><p>并行算法复杂性度量</p><ul><li>指标<ul><li>运行时间t(n): 包含计算时间和通讯时间，分别用计算时间步和选路时间步作单位。n为问题实例的输入规模。 </li><li>处理器数p(n) </li><li>并行算法成本c(n): c(n)=t(n)p(n) </li><li>总运算量W(n): 并行算法求解问题时所完成的总的操作步数</li></ul></li><li>设计并行算法时应尽可能地将每个时间步的工作量均匀地分摊给p台处理器，使各处理器处于活跃状态—-<font color="red"> 负载均衡 </font></li><li>三层并行计算模型  <img src="/2019/11/16/course-cuda/2.png" width="2">  <img src="/2019/11/16/course-cuda/3.png" width="3"></li></ul></li><li><p>并行算法设计<br>  。。。不写了，和本科学的操作系统有关</p></li><li><p>并行层次和代码粒度</p>  <img src="/2019/11/16/course-cuda/4.png" width="4"></li></ul><h1 id="OpenMP并行编程"><a href="#OpenMP并行编程" class="headerlink" title="OpenMP并行编程"></a>OpenMP并行编程</h1><ul><li><p>OpenMP并行编程模型</p><p>  程序开始时只有一个主线程，程序中的串行部分都是由主线程执行；并行的部分是通过派生其它线程来执行。但是如果并行部分没有结束时是不会执行串行部分的。</p>  <img src="/2019/11/16/course-cuda/5.png" width="5"></li></ul><h1 id="GPU-硬件架构"><a href="#GPU-硬件架构" class="headerlink" title="GPU 硬件架构"></a>GPU 硬件架构</h1><ul><li><p>GPU体系结构相关术语</p><ul><li>SP（Streaming Processor）:流处理器是GPU运算的最基本计算单元。</li><li>SFU（Special Function Unit）:特殊函数单元  用来执行超越函数指令，比如正弦、余弦、平方根等函数。</li><li>Shader core（渲染核/着色器），SP的另一个名称，又称为CUDA core，始于Fermi架构</li><li>DP （双精度浮点运算单元）</li><li>SM（Streaming Multiprocessors）:流式多处理器是GPU架构中的基本计算单元，也是GPU性能的源泉，由SP、DP、SFU等运算单元组成。这是一个典型的阵列机，其执行方式为SIMT（单指令多线程），区别于传统的   SIMD（单指令流多数据流），能够保证多线程的同时执行<br>SMX: Kepler架构中的SM</li><li>SMM: Maxwell架构中的SM</li><li>TPC（Thread Processing Cluster）线程处理器簇：由SM和L1 Cache组成，存在于Tesla架构中。</li><li>TPC（Texture Processing Cluster）纹理处理器簇：出现在Pascal架构中。</li><li>GPC（Graph Processing Cluster）图形处理器簇：类似于TPC，是介于整个GPU和SM间的硬件单元，始于Fermi构架。</li><li>SPA（Scalable streaming Processor Array）可扩展的流处理器阵列：所有处理核心和高速缓存的总和，包含所有的SM、TPC、GPC。与存储器系统共同组成GPU构架。</li><li>MMC（MeMory Controller）存储控制器：控制存储访问的单元，合并访存。每个存储控制器可以支持一定位宽的数据合并访存。</li><li>ROP（raster operation processors）光栅操作单元</li><li>LD/ST（Load/Store Unit）存储单元</li></ul></li><li><p>GPU计算能力<br>  需要区分计算能力和运算性能两个概念：</p><ul><li>运算性能包括整数运算性能、单精度浮点运算性能和双精度浮点运算性能等，表示GPU处理算术运算的能力，也是衡量GPU好坏的关键指标之一</li><li>计算能力是指GPU架构或GPU支持的功能，而与GPU的浮点运算性能无关</li></ul></li></ul><h2 id="几种架构"><a href="#几种架构" class="headerlink" title="几种架构"></a>几种架构</h2><ul><li>几种典型架构<ul><li>Tesla架构的SM<br>由8个SP、2个SF和一个执行双精度运算的DP组成，同时还包含了寄存器、共享存储、常量存储等单元<img src="/2019/11/16/course-cuda/6.png" width="6"></li><li>Tesla架构的TPC<br>2~3个SM配合L1 Cache构成TPC，Tesla架构主要核心型号有G80和GT200.每个TPC均由一个SM控制器进行统一控制<img src="/2019/11/16/course-cuda/7.png" width="7"></li><li>Tesla架构的G80<img src="/2019/11/16/course-cuda/8.png" width="8"></li><li>Tesla架构的GT200<img src="/2019/11/16/course-cuda/9.png" width="9"></li></ul></li></ul><ul><li>后面讲的全是GPU体系结构，感觉挺枯燥的，也不想做老本行了，算了就不看了叭</li></ul><h1 id="GPU-软件体系及CUDA编程模型"><a href="#GPU-软件体系及CUDA编程模型" class="headerlink" title="GPU 软件体系及CUDA编程模型"></a>GPU 软件体系及CUDA编程模型</h1><h2 id="GPU软件体系"><a href="#GPU软件体系" class="headerlink" title="GPU软件体系"></a>GPU软件体系</h2><ul><li>编译器<ul><li><font color="red">NVCC(NVIDIA CUDA Compiler)</font></li></ul></li><li>编程模型<ul><li>CUDA (CUDA C、CUDA Python、CUDA Java、CUDA C++、CUDA.NET)</li><li>OpenCL</li><li>OpenACC</li></ul></li><li>数学库函数<ul><li>包括但不限于：线性代数库CUBLAS、快速傅里叶变换CUFFT、深度学习CUDNN、FFmpeg</li></ul></li><li>性能分析工具<ul><li>NVIDIA Visual Profiler</li></ul></li><li><p>程序调试工具</p></li><li><p>管理软件</p><ul><li>nvidia-smi</li></ul></li><li>代码实例及使用文档</li></ul><h2 id="CUDA软件"><a href="#CUDA软件" class="headerlink" title="CUDA软件"></a>CUDA软件</h2><p>CUDA(Compute Unified Device Architecture，统一计算设备架构)是由 NVIDIA 推出的通用并行计算架构，该架构使 GPU 能够解决复杂的计算问题。 它包含了 CUDA 指令集架构（ISA）以及 GPU 内部的并行计算引擎。CUDA 是一个全新的软硬件架构，可将 GPU 视为一个并行数据计算的设备，对所进行的计算进行分配和管理，无需将其映射到图形 API</p><p>CUDA的基本思想是支持大量的线程级并行，并在硬件中动态地调度和执行这些线程。<font color="blue">GPU与CPU协同工作，GPU只有在计算高密度数据并行任务时才发挥作用!!</font><br><img src="/2019/11/16/course-cuda/10.png" width="10"></p><ul><li><p>GPU软件体系<br>分为三层结构：CUDA函数库、CUDA运行时API、CUDA驱动API</p><img src="/2019/11/16/course-cuda/11.png" width="11"></li><li><p>宿主代码和设备代码<br>基于 CUDA 开发的程序代码在实际执行中分为两种，<font color="red">一种是运行在CPU上的宿主代码（Host Code），一种是运行在GPU上的设备代码（Device Code）。</font>不同类型的代码由于其运行的物理位置不同，能够访问到的资源不同，因此对应的运行期组件也分为公共组件、宿主组件和设备组件三个部分，基本上囊括了所有在 GPU 开发中所需要的功能和能够使用到的资源接口，开发人员可以通过运行期环境的编程接口实现各种类型的计算。CUDA 所提供的运行期环境是通过驱动来实现各种功能的。 </p></li><li><p>CUDA软件环境<br>CUDA最主要的包含两个方面： ISA指令集架构与硬件计算引擎；实际上是硬件和指令集。见下图中的绿色部分，CUDA 架构的组件组成是： </p><ul><li>NVIDIA GPU中的并行计算引擎； </li><li>对硬件初始化、配置的OS内核级支持； </li><li>用户模式的驱动，为开发者提供设备级的API； </li><li>用于并行计算 kernel函数 的PTX 指令集架构(ISA，Instruction set architecture) <img src="/2019/11/16/course-cuda/12.png" width="12"></li></ul></li><li><p>CUDA编程术语</p><ul><li><p>kernel函数<br><font color="blue">Kernel函数是指为GPU设备编译的一个函数。也就是一个编译好的、在GPU上并行运行的计算函数。Kernel在GPU上以多个线程的方式被执行 </font>。一个完整的CUDA程序是由<font color="red">一系列的设备端kernel函数并行部分和主机端的串行处理部分</font>共同组成的。这些处理步骤会按照程序中相应语句的顺序依次执行，满足顺序一致性。 </p></li><li><p>Host，宿主，CPU、系统的CPU<br>负责启动应用程序，运行程序的串行部分，将程序的并行、计算密集的部分offload到GPU上运行，并最终返回程序的运行结果。</p></li><li><p>Device，设备，GPU，CPU的协处理<br>负责程序的并行、计算密集部分的处理，并将处理结果返回给Host。</p></li><li><p>Host Memory：宿主内存<br>是指安装GPU产品的主机的主板板载内存</p></li><li><p>device Memory：设备内存<br>GPU设备的板载内存（显卡的显存），是高性能的 GDDR5 内存，支持DMA访问</p></li><li><p>Block：线程块<br>执行Kernel的一组线程组成一个线程块。一个线程块<font color="red">最多可包含1024个</font><font color="green">并行执行</font>的线程，线程之间通过共享内存有效地共享数据，并实现线程的通信和栅栏同步。</p></li><li><p>线程ID：线程在线程块中的线程号（唯一标识）<br>基于线程ID的复杂寻址，应用程序可以将线程块指定为任意大小的二维或三维数组，并使用2个或3个索引来标识每个线程。</p><ul><li>对于大小是（Dx，Dy）的二维线程块，索引为（x，y）的线程的线程ID为（x+y*Dx）</li><li>对于大小为（Dx，Dy，Dz）的三维线程块，索引为（x，y，z）的线程的线程ID为：（x+y<em>Dx+z</em>Dx*Dy）</li><li><font color="blue">类似于C语言二维数组、三维数组中通过元素下标计算元素位置的方法</font></li></ul></li><li><p>Grid：线程块组成的线程网格（最多2^32 个blocks）<br>执行相同Kernel、具有相同维数和大小的线程块可以组合到一个网格中。这样单个Kernel调用中启动的线程数就可以很大。同一网格中的不同线程块中的线程不能互相通信和同步。Grid 是一个线程块阵列，执行相同的内核，从全局内存读取输入数据，将计算结果写入全局内存。 </p></li><li><p>Block ID：线程块ID<br>线程块ID是线程块在Grid中的块号。实现基于块ID的复杂寻址，应用程序可以将Grid指定为任意大小的二维数组，并用2个索引来标识每个线程块。对于大小为（Dx，Dy）的二维线程块，索引为（x，y）的线程块的ID为（x+y*Dx）。现已支持三维</p></li><li><p>Wrap：线程束<br>一个线程块中连续的固定数量（32）的线程组。将线程块中的线程划分成wrap的方式是：每个wrap包含线程ID连续递增的32个线程，从线程0开始递增到线程31。 </p></li><li><p>Stream：CUDA的一个Stream表示一个按特定顺序执行的GPU操作序列。诸如kernel启动、内存拷贝、事件启动和停止等操作可以排序放置到一个Stream中。一个Stream包含了一系列Grids，并且可以多个Stream并行执行。</p><p>下图很形象的展示了CPU、GPU以及GPU内部grid、block、thread的层次关系</p><img src="/2019/11/16/course-cuda/13.png" width="13"></li></ul></li><li><p>Grid、block 和 thread 的关系<br>  在 CUDA 架构下，GPU芯片执行时的最小单位是thread。若干个thread可以组成一个线程块（block）。<font color="red">一个block中的thread能存取同一块共享内存，可以快速进行同步和通信操作</font>。 每一个 block 所能包含的 thread 数目是有限的。执行相同程序的 block，可以组成grid。不同 block 中的 thread 无法存取同一共享内存，因此无法直接通信或进行同步。<font color="red">不同的 grid可以执行不同的程序（kernel）</font>。</p><p>  Grid是由线程块组成的网格。每个线程都执行该kernel，应用程序指定了Grid和线程块的维数，Grid的布局可以是一维、二维或三维的。每个线程块有一个唯一的线程块ID，线程块中的每个线程具有唯一的线程ID。同一个线程块中的线程可以协同访问共享内存，实现线程之间的通信和同步。每个线程块最多可以包含的线程的个数为1024个，线程块中的线程以32个线程为一组的Wrap的方式进行分时调度。<font color="red">每个线程在数据的不同部分并行地执行相同的操作。</font> </p></li><li><p>CUDA处理流程<br>  一个程序分为两个部份：Host 端和 Device 端。Host 端是指在 CPU 上执行的部份，而 Device 端则是在GPU上执行的部份。Device端的程序又称为kernel函数。通常 Host 端程序会将数据准备好后，复制到GPU的内存中，再由GPU执行 Device 端程序，完成后再由 Host 端程序将结果从GPU的内存中取回。CPU 存取 GPU 内存时只能通过 PCI-E 接口，速度有限。</p><ul><li>从系统内存中复制数据到GPU内存 </li><li>CPU指令驱动GPU运行； </li><li>GPU 的每个CUDA核心并行处理 </li><li>GPU 将CUDA处理的最终结果返回到系统的内存<img src="/2019/11/16/course-cuda/14.png" width="14"></li></ul></li><li><p>CUDA程序执行的基本流程</p><ul><li>分配内存空间和显存空间 </li><li>初始化内存空间 </li><li>将要计算的数据从Host内存上复制到GPU内存上 </li><li>执行kernel计算 </li><li>将计算后GPU内存上的数据复制到Host内存上 </li><li>处理复制到Host内存上的数据 </li></ul></li><li><p>CUDA编程模型<br>完整的CUDA程序包括主机端和设备端两部分代码，主机端代码在CPU上执行。设备端代码（kernel函数）运行在GPU上。其中一个kernel函数对应一个grid，每个grid根据需要配置不同的block数量和thread数量。</p><ul><li>CPU作为主机端只能有一个+GPU作为设备端可以有多个+CPU主要负责逻辑处理+GPU负责密集型的并行计算</li><li>CUDA包含两个并行逻辑层：block层和thread层。在执行时，block映射到SM、thread映射到SP(Core)</li><li><font color="blue">如何在实际应用程序中高效地开发这两个层次的并行是CUDA编程与优化的关键之一</font></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;中科大《GPU并行计算和CUDA程序开发及优化》课程笔记，授课教师：谭立湘。&lt;br&gt;
    
    </summary>
    
      <category term="course note" scheme="https://zeyuxiao1997.github.io/categories/course-note/"/>
    
    
      <category term="GPU programming" scheme="https://zeyuxiao1997.github.io/tags/GPU-programming/"/>
    
      <category term="cuda" scheme="https://zeyuxiao1997.github.io/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Automatic Video Object Segmentation Based on Visual and Motion Saliency(TMM2019)</title>
    <link href="https://zeyuxiao1997.github.io/2019/11/15/VOSVMS/"/>
    <id>https://zeyuxiao1997.github.io/2019/11/15/VOSVMS/</id>
    <published>2019-11-15T02:12:38.000Z</published>
    <updated>2019-11-15T08:04:39.321Z</updated>
    
    <content type="html"><![CDATA[<p>TMM2019的作品。</p><h2 id="关于-视频-物体分割"><a href="#关于-视频-物体分割" class="headerlink" title="关于(视频)物体分割"></a>关于(视频)物体分割</h2><p>视频分割的挑战主要有两个，一是非刚性的前景变换，二是前景和背景的运动模糊且难以区分。越来越多的方法使用显著性进行prior的估计，以便更好的区分前景和后景【但是大多是使用image saliency + motion cues，并不能很好的为视频服务】</p><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><p>视觉显着性计算是在帧内找到显着对象，而运动线索计算能够在帧间定位对象。因此方法分为三个阶段，visual saliency computing、motion cues computing、segmentation<br><!-- ![pipeline](VOSVMS、1.png) --></p><p>本文均基于传统方法，没有用到深度学习。</p><p>文中比较有启发的一点是，由于现实中运动的复杂性，使用motion boundary and static boundary提取不同的外观，前者使用光流梯度进行优化，后者使用超像素的边缘进行表示。</p><h2 id="后记-显著性检测之流行排序"><a href="#后记-显著性检测之流行排序" class="headerlink" title="后记-显著性检测之流行排序"></a>后记-显著性检测之流行排序</h2><p><a href="http://papers.nips.cc/paper/2447-ranking-on-data-manifolds.pdf" target="_blank" rel="noopener">参考文献</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;TMM2019的作品。&lt;/p&gt;
&lt;h2 id=&quot;关于-视频-物体分割&quot;&gt;&lt;a href=&quot;#关于-视频-物体分割&quot; class=&quot;headerlink&quot; title=&quot;关于(视频)物体分割&quot;&gt;&lt;/a&gt;关于(视频)物体分割&lt;/h2&gt;&lt;p&gt;视频分割的挑战主要有两个，一是非刚性的
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="video" scheme="https://zeyuxiao1997.github.io/tags/video/"/>
    
      <category term="object segmentation" scheme="https://zeyuxiao1997.github.io/tags/object-segmentation/"/>
    
      <category term="saliency" scheme="https://zeyuxiao1997.github.io/tags/saliency/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记汇总</title>
    <link href="https://zeyuxiao1997.github.io/2019/11/15/index/"/>
    <id>https://zeyuxiao1997.github.io/2019/11/15/index/</id>
    <published>2019-11-15T01:52:15.000Z</published>
    <updated>2020-01-06T04:12:39.935Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SIGGRAPH"><a href="#SIGGRAPH" class="headerlink" title="SIGGRAPH"></a>SIGGRAPH</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>Manipulating Atributes of Natural Scenes via Hallucination</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="CVPR"><a href="#CVPR" class="headerlink" title="CVPR"></a>CVPR</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="ECCV"><a href="#ECCV" class="headerlink" title="ECCV"></a>ECCV</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="ICCV"><a href="#ICCV" class="headerlink" title="ICCV"></a>ICCV</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>Face Video Deblurring using 3D Facial Priors</td><td><a href="https://zeyuxiao1997.github.io/2019/11/16/FVD3DFP/">笔记</a></td><td>ICCV2019</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="NIPS"><a href="#NIPS" class="headerlink" title="NIPS"></a>NIPS</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="JOURNAL"><a href="#JOURNAL" class="headerlink" title="JOURNAL"></a>JOURNAL</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>A Video Deblurring Algorithm Based on Motion Vector and An Encorder-Decoder Network</td><td><a href="https://zeyuxiao1997.github.io/2019/11/16/FVD3DFP/">笔记</a></td><td>IEEE Access 2019</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="ARXIV"><a href="#ARXIV" class="headerlink" title="ARXIV"></a>ARXIV</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>EnlightenGAN：Deep Light Enhancement without Paired Supervision</td><td></td><td>arxiv201906</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SIGGRAPH&quot;&gt;&lt;a href=&quot;#SIGGRAPH&quot; class=&quot;headerlink&quot; title=&quot;SIGGRAPH&quot;&gt;&lt;/a&gt;SIGGRAPH&lt;/h1&gt;&lt;div class=&quot;table-container&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr
      
    
    </summary>
    
      <category term="quick search" scheme="https://zeyuxiao1997.github.io/categories/quick-search/"/>
    
    
      <category term="conference" scheme="https://zeyuxiao1997.github.io/tags/conference/"/>
    
  </entry>
  
</feed>
