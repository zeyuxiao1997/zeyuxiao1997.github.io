<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ZeyuXiao @ USTC</title>
  
  <subtitle>Paper reading notes, code sharing platforms</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zeyuxiao1997.github.io/"/>
  <updated>2020-02-05T12:00:16.912Z</updated>
  <id>https://zeyuxiao1997.github.io/</id>
  
  <author>
    <name>Zeyuxiao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【论文阅读】FC2N：Fully Channel-Concatenated Network for Single Image Super-Resolution</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/05/FC2N/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/05/FC2N/</id>
    <published>2020-02-05T09:15:06.000Z</published>
    <updated>2020-02-05T12:00:16.912Z</updated>
    
    <content type="html"><![CDATA[<p>arxiv上的preprint，关注于轻量化的超分辨率网络。论文使用weighted channel concatenation，避免直接堆residual。</p><p>论文使用的加权的通道连接方式和现有的residual-based的方式对比如下图所示，可以很显然的发现，最大的区别是将element-wise相加变成了不同分支间的channel concat操作。<br><img src="/2020/02/05/FC2N/2.png" width="2"></p><p>因此，由上面的操作组成的layers组成大的blocks，成为这个SR网络的basic block<br><img src="/2020/02/05/FC2N/1.png" width="1"></p><p><a href="https://blog.csdn.net/qq_29257201/article/details/97931770" target="_blank" rel="noopener">CSDN</a>上有一篇这个文章的解读，可以参看，这个文章怎么说呢，故事讲的不错，不知道有没有中CVPR2020。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;arxiv上的preprint，关注于轻量化的超分辨率网络。论文使用weighted channel concatenation，避免直接堆residual。&lt;/p&gt;
&lt;p&gt;论文使用的加权的通道连接方式和现有的residual-based的方式对比如下图所示，可以很显然的发
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="super resolution" scheme="https://zeyuxiao1997.github.io/tags/super-resolution/"/>
    
  </entry>
  
  <entry>
    <title>轻量级、快速的超分辨率网络(2篇)</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/05/FastSR/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/05/FastSR/</id>
    <published>2020-02-05T02:10:42.000Z</published>
    <updated>2020-02-05T09:37:43.900Z</updated>
    
    <content type="html"><![CDATA[<p>和慧政大佬交流当面交流过后很受启发，今天拜读两篇他做快速SR的两篇论文。</p><h1 id="Fast-and-Accurate-Single-Image-Super-Resolution-via-Information-Distillation-Network"><a href="#Fast-and-Accurate-Single-Image-Super-Resolution-via-Information-Distillation-Network" class="headerlink" title="Fast and Accurate Single Image Super-Resolution via Information Distillation Network"></a>Fast and Accurate Single Image Super-Resolution via Information Distillation Network</h1><p>CVPR2018论文。<br>网络整体的pipeline不是很难。如图<img src="/2020/02/05/FastSR/1.png" width="1"></p><p>网络的重点在于IDN模块，<img src="/2020/02/05/FastSR/2.png" width="2"></p><p>IDN模块的核心操作是，其在上面部分和下面部分之间引入一个slice操作，这个操作将特征给分成了原来部分的1-1/s送入下面的模块，以及1/s和上一个Block的信息concat在一起，然后再加到增强模块的最后。论文说这样做的目的是为了保留部分上面的模块的信息，并且重利用前面Block的信息。</p><p>然后就是压缩模块，就是1x1的卷积进行通道压缩，论文说这里是去除对于后面网络来说多余的信息。在具体训练时，这里D3,d和s分别设成64,16和4，而且对增强模块的第四层和第二层使用分组为4的分组卷积用于减少参数量。最后是实验结论，简要来说就是增强模块学习图像的轮廓信息，压缩模块继续减小像素值。</p><p>使用IDN的网络确实很快，测试时间如下：<br><img src="/2020/02/05/FastSR/3.png" width="3"></p><h1 id="Lightweight-Image-Super-Resolution-with-Information-Multi-distillation-Network"><a href="#Lightweight-Image-Super-Resolution-with-Information-Multi-distillation-Network" class="headerlink" title="Lightweight Image Super-Resolution with Information Multi-distillation Network"></a>Lightweight Image Super-Resolution with Information Multi-distillation Network</h1><p>这篇是上一篇的加强版，最大的亮点在于使用了multi-distillation模块和CCALayer。在我们的4K AI HDR的比赛中，均使用到了CCALayer，可以证明这个模块的设计非常成功。</p><p>这里使用了更多的distillation进行concat。<br><img src="/2020/02/05/FastSR/4.png" width="4"></p><p>CCALayer使用均值方差的计算达到更好的PSNR和SSIM值。<br><img src="/2020/02/05/FastSR/6.png" width="6"></p><p>网络整体的pipieline如下：<img src="/2020/02/05/FastSR/5.png" width="5"></p><p>两篇论文是很早的从轻量级网络设计角度切入SR的，占坑比较早。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;和慧政大佬交流当面交流过后很受启发，今天拜读两篇他做快速SR的两篇论文。&lt;/p&gt;
&lt;h1 id=&quot;Fast-and-Accurate-Single-Image-Super-Resolution-via-Information-Distillation-Network&quot;&gt;&lt;a
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="super resolution" scheme="https://zeyuxiao1997.github.io/tags/super-resolution/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】水下图像增强与重建综述(2篇)</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/04/UnderWaterSurvey/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/04/UnderWaterSurvey/</id>
    <published>2020-02-04T02:57:26.000Z</published>
    <updated>2020-02-04T06:48:27.989Z</updated>
    
    <content type="html"><![CDATA[<p>两篇来自IEEE Access2019的论文，水下图像增强与重建的综述。</p><h1 id="A-Survey-of-Restoration-and-Enhancement-for-Underwater-Images"><a href="#A-Survey-of-Restoration-and-Enhancement-for-Underwater-Images" class="headerlink" title="A Survey of Restoration and Enhancement for Underwater Images"></a>A Survey of Restoration and Enhancement for Underwater Images</h1><h2 id="水下图像和自然图像的区别"><a href="#水下图像和自然图像的区别" class="headerlink" title="水下图像和自然图像的区别"></a>水下图像和自然图像的区别</h2><p>直接分量和前向散射分量来自水下介质中物体的反射光，后向散射分量是由水下环境光和水下悬浮粒子之间的相互作用形成的。下图是相机和光线的成像原理<br><img src="/2020/02/04/UnderWaterSurvey/1.png" width="1"></p><p>波长较长，频率较低的红光优先被水吸收，其次是橙色，黄色，绿色和蓝色。 由于水的这种不均匀吸收，大多数捕获的水下图像将呈现青色。<br><img src="/2020/02/04/UnderWaterSurvey/2.png" width="2"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;两篇来自IEEE Access2019的论文，水下图像增强与重建的综述。&lt;/p&gt;
&lt;h1 id=&quot;A-Survey-of-Restoration-and-Enhancement-for-Underwater-Images&quot;&gt;&lt;a href=&quot;#A-Survey-of-Res
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="enhancement" scheme="https://zeyuxiao1997.github.io/tags/enhancement/"/>
    
      <category term="underwater" scheme="https://zeyuxiao1997.github.io/tags/underwater/"/>
    
      <category term="survey" scheme="https://zeyuxiao1997.github.io/tags/survey/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】 An Underwater Image Enhancement Benchmark Dataset and Beyond</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/03/UnderWaterEnhancer/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/03/UnderWaterEnhancer/</id>
    <published>2020-02-03T09:15:42.000Z</published>
    <updated>2020-02-04T02:56:57.713Z</updated>
    
    <content type="html"><![CDATA[<p>the red light first disappears because of its longest wavelength<br>在自然情况的水中，波的选择性衰减会导致水下图像发蓝或发绿，例如图1中的原始水下图像。</p><p>在制作数据集的时候，作者比较了现有的用于水下图像增强的，然后使用不同方法进行增强，然后人工去判断哪个效果好，效果最好的作为reference，增强之前的图片用作input。</p><p>作者又设计了WaterNet，用于baseline。<img src="/2020/02/03/UnderWaterEnhancer/1.png" width="1"><br>从作者提供的baseline来看，水下图像增强确实还有继续灌水的地方，如果能从水下成像或者是其他物理模型入手刻画降质模型，应该还是可以发发顶会的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;the red light first disappears because of its longest wavelength&lt;br&gt;在自然情况的水中，波的选择性衰减会导致水下图像发蓝或发绿，例如图1中的原始水下图像。&lt;/p&gt;
&lt;p&gt;在制作数据集的时候，作者比较了现有的用
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="enhancement" scheme="https://zeyuxiao1997.github.io/tags/enhancement/"/>
    
      <category term="underwater" scheme="https://zeyuxiao1997.github.io/tags/underwater/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Learning a Deep Single Image Contrast Enhancer from Multi-Exposure Images</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/03/SICE/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/03/SICE/</id>
    <published>2020-02-03T08:25:55.000Z</published>
    <updated>2020-02-03T09:11:47.547Z</updated>
    
    <content type="html"><![CDATA[<p>TIP2018单图对比度增强的论文。<br>论文构建了一个可用于单图对比度增强学习的数据集，并且设计了网络结构用于SICE。<a href="https://kevinj-huang.github.io/2019/05/28/%E5%8D%9A%E5%AE%A2133/" target="_blank" rel="noopener">博客</a>中有论文解读，可以参考。</p><p>one stage CNN in original intensity may have difficulties in balancing the enhancement of smooth and texture components of an image. </p><p>这里根据Retinex理论，图像的低频信息代表全局自然程度，而高频信息代表图像的局部细节，所以很自然的就是把低频信息和高频信息分开来<br><img src="/2020/02/03/SICE/1.png" width="1"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;TIP2018单图对比度增强的论文。&lt;br&gt;论文构建了一个可用于单图对比度增强学习的数据集，并且设计了网络结构用于SICE。&lt;a href=&quot;https://kevinj-huang.github.io/2019/05/28/%E5%8D%9A%E5%AE%A2133/&quot; 
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="enhancement" scheme="https://zeyuxiao1997.github.io/tags/enhancement/"/>
    
  </entry>
  
  <entry>
    <title>HDR(高动态范围图像)相关论文【硕博论文+国内期刊】</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/03/HDRRelated/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/03/HDRRelated/</id>
    <published>2020-02-03T06:47:16.000Z</published>
    <updated>2020-02-03T08:15:23.016Z</updated>
    
    <content type="html"><![CDATA[<p>这里主要是看了几篇硕博论文和国内论文后，摘录出来的一些关于HDR Generation或者是HDR Reconstruction上可能有用的KeyPoints。</p><h1 id="《高动态范围图像的色调映射算法研究》-2019中科院光电所"><a href="#《高动态范围图像的色调映射算法研究》-2019中科院光电所" class="headerlink" title="《高动态范围图像的色调映射算法研究》(2019中科院光电所)"></a>《高动态范围图像的色调映射算法研究》(2019中科院光电所)</h1><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><ul><li><p>HDR图像格式</p><ul><li>RGBE格式：该格式文件的扩展名为.hdr，目前广泛应用于摄影和图像光照渲染等领域。RGBE格式由R, G, B和E一共4个通道组成，每个通道是8位的数据类型，4个通道一共是32位。其中E通道是指数位通道，存储的是以2为底的指数。通过指数位通道，能够使用整数来表示浮点型小数。这种存储格式既提高了数据精度又节省了存储空间。 </li><li>TIFF格式：该格式文件的扩展名为.tiff，现广泛应用于摄影、图像艺术等领域。TIFF格式每个通道都是采用32位的浮点数(32 bit Float Point, FP32)存储的，3个通道一共是96位。可以直接将HDR图像的FP32保存到TIFF格式的文件中。</li><li>OpenEXR格式：常用格式为FP16 (16bitFloatPoint)，每个通道采用16位浮点数进行存储，4个通道一共是64位。在每个通道中，1位是指数标志位，5位用来存放指数的值，其他的10位用来存放色度坐标的尾数。该格式图像具有良好的移植性和扩展性。</li></ul></li><li><p>trade off：高动态范围图像的映射一方面必须通过压缩将大的动态范围映射到显示设备的动态范围，这势必导致细节信息的丢失;另一方面为保持高亮与低暗的细节信息则需进行相应的增强，而这将导致动态范围的压缩与细节增强间的矛盾，容易造成光晕现象和梯度反转等视觉瑕疵。</p></li></ul><h2 id="色调映射算法"><a href="#色调映射算法" class="headerlink" title="色调映射算法"></a>色调映射算法</h2><ul><li><p>全局：全局色调映射算法的优点是应用起来简单高效，有的算子能够满足实时应用。缺点是不考虑像素空域信息，使得映射后的图像在对比度和细节信息上有一些损失。现有的一些色调映射算法是全局色调映射算法和局部色调映射算法的结合，往往能够取得不错的映射结果。   【好像里面包含3DLUT方法？？就是港理工他们的方法？】</p></li><li><p>局部：局部色调映射算法考虑到了像素的邻域信息，能够更好的保留图像的局部对比度，映射后的图像层次感强，细节信息丰富。局部色调映射算法的缺点是参数比较多，实现起来比较复杂，适应性受到限制。局部色调映射算法对一些图像容易产生梯度反转、光晕现象等问题。</p></li></ul><h2 id="边缘保持滤波器在色调映射中的应用"><a href="#边缘保持滤波器在色调映射中的应用" class="headerlink" title="边缘保持滤波器在色调映射中的应用"></a>边缘保持滤波器在色调映射中的应用</h2><p>边缘保持滤波器在色调映射算法中用于图像的尺度分解，图像经边缘保持平滑滤波器滤波后的图像为基本层，原图像和基本层之差为细节层。基于边缘保持滤波器的色调映射算法通常是将图像转化为亮度分量，然后进行滤波分层，对基本层和细节层分别进行处理后再进行合成，最后进行颜色恢复，就得到了色调映射后的图像。</p><ul><li>光晕现象：基于边缘保持滤波器的色调映射算法这种对图像进行分层处理然后合成的算法容易产生光晕现象。例如用高斯滤波器对图像进行分层处理用于色调映射时，就会在图像的高对比度边缘产生光晕现象。图像的光晕现象是指图像的暗区域或高亮区域的交界处，产生颜色反差的一种现象，在色调映射中比较常见。光晕现象用红色椭圆框出，两棵比较暗的树木旁边又有比较明亮的阳光，这样高亮度像素影响到了低亮度像素的处理，就会两者的交界处产生明亮的光晕。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这里主要是看了几篇硕博论文和国内论文后，摘录出来的一些关于HDR Generation或者是HDR Reconstruction上可能有用的KeyPoints。&lt;/p&gt;
&lt;h1 id=&quot;《高动态范围图像的色调映射算法研究》-2019中科院光电所&quot;&gt;&lt;a href=&quot;#《高动
      
    
    </summary>
    
      <category term="Summary" scheme="https://zeyuxiao1997.github.io/categories/Summary/"/>
    
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="HDR" scheme="https://zeyuxiao1997.github.io/tags/HDR/"/>
    
  </entry>
  
  <entry>
    <title>EEMEFN：Low-Light Image Enhancement via Edge-Enhanced Multi-Exposure Fusion Network</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/03/EEMEFN/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/03/EEMEFN/</id>
    <published>2020-02-03T02:44:22.000Z</published>
    <updated>2020-02-03T03:37:53.679Z</updated>
    
    <content type="html"><![CDATA[<p>AAAI2020论文，低光照增强论文。</p><p><a href="https://blog.csdn.net/Najlepszy/article/details/104051098" target="_blank" rel="noopener">CSDN</a>和<a href="https://zhuanlan.zhihu.com/p/103988862" target="_blank" rel="noopener">知乎</a>上都有对这篇论文的解读，可以参考。</p><p>论文使用多曝光融合先产生first-stage的结果，然后进行edge information refinement进行refine。提出的EEMEFM旨在recover well-exposed image details, reduce noise and color bias, and maintain sharp edges for extremely low-light image enhancement.</p><p>网络中使用了一个fusion模块，还是很不错的，具有参考意义<br><img src="/2020/02/03/EEMEFN/1.png" width="1"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;AAAI2020论文，低光照增强论文。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/Najlepszy/article/details/104051098&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;和&lt;a
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="enhancement" scheme="https://zeyuxiao1997.github.io/tags/enhancement/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Real-Time Semantic Segmentation via Multiply Spatial Fusion Network</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/02/MSFNet/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/02/MSFNet/</id>
    <published>2020-02-02T11:51:58.000Z</published>
    <updated>2020-02-02T12:26:34.219Z</updated>
    
    <content type="html"><![CDATA[<p>arxiv201911的preprint。</p><p>详见链接吧。<a href="https://mp.weixin.qq.com/s/v_TTwTWx0lu2rJmxvzQQ4g" target="_blank" rel="noopener">解析</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;arxiv201911的preprint。&lt;/p&gt;
&lt;p&gt;详见链接吧。&lt;a href=&quot;https://mp.weixin.qq.com/s/v_TTwTWx0lu2rJmxvzQQ4g&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;解析&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="high-level" scheme="https://zeyuxiao1997.github.io/tags/high-level/"/>
    
      <category term="semantic segmentation" scheme="https://zeyuxiao1997.github.io/tags/semantic-segmentation/"/>
    
      <category term="segmentation" scheme="https://zeyuxiao1997.github.io/tags/segmentation/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Gradient Information Guided Deraining with A Novel Network and Adversarial Training</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/02/GRASPPGAN/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/02/GRASPPGAN/</id>
    <published>2020-02-02T06:38:12.000Z</published>
    <updated>2020-02-02T08:52:06.559Z</updated>
    
    <content type="html"><![CDATA[<p>arxiv201910的一篇preprint。</p><p>作者主要改进了现有ASPP(现有的ASPP在更大的感受野上不一定更有助于feature提取，反而会损害某些细节)；并且作者发现，雨水在梯度层面上更加明显，因此使用梯度信息进行指导更好。</p><p>网络如下：<img src="/2020/02/02/GRASPPGAN/1.png" width="1"></p><img src="/2020/02/02/GRASPPGAN/2.png" width="2"><script type="math/tex; mode=display">L_{g}=\frac{1}{n} \sum_{i=1}^{n}\left[\left\|\left(\nabla_{x}\left(d_{i}\right)-\nabla_{x}\left(g_{i}\right)\left\|_{2}+\right\|\left(\nabla_{y}\left(d_{i}\right)-\nabla_{y}\left(g_{i}\right) \|_{2}\right]\right.\right.\right.</script><img src="/2020/02/02/GRASPPGAN/3.png" width="3"><p>但是有一点需注意，derain的rain分布在高频区域，但是摩尔纹并不是均匀分布的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;arxiv201910的一篇preprint。&lt;/p&gt;
&lt;p&gt;作者主要改进了现有ASPP(现有的ASPP在更大的感受野上不一定更有助于feature提取，反而会损害某些细节)；并且作者发现，雨水在梯度层面上更加明显，因此使用梯度信息进行指导更好。&lt;/p&gt;
&lt;p&gt;网络如下：&lt;
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="derain" scheme="https://zeyuxiao1997.github.io/tags/derain/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】LAP-Net：Level-Aware Progressive Network for Image Dehazing</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/02/LAPNet/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/02/LAPNet/</id>
    <published>2020-02-02T02:10:03.000Z</published>
    <updated>2020-02-02T08:43:52.339Z</updated>
    
    <content type="html"><![CDATA[<p>ICCV2019论文。<br>作者认为现有方法不能很好解决真实场景中dehaze问题，并且不能将不同程度的haze区分对待，具有较大局限性</p><p>网络由4各部分组成。the progressive transmission subnet (t-net) + the discrete atmospheric light subet (A-net) +  the restoration layer and the adaptive integration subnet (I-net)<br><img src="/2020/02/02/LAPNet/1.png" width="1"></p><h1 id="The-t-net-for-transmission-map-estimation"><a href="#The-t-net-for-transmission-map-estimation" class="headerlink" title="The t-net for transmission map estimation"></a>The t-net for transmission map estimation</h1><script type="math/tex; mode=display">\hat{t}^{s}=\left\{\begin{array}{ll}{\mathcal{F}\left(\mathbf{I}, \theta^{s}\right)} & {s=1} \\{\mathcal{F}\left(\mathbf{I}, \theta^{s}, \hat{t}^{s-1}\right)} & {s>1}\end{array}\right</script><h1 id="The-A-net-for-atmospheric-light-estimation"><a href="#The-A-net-for-atmospheric-light-estimation" class="headerlink" title="The A-net for atmospheric light estimation"></a>The A-net for atmospheric light estimation</h1><p>这里将light estimation使用n=100的分类任务</p><h1 id="The-I-net-for-integrating-dehazed-images-from-multiple-stages"><a href="#The-I-net-for-integrating-dehazed-images-from-multiple-stages" class="headerlink" title="The I-net for integrating dehazed images from multiple stages"></a>The I-net for integrating dehazed images from multiple stages</h1><p>局部熵可以评估清晰度，因为它可以评估像素的局部变化。如果图像恢复良好，则补丁中的局部变化会非常明显。否则，如果图像处理不足，则残留的雾度会导致清晰度下降，并导致较低的局部变化。</p><p>这里详见论文。</p><p>将不同level上dehaze出来的结果进行融合是本文本文最精彩的地方。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ICCV2019论文。&lt;br&gt;作者认为现有方法不能很好解决真实场景中dehaze问题，并且不能将不同程度的haze区分对待，具有较大局限性&lt;/p&gt;
&lt;p&gt;网络由4各部分组成。the progressive transmission subnet (t-net) + the 
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="dehaze" scheme="https://zeyuxiao1997.github.io/tags/dehaze/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Pyramid Convolutional Network for Single Image Deraining</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/01/PDRNet/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/01/PDRNet/</id>
    <published>2020-02-01T03:42:06.000Z</published>
    <updated>2020-02-01T13:26:51.396Z</updated>
    
    <content type="html"><![CDATA[<p>CVPRW2019论文，目的是消除rain steak。</p><p>论文的motivation(这里是直接谷歌翻译的)：<font color="blue">为了充分利用多尺度冗余，网络通过层次小波变换将多雨图像分解为多尺度子带，然后分别由几个子网进行处理。 特别地，小波变换还起到下采样的作用，并在不增加网络深度或不牺牲网络效率的情况下扩大了接收场。</font>现有的方法仍然无法消除复杂背景场景中的大雨斑，并且无法将雨斑与无雨背景的相似图像纹理区分开。<br><img src="/2020/02/01/PDRNet/1.png" width="1"><br>使用类似上图的方法进行不同band上的derain</p><img src="/2020/02/01/PDRNet/2.png" width="1"><p>使用multi-scale loss进行训练</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;CVPRW2019论文，目的是消除rain steak。&lt;/p&gt;
&lt;p&gt;论文的motivation(这里是直接谷歌翻译的)：&lt;font color=&quot;blue&quot;&gt;为了充分利用多尺度冗余，网络通过层次小波变换将多雨图像分解为多尺度子带，然后分别由几个子网进行处理。 特别地，小
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="derain" scheme="https://zeyuxiao1997.github.io/tags/derain/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Single Image Deraining：A Comprehensive Benchmark Analysis</title>
    <link href="https://zeyuxiao1997.github.io/2020/02/01/derainBenchmark/"/>
    <id>https://zeyuxiao1997.github.io/2020/02/01/derainBenchmark/</id>
    <published>2020-02-01T03:26:22.000Z</published>
    <updated>2020-02-01T03:38:56.508Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2019论文。论文没有细看，重点在于关注去雨模型中常用的三种建模方式。</p><img src="/2020/02/01/derainBenchmark/1.png" width="1"><ul><li><p>rain steak</p><script type="math/tex; mode=display">\mathbf{R}_{s}=\mathbf{B}+\mathbf{S}</script></li><li><p>rain drop </p><script type="math/tex; mode=display">\mathbf{R}_{d}=(1-\mathbf{M}) \odot \mathbf{B}+\mathbf{D}</script></li></ul><p>M is a binary mask and ⊙ means element-wise multiplication. In the mask, a pixel x is part of a raindrop region if<br>M(x) = 1, and otherwise belongs to the background.</p><ul><li>rain and mist<script type="math/tex; mode=display">\mathbf{R}_{m}=\mathbf{B} \odot t+A(1-t)+\mathbf{S}</script></li></ul><p>where S is the rain streak component; t and A are the<br>transmission map and atmospheric light that determines the<br>fog/mist component, respectively.</p><p>三种不同的建模方式决定了去雨网络不同。未来我应该更关注rain drop和mist混合的情况。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;CVPR2019论文。论文没有细看，重点在于关注去雨模型中常用的三种建模方式。&lt;/p&gt;
&lt;img src=&quot;/2020/02/01/derainBenchmark/1.png&quot; width=&quot;1&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;rain steak&lt;/p&gt;
&lt;script ty
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="derain" scheme="https://zeyuxiao1997.github.io/tags/derain/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】CRRN：Multi-Scale Guided Concurrent Reflection Removal Network</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/31/CRRN/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/31/CRRN/</id>
    <published>2020-01-31T07:13:49.000Z</published>
    <updated>2020-02-01T03:03:06.702Z</updated>
    
    <content type="html"><![CDATA[<p>CVPR2018论文。</p><p>没有阴影的图片的梯度信息在去反射任务中非常重要，论文把之前的two-stage方案中的梯度推测和图片推测融合在统一的框架中。网络整体架构如下：<br><img src="/2020/01/31/CRRN/1.png" width="1"></p><h1 id="Gradient-Inference-Network-GiN"><a href="#Gradient-Inference-Network-GiN" class="headerlink" title="Gradient Inference Network(GiN)"></a>Gradient Inference Network(GiN)</h1><p>对于GIN网络，网络输入4-channel的tensor（3通道图像和1通道梯度），使用Unet的结构进行cross-scale的feature提取。为了更好的保留sharp details和避免gradient information丢失</p><h1 id="Image-inference-Network-IiN"><a href="#Image-inference-Network-IiN" class="headerlink" title="Image inference Network(IiN)"></a>Image inference Network(IiN)</h1><p>使用fine-tuned的VGG网络进行feature extraction，使用Inception-ResNet-v2用于更好的提取特征。多尺度表征在inverse problems上有较好的表现，因此在网络上进行相连（网络图上很清楚）</p><h1 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h1><p>在IiN网络中使用SSIM loss，<font color="blue">但是尽管SSIM具有感知上的贡献，但由于它对均匀偏差不敏感，因此可能会导致亮度变化和颜色偏移，从而使最终结果变得晦暗</font>。所以又使用了L1 loss对background layer进行恢复。</p><p>在GiN中，SSIM中的亮度和对比度分量变得不确定。 因此，我们忽略了原始SSIM中对比度和亮度的依赖性，并将GiN的损失函数定义为</p><script type="math/tex; mode=display">\mathcal{L}^{\mathrm{SI}}\left(x, x^{\star}\right)=1-\mathbf{S I}\left(x, x^{\star}\right)$$，where$$\mathbf{S I}=\frac{2 \sigma_{x x^{\star}}+c}{\sigma_{x}^{2}+\sigma_{x^{\star}}^{2}+c}</script><p>所以loss为<script type="math/tex">\begin{aligned} \mathcal{L}=& \gamma \mathcal{L}^{\mathrm{SSIM}}\left(\mathbf{B}, \mathbf{B}^{\star}\right)+\mathcal{L}_{1}\left(\mathbf{B}, \mathbf{B}^{\star}\right)+\\ & \mathcal{L}^{\mathrm{SSIM}}\left(\mathbf{R}, \mathbf{R}^{\star}\right)+\mathcal{L}^{\mathbf{S I}}\left(\nabla \mathbf{B}, \nabla \mathbf{B}^{\star}\right) \end{aligned}</script></p><h1 id="训练方案"><a href="#训练方案" class="headerlink" title="训练方案"></a>训练方案</h1><p>GiN先训练50epoch，然后和IiN一起训练。并且使用multi-size training strategy by feeding images of two sizes: coarse scales 96 × 160 and fine scale 224 × 288</p><h1 id="metric"><a href="#metric" class="headerlink" title="metric"></a>metric</h1><p>使用SSIM和SI，并且又基于region提出改进的SSIM和SI，这个还是比较创新的地方。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;CVPR2018论文。&lt;/p&gt;
&lt;p&gt;没有阴影的图片的梯度信息在去反射任务中非常重要，论文把之前的two-stage方案中的梯度推测和图片推测融合在统一的框架中。网络整体架构如下：&lt;br&gt;&lt;img src=&quot;/2020/01/31/CRRN/1.png&quot; width=&quot;1&quot;
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="dereflection" scheme="https://zeyuxiao1997.github.io/tags/dereflection/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Revisiting Shadow Detection：A New Benchmark Dataset for Complex World</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/31/FSDNet/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/31/FSDNet/</id>
    <published>2020-01-31T06:34:16.000Z</published>
    <updated>2020-01-31T07:01:22.502Z</updated>
    
    <content type="html"><![CDATA[<p>arxiv上的一篇preprint文章，通过构建更复杂场景的数据集进行阴影检测。论文没有详细看，我关注的重点在<font color="blue">特征融合模块</font>，这里的特征融合还是具有不错的参考价值的。</p><p>low-level feature maps包含细节信息，可以帮助discover shadow boundaries and tiny shadows；而深层次的features contain global context information to help recognize the shadows，因此这里使用如下的结构进行特征融合，也就是detail enhance module。</p><img src="/2020/01/31/FSDNet/1.png" width="1"><p>使用log函数衡量深层特征和浅层特征的距离，再引入alpha参数作为可学参数进行gate，没有引入过多参数量。<br>使用该模块可以<font color="red">enhance the spatial details and produce the refined low-level feature</font>。</p><p>整个网络的Pipeline如图<img src="/2020/01/31/FSDNet/2.png" width="2"></p><p>下面的表格是ablation study，使用DEM后效果更好<br><img src="/2020/01/31/FSDNet/3.png" width="3"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;arxiv上的一篇preprint文章，通过构建更复杂场景的数据集进行阴影检测。论文没有详细看，我关注的重点在&lt;font color=&quot;blue&quot;&gt;特征融合模块&lt;/font&gt;，这里的特征融合还是具有不错的参考价值的。&lt;/p&gt;
&lt;p&gt;low-level feature map
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="high-level" scheme="https://zeyuxiao1997.github.io/tags/high-level/"/>
    
      <category term="detection" scheme="https://zeyuxiao1997.github.io/tags/detection/"/>
    
      <category term="shadow" scheme="https://zeyuxiao1997.github.io/tags/shadow/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Bidirectional Feature Pyramid Network with Recurrent Attention Residual Modules for Shadow Detection</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/31/BFPN/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/31/BFPN/</id>
    <published>2020-01-31T03:17:46.000Z</published>
    <updated>2020-01-31T03:54:21.563Z</updated>
    
    <content type="html"><![CDATA[<p>ECCV2018的论文，使用双向注意力金字塔网络进行阴影检测。论文没有详细看，我关注的重点在<font color="blue">特征融合模块</font>，这里的特征注意力融合还是具有不错的参考价值的。</p><p>网络结构如图，<img src="/2020/01/31/BFPN/1.png" width="1"></p><p>在高分辨率特征图中抑制非阴影细节的能力有限，并将非阴影区域引入结果中的能力有限，因此作者摒弃之前直接将不同层间的feature maps直接concat，使用循环残差注意力结构，更好的对不同特征进行融合。<br><img src="/2020/01/31/BFPN/2.png" width="2"></p><p>最后，在双向结果融合的时候也使用了注意力融合方式。<br><img src="/2020/01/31/BFPN/3.png" width="3"></p><p>个人感觉这篇文章的亮点就是融合cross-scale的方式好，不过这是2018的论文了，距今已经有两年时间，两年间应该有更好的融合方式了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ECCV2018的论文，使用双向注意力金字塔网络进行阴影检测。论文没有详细看，我关注的重点在&lt;font color=&quot;blue&quot;&gt;特征融合模块&lt;/font&gt;，这里的特征注意力融合还是具有不错的参考价值的。&lt;/p&gt;
&lt;p&gt;网络结构如图，&lt;img src=&quot;/2020/01/3
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="high-level" scheme="https://zeyuxiao1997.github.io/tags/high-level/"/>
    
      <category term="detection" scheme="https://zeyuxiao1997.github.io/tags/detection/"/>
    
      <category term="shadow" scheme="https://zeyuxiao1997.github.io/tags/shadow/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Deep Multi-Model Fusion for Single-Image Dehazing(ICCV2019)</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/30/DM2FNet/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/30/DM2FNet/</id>
    <published>2020-01-30T12:47:31.000Z</published>
    <updated>2020-01-31T03:18:36.650Z</updated>
    
    <content type="html"><![CDATA[<p>ICCV2019论文。<a href="https://blog.csdn.net/Najlepszy/article/details/103354223" target="_blank" rel="noopener">CSDN</a>上有这篇论文的一个解析，可以参考。</p><p>大气散射模型为：<script type="math/tex">I(p)=J(p) \times T(p)+A(p) \times(1-T(p))</script>，I是输入的含雾的图像； J是所需要的没有雾的图像；T是传输图(trasmission map),传输地图，其表示影响光的到达照相机传感器的距离相关的分量；A是全局的大气散射图，代表着环境的光强度。深度学习方法将图像去雾任务视为layer separation model，分成无雾层和有雾层。</p><p>论文把两种建模方式进行融合，提高去雾的表现。<br>另外论文还指出：这些功能的除雾预测可以捕获大多数背景细节，但是许多非雾细节也会因雾而损坏。 另一方面，深CNN层的要素负责捕获语义信息以消除输入图像中的大多数雾度，但由于它们的接收场比浅层大，因此某种程度上缺乏非雾背景细节。</p><p>网络结构如下：<br><img src="/2020/01/30/DM2FNet/1.png" width="1"><br><img src="/2020/01/30/DM2FNet/2.png" width="2"><br><img src="/2020/01/30/DM2FNet/3.png" width="3"></p><p>论文说明了，使用多种建模方法进行模型融合可能会获得更好的结果，这启发我们是否可以在其他low-level tasks上也使用类似的方法进行融合呢？此外，论文的cross-scale设计的还是可以的，具有较大的借鉴意义.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ICCV2019论文。&lt;a href=&quot;https://blog.csdn.net/Najlepszy/article/details/103354223&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;上有这篇论文的一个解析，可以参考。&lt;/
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="dehaze" scheme="https://zeyuxiao1997.github.io/tags/dehaze/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】MOIRE PATTERN REMOVAL WITH MULTI-SCALE FEATURE ENHANCING NETWORK(ICMEW2019)</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/30/MFSE/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/30/MFSE/</id>
    <published>2020-01-30T04:03:54.000Z</published>
    <updated>2020-02-05T03:28:58.984Z</updated>
    
    <content type="html"><![CDATA[<p>ICMEW2019的论文。</p><p>论文基于以下出发点：高分辨率的底层特征有空间信息；低分辨率的高层特征有语义信息；之前的方法没有将两者良好结合，并且在cross-scale的时候没有结合不同scale上的features。</p><p>因此论文用下面的网络结构进行摩尔纹去除。<br><img src="/2020/01/30/MFSE/1.png" width="1"><br><img src="/2020/01/30/MFSE/2.png" width="2"><br>loss function是常规的loss。</p><p>论文比较好的地方是，通过融合不同深度的特征和不同语义、高频信息进行摩尔纹去除。这个和之前的HENet有异曲同工之妙.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ICMEW2019的论文。&lt;/p&gt;
&lt;p&gt;论文基于以下出发点：高分辨率的底层特征有空间信息；低分辨率的高层特征有语义信息；之前的方法没有将两者良好结合，并且在cross-scale的时候没有结合不同scale上的features。&lt;/p&gt;
&lt;p&gt;因此论文用下面的网络结构进行
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="demoire" scheme="https://zeyuxiao1997.github.io/tags/demoire/"/>
    
  </entry>
  
  <entry>
    <title>ISP Pipeline</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/30/ISPPipeline/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/30/ISPPipeline/</id>
    <published>2020-01-30T01:29:57.000Z</published>
    <updated>2020-01-31T06:46:30.645Z</updated>
    
    <content type="html"><![CDATA[<p>【coming soon】</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;【coming soon】&lt;/p&gt;

      
    
    </summary>
    
      <category term="theoretical analysis" scheme="https://zeyuxiao1997.github.io/categories/theoretical-analysis/"/>
    
    
      <category term="photography theory" scheme="https://zeyuxiao1997.github.io/tags/photography-theory/"/>
    
  </entry>
  
  <entry>
    <title>Demoire相关论文【硕博论文】</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/29/DemoireeireRelated/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/29/DemoireeireRelated/</id>
    <published>2020-01-29T07:41:24.000Z</published>
    <updated>2020-02-03T06:49:22.471Z</updated>
    
    <content type="html"><![CDATA[<!-- <font color='red'> </font> --><p>这里主要是看了几篇硕博论文后，摘录出来的一些关于Demoire上可能有用的KeyPoints。</p><h1 id="《基于图像分解的纹理图像摩尔纹消除方法》-2016天大"><a href="#《基于图像分解的纹理图像摩尔纹消除方法》-2016天大" class="headerlink" title="《基于图像分解的纹理图像摩尔纹消除方法》(2016天大)"></a>《基于图像分解的纹理图像摩尔纹消除方法》(2016天大)</h1><h2 id="大致思路"><a href="#大致思路" class="headerlink" title="大致思路"></a>大致思路</h2><p>纹理图像中摩尔纹现象是由数码相机场景采集过程中欠采样导致，纹理高频信息在采样频率下混叠到低频成分即对应空间域可见的摩尔纹结构。结合<font color="red"> 人眼对高频色彩信息的不敏感特性</font>，论文将纹理图像中的摩尔纹干扰简化为<font color="red">加性干扰模型</font>，即摩尔纹干扰降质纹理图像可分解为纹理图像成分和摩尔纹成分，通过设计合理的非相关特征分别约束纹理图像成分和摩尔纹成分，从而实现两者的有效分离。</p><h2 id="keypoints"><a href="#keypoints" class="headerlink" title="keypoints"></a>keypoints</h2><ul><li><p>考虑到Bayer CFA应用于数码相机成像设备中，输入的摩尔纹干扰降质纹理图像I中，相比红蓝两色通道，<font color="blue"> 绿色通道的图像信息采样频率最高，摩尔纹干扰程度最小</font>。【这一点感觉还是比较靠谱的，看MopNet里面展示的效果图确实看出来绿色通道上摩尔纹干扰最小，也许可以使用绿色做文章，是不是可以把绿色作为prior】</p></li><li><p>摩尔纹的频域能量分布集中</p></li><li><p>无论是空域结构统计特性还是频域能量分布，摩尔纹成分都和自然图像信息特性比较接近，相比图像去噪问题，摩尔纹消除方法研究更具有挑战性</p></li><li><p>LED屏幕的摩尔纹成分与屏幕显示图像无关，只和屏幕显示器显示参数、数码相机参数和成像位置角度有关。所以，屏摄图像中的摩尔纹成分和图像信息具有<font color="blue">加性关系</font>。</p></li><li><p>摩尔纹图案类似于结构性的干扰，图像去雨、扫描图像去网纹等图像分解算法对去摩尔有一定的作用。将纹理图像中摩尔纹与纹理成分间的关系简化为加性模型提供了思路借鉴。</p></li><li><p>用导向滤波后，同时红、蓝色通道内的纹理结构信息几乎完全保留</p></li><li><p>论文将图像模型简化为加性模型，然而实际情况下，摩尔纹分量与纹理分量在频域存在能量混叠，因此容易将纹理信息误判为摩尔纹成分，纹理恢复图像中存在轻微的振铃效应。另外，由于导向滤波算法本身固有的缺陷，容易导致恢复图像的局部出现光晕现象。</p></li></ul><h1 id="《基于CCD静态图像的摩尔纹去除算法研究》-2012南理工"><a href="#《基于CCD静态图像的摩尔纹去除算法研究》-2012南理工" class="headerlink" title="《基于CCD静态图像的摩尔纹去除算法研究》(2012南理工)"></a>《基于CCD静态图像的摩尔纹去除算法研究》(2012南理工)</h1><h2 id="keypoints-1"><a href="#keypoints-1" class="headerlink" title="keypoints"></a>keypoints</h2><ul><li><p>常见的几种CFA(color filter array)：Bayer型CFA, Mosaic型CFA, Stripe型CFA等，下图按次序依次表示上述三种CFA<img src="/2020/01/29/DemoireeireRelated/2.png" title="CFA"></p></li><li><p>人眼视觉对亮度的敏感度大于对色彩的敏感度。在Bayer模板中，绿色采样点的分布呈梅花点状，红色和蓝色点呈四方形分布。采样总数的1/2是绿色像素，这是人眼对绿色波长能量更为敏感的缘故，亮度信息主要反映在绿色分量之中，这样的数量分配使得人眼看到的亮度更为适宜，因此Bayer模板具有较好的彩色信号敏感特性和彩色恢复特性</p></li><li><p>采用Bayer CFA滤波后，每个像素只有红、绿、蓝一种分量。(a)图为经过单片CCD采集的原始图像，任意选取一块区域用红框表示，将框内区域放大如图(b)。在放大的图像里，可以清晰地看到红框内像素的排列格式。此时图像是不完整的，只有1/3的信息，为了得到完整的彩色图像，必须估算每个采样点的另外两个颜色分量值，这个估算的过程也被称为是去马赛克</p><img src="/2020/01/29/DemoireeireRelated/1.png" width="1"></li><li><p>只有在含有密布纹理的图像里才会出现摩尔纹，摩尔纹不会出现在一幅平滑的图像里。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- &lt;font color=&#39;red&#39;&gt; &lt;/font&gt; --&gt;
&lt;p&gt;这里主要是看了几篇硕博论文后，摘录出来的一些关于Demoire上可能有用的KeyPoints。&lt;/p&gt;
&lt;h1 id=&quot;《基于图像分解的纹理图像摩尔纹消除方法》-2016天大&quot;&gt;&lt;a href=&quot;#《基
      
    
    </summary>
    
      <category term="Summary" scheme="https://zeyuxiao1997.github.io/categories/Summary/"/>
    
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="demoire" scheme="https://zeyuxiao1997.github.io/tags/demoire/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】HIGH RESOLUTION DEMOIRE NETWORK(submit to ICIP?)</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/27/HRDN/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/27/HRDN/</id>
    <published>2020-01-27T07:30:38.000Z</published>
    <updated>2020-01-28T08:46:15.513Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是在github上看到的，作者开源了代码，看文章样式，应该是投稿ICIP的.</p><p>文中没有深入分析moire的建模方式，或者是其他什么高深的理论，只使用HRNet作为backbone进行parallel multi scale上的demoire，文章一般。<br><img src="/2020/01/27/HRDN/1.png" width="1"><br><img src="/2020/01/27/HRDN/2.png" width="2"></p><p>论文使用L1 Charbonnier Loss 和 L1 Sobel Loss。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章是在github上看到的，作者开源了代码，看文章样式，应该是投稿ICIP的.&lt;/p&gt;
&lt;p&gt;文中没有深入分析moire的建模方式，或者是其他什么高深的理论，只使用HRNet作为backbone进行parallel multi scale上的demoire，文章一般。
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="demoire" scheme="https://zeyuxiao1997.github.io/tags/demoire/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Mop Moire Patterns Using MopNet(ICCV2019)</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/27/MopNet/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/27/MopNet/</id>
    <published>2020-01-27T04:14:38.000Z</published>
    <updated>2020-01-27T08:42:38.777Z</updated>
    
    <content type="html"><![CDATA[<p>ICCV2019的论文。算是比较新的一篇用于demoire的论文。论文对摩尔纹的三种不同特性进行分析，并针对性的使用网络进行处理，把demoire的热度又炒了起来。</p><p>moire多样复杂，形状呈空间变化的条纹、曲线或波纹。 波纹图案还会将颜色变化叠加到图像上，从而大大降低图像的视觉质量。因此用传统的image restoration方法并不能很好的进行demoire，所以论文详细分析了moire的特性和机理，从以下三个方面进行demoire：</p><ul><li>Multi-scale feature aggregation</li><li>Channel-wise target edge predictor</li><li>Attributes-aware moire pattern classifier<br>为此分别设计了下面的网络结构进行demoire。<img src="/2020/01/27/MopNet/1.png" width="1"></li></ul><h2 id="Multi-scale-feature-aggregation"><a href="#Multi-scale-feature-aggregation" class="headerlink" title="Multi-scale feature aggregation"></a>Multi-scale feature aggregation</h2><p>摩尔纹出现在低频区域，和图片相互重合，很难去除。因此使用多尺度feature extractor进行特征提取，并使用SENet对features进行不同权重的组合。网络结构如图<br><img src="/2020/01/27/MopNet/2.png" width="2"></p><h2 id="Channel-wise-target-edge-predictor"><a href="#Channel-wise-target-edge-predictor" class="headerlink" title="Channel-wise target edge predictor"></a>Channel-wise target edge predictor</h2><p>moire形成具有明显边缘幅度的曲线和条纹，在RGB上的表现是不同的，见下图。<br><img src="/2020/01/27/MopNet/22.png" width="2"><br>因此设计了下图的网络进行channel-wise的颜色特征的边缘提取网络，注意：该部分需要先进行训练以便初始化。<br><img src="/2020/01/27/MopNet/3.png" width="3"></p><h2 id="Attributes-aware-moire-pattern-classifier"><a href="#Attributes-aware-moire-pattern-classifier" class="headerlink" title="Attributes-aware moire pattern classifier"></a>Attributes-aware moire pattern classifier</h2><p>Moire的形状不同，这里作者对不同类别的pattern进行标注，并pretrain了一个classifier，结构如下：<br><img src="/2020/01/27/MopNet/4.png" width="4"></p><h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><p>使用边缘loss+reconstruction loss+perceptual loss</p><p>论文针对数据集的外在表现，进行三种不同模块的设计。可能并没有涉及到成像原理或者是更加本质的东西，这个可能是现在demoire网络的通病，是否可以针对传统算法进行网络化处理，或者是针对更加本质的数据特征进行demoire重建，都是可能深入研究的重点。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ICCV2019的论文。算是比较新的一篇用于demoire的论文。论文对摩尔纹的三种不同特性进行分析，并针对性的使用网络进行处理，把demoire的热度又炒了起来。&lt;/p&gt;
&lt;p&gt;moire多样复杂，形状呈空间变化的条纹、曲线或波纹。 波纹图案还会将颜色变化叠加到图像上，从
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="low-level" scheme="https://zeyuxiao1997.github.io/tags/low-level/"/>
    
      <category term="demoire" scheme="https://zeyuxiao1997.github.io/tags/demoire/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Manipulating Attributes of Natural Scenes via Hallucination</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/06/attribute-hallucination/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/06/attribute-hallucination/</id>
    <published>2020-01-06T04:11:28.000Z</published>
    <updated>2020-01-31T06:44:39.244Z</updated>
    
    <content type="html"><![CDATA[<p>Siggraph Asia2019的工作，<br>【coming soon】</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Siggraph Asia2019的工作，&lt;br&gt;【coming soon】&lt;/p&gt;

      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="GAN" scheme="https://zeyuxiao1997.github.io/tags/GAN/"/>
    
      <category term="manipulation" scheme="https://zeyuxiao1997.github.io/tags/manipulation/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】EnlightenGAN：Deep Light Enhancement without Paired Supervision(Arxiv201906)</title>
    <link href="https://zeyuxiao1997.github.io/2020/01/06/VDMV/"/>
    <id>https://zeyuxiao1997.github.io/2020/01/06/VDMV/</id>
    <published>2020-01-06T03:02:24.000Z</published>
    <updated>2020-01-06T03:53:06.671Z</updated>
    
    <content type="html"><![CDATA[<p>arxiv上一篇关于unpair数据低光照增强的论文，看格式应该是投稿CVPR。</p><p>文章使用非成对图像做无监督的图像增强，实现低照域图像转换，不仅解决成对数据收集难的问题，还实现了不同场景低光照图像对应参考图像亮度不一致的问题。结构图如下所示:<br><img src="/2020/01/06/VDMV/1.png" width="1"><br>如上所示，网络整体是一个UNet的结构。对于一张图，一般希望增强其亮度暗的区域，所以这里将输入图提取亮度分量I，然后1-I并将其resize到对应的feature大小，和UNet转换中各部分的feature相乘，如上图所示。上采样使用的是resize加卷积的方式去除棋盘效应。Loss部分，包括，一个全局的GAN loss，判别器输入是生成图和目标域的整张图像，一个局部的GAN loss，输出是五个上述两者的patch，两个部分分别做全局和局部的约束，这里的GAN使用了LSGAN。此外，论文还使用了特征保留loss，即生成图和输入图内容要有一致性，这里使用的是VGG特征，同样也分为全局和局部两个部分。</p><p>增强常使用attention map、UNet等方法。这个方法是否可以用于video enhancement呢？</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;arxiv上一篇关于unpair数据低光照增强的论文，看格式应该是投稿CVPR。&lt;/p&gt;
&lt;p&gt;文章使用非成对图像做无监督的图像增强，实现低照域图像转换，不仅解决成对数据收集难的问题，还实现了不同场景低光照图像对应参考图像亮度不一致的问题。结构图如下所示:&lt;br&gt;&lt;img 
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="image" scheme="https://zeyuxiao1997.github.io/tags/image/"/>
    
      <category term="enhancement" scheme="https://zeyuxiao1997.github.io/tags/enhancement/"/>
    
      <category term="GAN" scheme="https://zeyuxiao1997.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Face Video Deblurring using 3D Facial Priors(ICCV2019)</title>
    <link href="https://zeyuxiao1997.github.io/2019/11/16/FVD3DFP/"/>
    <id>https://zeyuxiao1997.github.io/2019/11/16/FVD3DFP/</id>
    <published>2019-11-16T06:15:23.000Z</published>
    <updated>2019-11-20T02:46:49.330Z</updated>
    
    <content type="html"><![CDATA[<p>ICCV2019的作品。作者将人脸的3D信息加入到人脸deblur的网络中，相当于多给了一个表征层面的监督，获得了不错的效果。</p><p>文章中直接用了xin tong的人脸重建算法，把其中的identity信息揉到UNet中。网络结构如下：<br><img src="/2019/11/16/FVD3DFP/1.png" width="1"></p><p>具体没有什么好多说的，总之能揉的信息往里面揉就是了。个人感觉文章有些水吧。</p><p>【思考】</p><ul><li><p>人脸3D层面的identity information是否可以加到其他low-level的重建网络中？</p></li><li><p>更广意义上的3D信息（body gesture等）是否也可以加入到SR、deblur中呢？</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ICCV2019的作品。作者将人脸的3D信息加入到人脸deblur的网络中，相当于多给了一个表征层面的监督，获得了不错的效果。&lt;/p&gt;
&lt;p&gt;文章中直接用了xin tong的人脸重建算法，把其中的identity信息揉到UNet中。网络结构如下：&lt;br&gt;&lt;img src=&quot;
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="video" scheme="https://zeyuxiao1997.github.io/tags/video/"/>
    
      <category term="deblurring" scheme="https://zeyuxiao1997.github.io/tags/deblurring/"/>
    
      <category term="face" scheme="https://zeyuxiao1997.github.io/tags/face/"/>
    
  </entry>
  
  <entry>
    <title>【课程笔记】GPU编程(cuda)</title>
    <link href="https://zeyuxiao1997.github.io/2019/11/16/course-cuda/"/>
    <id>https://zeyuxiao1997.github.io/2019/11/16/course-cuda/</id>
    <published>2019-11-16T06:15:23.000Z</published>
    <updated>2019-11-26T02:43:31.381Z</updated>
    
    <content type="html"><![CDATA[<p>中科大《GPU并行计算和CUDA程序开发及优化》课程笔记，授课教师：谭立湘。<br><a id="more"></a></p><h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><p>GPU计算强控制弱，更多的资源用于数据计算；CPU强控制弱计算，更多的资源用于缓存。在使用GPU计算前，CPU必须先将数据传到GPU显存中；在GPU计算完成后，GPU再将结果数据返回给主机内存。CPU和GPU的通信尤其重要！！</p><h1 id="并行计算基础"><a href="#并行计算基础" class="headerlink" title="并行计算基础"></a>并行计算基础</h1><h2 id="并行计算"><a href="#并行计算" class="headerlink" title="并行计算"></a>并行计算</h2><ul><li><p>时间上并行（流水线）+空间上并行（矩阵分块） </p></li><li><p>并行计算的三个基本条件</p><ul><li>并行机（包含多个处理器核心+通过特定硬件相互连接、相互通信）</li><li>问题具有并行度，可以分解为多个可以并行执行的子任务</li><li>并行编程</li></ul></li></ul><h2 id="PCAM设计方法"><a href="#PCAM设计方法" class="headerlink" title="PCAM设计方法"></a>PCAM设计方法</h2><ul><li><p>设计并行算法的四个阶段：</p><ul><li>划分(Partitioning)；分解成小的任务，开拓并发性<ul><li>又分为域分解和功能分解</li><li>域分解的对象是数据，如果一个任务需要别的任务中的数据， 则会产生任务间的通讯</li><li>功能分解划分的对象是计算，将计算划分为不同的任务；划分后研究不同任务所需的数据，数据应当不相交</li></ul></li><li>通讯(Communication)：确定诸任务间的数据交换，检测划分的合理性<ul><li>划分产生的多个任务不能完全独立执行，需要在任务间进行数据交流； 功能分解确定了任务之间的数据流</li><li>分为局部/全局通讯、结构化/非结构化通讯、静态/动态通讯、同步/异步通讯</li></ul></li><li>组合(Agglomeration)：依据任务的局部性，组合成更大的任务<ul><li>组合是由抽象到具体的过程，是将组合的任务能在一类并行机上有效的执行</li><li>合并小尺寸任务，减少任务数。如果任务数恰好等于处理器数，则也完成了映射过程</li><li>通过增加任务的粒度和重复计算，可以减少通讯成本</li></ul></li><li>映射(Mapping)：将每个任务分配到处理器上，提高算法的性能<ul><li>每个任务要映射到具体的处理器，定位到运行机器上</li><li>任务数大于处理器数时，存在负载平衡和任务调度问题</li><li>映射的目标：减少算法的执行时间<img src="/2019/11/16/course-cuda/1.png" width="1"></li></ul></li></ul></li><li><p>原则<br>先尽量开发算法的并发性和扩展性，其次考虑通信成本和局部性，再次利用局部性相互组合减少通信成本，最后将组合后的任务分配到各个处理器</p></li><li><p>并行算法复杂性度量</p><ul><li>指标<ul><li>运行时间t(n): 包含计算时间和通讯时间，分别用计算时间步和选路时间步作单位。n为问题实例的输入规模。 </li><li>处理器数p(n) </li><li>并行算法成本c(n): c(n)=t(n)p(n) </li><li>总运算量W(n): 并行算法求解问题时所完成的总的操作步数</li></ul></li><li>设计并行算法时应尽可能地将每个时间步的工作量均匀地分摊给p台处理器，使各处理器处于活跃状态—-<font color="red"> 负载均衡 </font></li><li>三层并行计算模型  <img src="/2019/11/16/course-cuda/2.png" width="2">  <img src="/2019/11/16/course-cuda/3.png" width="3"></li></ul></li><li><p>并行算法设计<br>  。。。不写了，和本科学的操作系统有关</p></li><li><p>并行层次和代码粒度</p>  <img src="/2019/11/16/course-cuda/4.png" width="4"></li></ul><h1 id="OpenMP并行编程"><a href="#OpenMP并行编程" class="headerlink" title="OpenMP并行编程"></a>OpenMP并行编程</h1><ul><li><p>OpenMP并行编程模型</p><p>  程序开始时只有一个主线程，程序中的串行部分都是由主线程执行；并行的部分是通过派生其它线程来执行。但是如果并行部分没有结束时是不会执行串行部分的。</p>  <img src="/2019/11/16/course-cuda/5.png" width="5"></li></ul><h1 id="GPU-硬件架构"><a href="#GPU-硬件架构" class="headerlink" title="GPU 硬件架构"></a>GPU 硬件架构</h1><ul><li><p>GPU体系结构相关术语</p><ul><li>SP（Streaming Processor）:流处理器是GPU运算的最基本计算单元。</li><li>SFU（Special Function Unit）:特殊函数单元  用来执行超越函数指令，比如正弦、余弦、平方根等函数。</li><li>Shader core（渲染核/着色器），SP的另一个名称，又称为CUDA core，始于Fermi架构</li><li>DP （双精度浮点运算单元）</li><li>SM（Streaming Multiprocessors）:流式多处理器是GPU架构中的基本计算单元，也是GPU性能的源泉，由SP、DP、SFU等运算单元组成。这是一个典型的阵列机，其执行方式为SIMT（单指令多线程），区别于传统的   SIMD（单指令流多数据流），能够保证多线程的同时执行<br>SMX: Kepler架构中的SM</li><li>SMM: Maxwell架构中的SM</li><li>TPC（Thread Processing Cluster）线程处理器簇：由SM和L1 Cache组成，存在于Tesla架构中。</li><li>TPC（Texture Processing Cluster）纹理处理器簇：出现在Pascal架构中。</li><li>GPC（Graph Processing Cluster）图形处理器簇：类似于TPC，是介于整个GPU和SM间的硬件单元，始于Fermi构架。</li><li>SPA（Scalable streaming Processor Array）可扩展的流处理器阵列：所有处理核心和高速缓存的总和，包含所有的SM、TPC、GPC。与存储器系统共同组成GPU构架。</li><li>MMC（MeMory Controller）存储控制器：控制存储访问的单元，合并访存。每个存储控制器可以支持一定位宽的数据合并访存。</li><li>ROP（raster operation processors）光栅操作单元</li><li>LD/ST（Load/Store Unit）存储单元</li></ul></li><li><p>GPU计算能力<br>  需要区分计算能力和运算性能两个概念：</p><ul><li>运算性能包括整数运算性能、单精度浮点运算性能和双精度浮点运算性能等，表示GPU处理算术运算的能力，也是衡量GPU好坏的关键指标之一</li><li>计算能力是指GPU架构或GPU支持的功能，而与GPU的浮点运算性能无关</li></ul></li></ul><h2 id="几种架构"><a href="#几种架构" class="headerlink" title="几种架构"></a>几种架构</h2><ul><li>几种典型架构<ul><li>Tesla架构的SM<br>由8个SP、2个SF和一个执行双精度运算的DP组成，同时还包含了寄存器、共享存储、常量存储等单元<img src="/2019/11/16/course-cuda/6.png" width="6"></li><li>Tesla架构的TPC<br>2~3个SM配合L1 Cache构成TPC，Tesla架构主要核心型号有G80和GT200.每个TPC均由一个SM控制器进行统一控制<img src="/2019/11/16/course-cuda/7.png" width="7"></li><li>Tesla架构的G80<img src="/2019/11/16/course-cuda/8.png" width="8"></li><li>Tesla架构的GT200<img src="/2019/11/16/course-cuda/9.png" width="9"></li></ul></li></ul><ul><li>后面讲的全是GPU体系结构，感觉挺枯燥的，也不想做老本行了，算了就不看了叭</li></ul><h1 id="GPU-软件体系及CUDA编程模型"><a href="#GPU-软件体系及CUDA编程模型" class="headerlink" title="GPU 软件体系及CUDA编程模型"></a>GPU 软件体系及CUDA编程模型</h1><h2 id="GPU软件体系"><a href="#GPU软件体系" class="headerlink" title="GPU软件体系"></a>GPU软件体系</h2><ul><li>编译器<ul><li><font color="red">NVCC(NVIDIA CUDA Compiler)</font></li></ul></li><li>编程模型<ul><li>CUDA (CUDA C、CUDA Python、CUDA Java、CUDA C++、CUDA.NET)</li><li>OpenCL</li><li>OpenACC</li></ul></li><li>数学库函数<ul><li>包括但不限于：线性代数库CUBLAS、快速傅里叶变换CUFFT、深度学习CUDNN、FFmpeg</li></ul></li><li>性能分析工具<ul><li>NVIDIA Visual Profiler</li></ul></li><li><p>程序调试工具</p></li><li><p>管理软件</p><ul><li>nvidia-smi</li></ul></li><li>代码实例及使用文档</li></ul><h2 id="CUDA软件"><a href="#CUDA软件" class="headerlink" title="CUDA软件"></a>CUDA软件</h2><p>CUDA(Compute Unified Device Architecture，统一计算设备架构)是由 NVIDIA 推出的通用并行计算架构，该架构使 GPU 能够解决复杂的计算问题。 它包含了 CUDA 指令集架构（ISA）以及 GPU 内部的并行计算引擎。CUDA 是一个全新的软硬件架构，可将 GPU 视为一个并行数据计算的设备，对所进行的计算进行分配和管理，无需将其映射到图形 API</p><p>CUDA的基本思想是支持大量的线程级并行，并在硬件中动态地调度和执行这些线程。<font color="blue">GPU与CPU协同工作，GPU只有在计算高密度数据并行任务时才发挥作用!!</font><br><img src="/2019/11/16/course-cuda/10.png" width="10"></p><ul><li><p>GPU软件体系<br>分为三层结构：CUDA函数库、CUDA运行时API、CUDA驱动API</p><img src="/2019/11/16/course-cuda/11.png" width="11"></li><li><p>宿主代码和设备代码<br>基于 CUDA 开发的程序代码在实际执行中分为两种，<font color="red">一种是运行在CPU上的宿主代码（Host Code），一种是运行在GPU上的设备代码（Device Code）。</font>不同类型的代码由于其运行的物理位置不同，能够访问到的资源不同，因此对应的运行期组件也分为公共组件、宿主组件和设备组件三个部分，基本上囊括了所有在 GPU 开发中所需要的功能和能够使用到的资源接口，开发人员可以通过运行期环境的编程接口实现各种类型的计算。CUDA 所提供的运行期环境是通过驱动来实现各种功能的。 </p></li><li><p>CUDA软件环境<br>CUDA最主要的包含两个方面： ISA指令集架构与硬件计算引擎；实际上是硬件和指令集。见下图中的绿色部分，CUDA 架构的组件组成是： </p><ul><li>NVIDIA GPU中的并行计算引擎； </li><li>对硬件初始化、配置的OS内核级支持； </li><li>用户模式的驱动，为开发者提供设备级的API； </li><li>用于并行计算 kernel函数 的PTX 指令集架构(ISA，Instruction set architecture) <img src="/2019/11/16/course-cuda/12.png" width="12"></li></ul></li><li><p>CUDA编程术语</p><ul><li><p>kernel函数<br><font color="blue">Kernel函数是指为GPU设备编译的一个函数。也就是一个编译好的、在GPU上并行运行的计算函数。Kernel在GPU上以多个线程的方式被执行 </font>。一个完整的CUDA程序是由<font color="red">一系列的设备端kernel函数并行部分和主机端的串行处理部分</font>共同组成的。这些处理步骤会按照程序中相应语句的顺序依次执行，满足顺序一致性。 </p></li><li><p>Host，宿主，CPU、系统的CPU<br>负责启动应用程序，运行程序的串行部分，将程序的并行、计算密集的部分offload到GPU上运行，并最终返回程序的运行结果。</p></li><li><p>Device，设备，GPU，CPU的协处理<br>负责程序的并行、计算密集部分的处理，并将处理结果返回给Host。</p></li><li><p>Host Memory：宿主内存<br>是指安装GPU产品的主机的主板板载内存</p></li><li><p>device Memory：设备内存<br>GPU设备的板载内存（显卡的显存），是高性能的 GDDR5 内存，支持DMA访问</p></li><li><p>Block：线程块<br>执行Kernel的一组线程组成一个线程块。一个线程块<font color="red">最多可包含1024个</font><font color="green">并行执行</font>的线程，线程之间通过共享内存有效地共享数据，并实现线程的通信和栅栏同步。</p></li><li><p>线程ID：线程在线程块中的线程号（唯一标识）<br>基于线程ID的复杂寻址，应用程序可以将线程块指定为任意大小的二维或三维数组，并使用2个或3个索引来标识每个线程。</p><ul><li>对于大小是（Dx，Dy）的二维线程块，索引为（x，y）的线程的线程ID为（x+y*Dx）</li><li>对于大小为（Dx，Dy，Dz）的三维线程块，索引为（x，y，z）的线程的线程ID为：（x+y<em>Dx+z</em>Dx*Dy）</li><li><font color="blue">类似于C语言二维数组、三维数组中通过元素下标计算元素位置的方法</font></li></ul></li><li><p>Grid：线程块组成的线程网格（最多2^32 个blocks）<br>执行相同Kernel、具有相同维数和大小的线程块可以组合到一个网格中。这样单个Kernel调用中启动的线程数就可以很大。同一网格中的不同线程块中的线程不能互相通信和同步。Grid 是一个线程块阵列，执行相同的内核，从全局内存读取输入数据，将计算结果写入全局内存。 </p></li><li><p>Block ID：线程块ID<br>线程块ID是线程块在Grid中的块号。实现基于块ID的复杂寻址，应用程序可以将Grid指定为任意大小的二维数组，并用2个索引来标识每个线程块。对于大小为（Dx，Dy）的二维线程块，索引为（x，y）的线程块的ID为（x+y*Dx）。现已支持三维</p></li><li><p>Wrap：线程束<br>一个线程块中连续的固定数量（32）的线程组。将线程块中的线程划分成wrap的方式是：每个wrap包含线程ID连续递增的32个线程，从线程0开始递增到线程31。 </p></li><li><p>Stream：CUDA的一个Stream表示一个按特定顺序执行的GPU操作序列。诸如kernel启动、内存拷贝、事件启动和停止等操作可以排序放置到一个Stream中。一个Stream包含了一系列Grids，并且可以多个Stream并行执行。</p><p>下图很形象的展示了CPU、GPU以及GPU内部grid、block、thread的层次关系</p><img src="/2019/11/16/course-cuda/13.png" width="13"></li></ul></li><li><p>Grid、block 和 thread 的关系<br>  在 CUDA 架构下，GPU芯片执行时的最小单位是thread。若干个thread可以组成一个线程块（block）。<font color="red">一个block中的thread能存取同一块共享内存，可以快速进行同步和通信操作</font>。 每一个 block 所能包含的 thread 数目是有限的。执行相同程序的 block，可以组成grid。不同 block 中的 thread 无法存取同一共享内存，因此无法直接通信或进行同步。<font color="red">不同的 grid可以执行不同的程序（kernel）</font>。</p><p>  Grid是由线程块组成的网格。每个线程都执行该kernel，应用程序指定了Grid和线程块的维数，Grid的布局可以是一维、二维或三维的。每个线程块有一个唯一的线程块ID，线程块中的每个线程具有唯一的线程ID。同一个线程块中的线程可以协同访问共享内存，实现线程之间的通信和同步。每个线程块最多可以包含的线程的个数为1024个，线程块中的线程以32个线程为一组的Wrap的方式进行分时调度。<font color="red">每个线程在数据的不同部分并行地执行相同的操作。</font> </p></li><li><p>CUDA处理流程<br>  一个程序分为两个部份：Host 端和 Device 端。Host 端是指在 CPU 上执行的部份，而 Device 端则是在GPU上执行的部份。Device端的程序又称为kernel函数。通常 Host 端程序会将数据准备好后，复制到GPU的内存中，再由GPU执行 Device 端程序，完成后再由 Host 端程序将结果从GPU的内存中取回。CPU 存取 GPU 内存时只能通过 PCI-E 接口，速度有限。</p><ul><li>从系统内存中复制数据到GPU内存 </li><li>CPU指令驱动GPU运行； </li><li>GPU 的每个CUDA核心并行处理 </li><li>GPU 将CUDA处理的最终结果返回到系统的内存<img src="/2019/11/16/course-cuda/14.png" width="14"></li></ul></li><li><p>CUDA程序执行的基本流程</p><ul><li>分配内存空间和显存空间 </li><li>初始化内存空间 </li><li>将要计算的数据从Host内存上复制到GPU内存上 </li><li>执行kernel计算 </li><li>将计算后GPU内存上的数据复制到Host内存上 </li><li>处理复制到Host内存上的数据 </li></ul></li><li><p>CUDA编程模型<br>完整的CUDA程序包括主机端和设备端两部分代码，主机端代码在CPU上执行。设备端代码（kernel函数）运行在GPU上。其中一个kernel函数对应一个grid，每个grid根据需要配置不同的block数量和thread数量。</p><ul><li>CPU作为主机端只能有一个+GPU作为设备端可以有多个+CPU主要负责逻辑处理+GPU负责密集型的并行计算</li><li>CUDA包含两个并行逻辑层：block层和thread层。在执行时，block映射到SM、thread映射到SP(Core)</li><li><font color="blue">如何在实际应用程序中高效地开发这两个层次的并行是CUDA编程与优化的关键之一</font></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;中科大《GPU并行计算和CUDA程序开发及优化》课程笔记，授课教师：谭立湘。&lt;br&gt;
    
    </summary>
    
      <category term="course note" scheme="https://zeyuxiao1997.github.io/categories/course-note/"/>
    
    
      <category term="GPU programming" scheme="https://zeyuxiao1997.github.io/tags/GPU-programming/"/>
    
      <category term="cuda" scheme="https://zeyuxiao1997.github.io/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】Automatic Video Object Segmentation Based on Visual and Motion Saliency(TMM2019)</title>
    <link href="https://zeyuxiao1997.github.io/2019/11/15/VOSVMS/"/>
    <id>https://zeyuxiao1997.github.io/2019/11/15/VOSVMS/</id>
    <published>2019-11-15T02:12:38.000Z</published>
    <updated>2019-11-15T08:04:39.321Z</updated>
    
    <content type="html"><![CDATA[<p>TMM2019的作品。</p><h2 id="关于-视频-物体分割"><a href="#关于-视频-物体分割" class="headerlink" title="关于(视频)物体分割"></a>关于(视频)物体分割</h2><p>视频分割的挑战主要有两个，一是非刚性的前景变换，二是前景和背景的运动模糊且难以区分。越来越多的方法使用显著性进行prior的估计，以便更好的区分前景和后景【但是大多是使用image saliency + motion cues，并不能很好的为视频服务】</p><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><p>视觉显着性计算是在帧内找到显着对象，而运动线索计算能够在帧间定位对象。因此方法分为三个阶段，visual saliency computing、motion cues computing、segmentation<br><!-- ![pipeline](VOSVMS、1.png) --></p><p>本文均基于传统方法，没有用到深度学习。</p><p>文中比较有启发的一点是，由于现实中运动的复杂性，使用motion boundary and static boundary提取不同的外观，前者使用光流梯度进行优化，后者使用超像素的边缘进行表示。</p><h2 id="后记-显著性检测之流行排序"><a href="#后记-显著性检测之流行排序" class="headerlink" title="后记-显著性检测之流行排序"></a>后记-显著性检测之流行排序</h2><p><a href="http://papers.nips.cc/paper/2447-ranking-on-data-manifolds.pdf" target="_blank" rel="noopener">参考文献</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;TMM2019的作品。&lt;/p&gt;
&lt;h2 id=&quot;关于-视频-物体分割&quot;&gt;&lt;a href=&quot;#关于-视频-物体分割&quot; class=&quot;headerlink&quot; title=&quot;关于(视频)物体分割&quot;&gt;&lt;/a&gt;关于(视频)物体分割&lt;/h2&gt;&lt;p&gt;视频分割的挑战主要有两个，一是非刚性的
      
    
    </summary>
    
      <category term="paper reading" scheme="https://zeyuxiao1997.github.io/categories/paper-reading/"/>
    
    
      <category term="video" scheme="https://zeyuxiao1997.github.io/tags/video/"/>
    
      <category term="object segmentation" scheme="https://zeyuxiao1997.github.io/tags/object-segmentation/"/>
    
      <category term="saliency" scheme="https://zeyuxiao1997.github.io/tags/saliency/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记汇总</title>
    <link href="https://zeyuxiao1997.github.io/2019/11/15/index/"/>
    <id>https://zeyuxiao1997.github.io/2019/11/15/index/</id>
    <published>2019-11-15T01:52:15.000Z</published>
    <updated>2020-02-05T09:38:49.236Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SIGGRAPH"><a href="#SIGGRAPH" class="headerlink" title="SIGGRAPH"></a>SIGGRAPH</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>Manipulating Atributes of Natural Scenes via Hallucination</td><td><a href="https://zeyuxiao1997.github.io/2020/01/06/attribute-hallucination/">笔记</a></td><td>SiggraphAsia2019</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="CVPR"><a href="#CVPR" class="headerlink" title="CVPR"></a>CVPR</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>CRRN：Multi-Scale Guided Concurrent Reflection Removal Network</td><td><a href="https://zeyuxiao1997.github.io/2020/01/31/CRRN/">笔记</a></td><td>CVPR2018</td></tr><tr><td>Single Image Deraining：A Comprehensive Benchmark Analysis</td><td><a href="https://zeyuxiao1997.github.io/2020/02/01/derainBenchmark/">笔记</a></td><td>CVPR2019</td></tr><tr><td>Pyramid Convolutional Network for Single Image Deraining</td><td><a href="https://zeyuxiao1997.github.io/2020/02/01/PDRNet/">笔记</a></td><td>CVPRW2019</td></tr><tr><td>Fast and Accurate Single Image Super-Resolution via Information Distillation Network</td><td><a href>笔记</a></td><td>CVPR2018</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="ECCV"><a href="#ECCV" class="headerlink" title="ECCV"></a>ECCV</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>Bidirectional Feature Pyramid Network with Recurrent Attention Residual Modules for Shadow Detection</td><td><a href="https://zeyuxiao1997.github.io/2020/01/31/BFPN/">笔记</a></td><td>ECCV2018</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="ICCV"><a href="#ICCV" class="headerlink" title="ICCV"></a>ICCV</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>Face Video Deblurring using 3D Facial Priors</td><td><a href="https://zeyuxiao1997.github.io/2019/11/16/FVD3DFP/">笔记</a></td><td>ICCV2019</td></tr><tr><td>Mop Moire Patterns Using MopNet</td><td><a href="https://zeyuxiao1997.github.io/2020/01/27/MopNet/">笔记</a></td><td>ICCV2019</td></tr><tr><td>Deep Multi-Model Fusion for Single-Image Dehazing</td><td><a href="https://zeyuxiao1997.github.io/2020/02/01/derainBenchmark/">笔记</a></td><td>ICCV2019</td></tr><tr><td>LAP-Net：Level-Aware Progressive Network for Image Dehazing</td><td><a href="https://zeyuxiao1997.github.io/2020/02/02/LAPNet/">笔记</a></td><td>ICCV2019</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="OTHER-CV-conference"><a href="#OTHER-CV-conference" class="headerlink" title="OTHER CV conference"></a>OTHER CV conference</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>MOIRE PATTERN REMOVAL WITH MULTI-SCALE FEATURE ENHANCING NETWORK</td><td><a href="https://zeyuxiao1997.github.io/2020/01/30/MFSE/">笔记</a></td><td>ICMEW2019</td></tr><tr><td>EEMEFN:Low-Light Image Enhancement via Edge-Enhanced Multi-Exposure Fusion Network</td><td><a href="https://zeyuxiao1997.github.io/2020/02/03/EEMEFN/">笔记</a></td><td>AAAI2020</td></tr><tr><td>Lightweight Image Super-Resolution with Information Multi-distillation Network</td><td><a href>笔记</a></td><td>MM2019</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="NIPS-ICLR-ICML"><a href="#NIPS-ICLR-ICML" class="headerlink" title="NIPS+ICLR+ICML"></a>NIPS+ICLR+ICML</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="JOURNAL"><a href="#JOURNAL" class="headerlink" title="JOURNAL"></a>JOURNAL</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>A Video Deblurring Algorithm Based on Motion Vector and An Encorder-Decoder Network</td><td><a href="https://zeyuxiao1997.github.io/2019/11/16/FVD3DFP/">笔记</a></td><td>IEEE Access 2019</td></tr><tr><td>Learning a Deep Single Image Contrast Enhancer from Multi-Exposure Images</td><td><a href="https://zeyuxiao1997.github.io/2020/02/03/SICE/">笔记</a></td><td>TIP2018</td></tr><tr><td>An Underwater Image Enhancement Benchmark Dataset and Beyond</td><td><a href="https://zeyuxiao1997.github.io/2020/02/03/UnderWaterEnhancer/">笔记</a></td><td>TIP2019</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h1 id="PREPRINT"><a href="#PREPRINT" class="headerlink" title="PREPRINT"></a>PREPRINT</h1><div class="table-container"><table><thead><tr><th>论文标题</th><th>链接</th><th>备注</th></tr></thead><tbody><tr><td>EnlightenGAN：Deep Light Enhancement without Paired Supervision</td><td><a href="https://zeyuxiao1997.github.io/2020/01/06/VDMV/">https://zeyuxiao1997.github.io/2020/01/06/VDMV/</a></td><td>arxiv201906</td></tr><tr><td>HIGH RESOLUTION DEMOIRE NETWORK</td><td><a href="https://zeyuxiao1997.github.io/2020/01/27/HRDN/">笔记</a></td><td>github</td></tr><tr><td>Revisiting Shadow Detection:A New Benchmark Dataset for Complex World</td><td><a href="https://zeyuxiao1997.github.io/2020/01/31/FSDNet/">笔记</a></td><td>arxiv201911</td></tr><tr><td>Gradient Information Guided Deraining with A Novel Network and Adversarial</td></tr></tbody></table></div><p>Training | <a href="https://zeyuxiao1997.github.io/2020/02/02/GRASPPGAN/">笔记</a> | arxiv201910 |<br>| Real-Time Semantic Segmentation via Multiply Spatial Fusion Network | <a href="https://zeyuxiao1997.github.io/2020/02/02/MSFNet/">笔记</a> | arxiv201911 |<br>| FC2N：Fully Channel-Concatenated Network for Single Image Super-Resolution | <a href>笔记</a> | arxiv201907 |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |<br>|  |  |  |</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SIGGRAPH&quot;&gt;&lt;a href=&quot;#SIGGRAPH&quot; class=&quot;headerlink&quot; title=&quot;SIGGRAPH&quot;&gt;&lt;/a&gt;SIGGRAPH&lt;/h1&gt;&lt;div class=&quot;table-container&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr
      
    
    </summary>
    
      <category term="quick search" scheme="https://zeyuxiao1997.github.io/categories/quick-search/"/>
    
    
      <category term="conference" scheme="https://zeyuxiao1997.github.io/tags/conference/"/>
    
  </entry>
  
</feed>
